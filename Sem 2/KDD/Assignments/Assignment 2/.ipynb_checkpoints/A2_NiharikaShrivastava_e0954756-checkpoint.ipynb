{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e6befe",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 2\n",
    "\n",
    "For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed). For ease of checking your answers + grading, please keep your plain text answers in blue text.\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A2_YourName_YourNUSNETID.ipynb** (e.g., **A2_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Remember to rename and save the script file **A2_script.py** as **A2_YourName_YourNUSNETID.py** (e.g., **A2_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is Sunday April 2, 11.59 pm. You can submit it to Canvas under Assignments. You can use up to 4 out of your 8 late days, which extend the deadline by 24 hours each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7afe2",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task.\n",
    "\n",
    "* **1 Decision Trees (16 Points)**\n",
    "    * 1.1 Implementing a Decision Tree Regressor\n",
    "        * 1.1 a) Calculating All Possible Thresholds (2 Points)\n",
    "        * 1.1 b) Finding the Best Split (4 Points)\n",
    "        * 1.1 c) Training the Decision Tree Regressor (4 Points)\n",
    "    * 1.2 True/False Questions about Decision Trees (6 Points)\n",
    "* **2 Tree Ensembles (12 Points)**\n",
    "    * 2.1 Implementing a Random Forest Regressor\n",
    "        * 2.1 a) Implementing Bagging (2 Points)\n",
    "        * 2.1 b) Implementing Feature Sampling (2 Points)\n",
    "        * 2.1 c) Training the Random Forest Regressor (2 Points)\n",
    "        * 2.1 d) Predicting Output Values (2 Points)\n",
    "    * 2.2 Questions about Tree Ensembles\n",
    "        * 2.2 a) Random Forest: Bagging Only vs. Bagging + Feature Sampling (2 Points)\n",
    "        * 2.2 b) Random Forest: Regression vs. Classification (2 Points)\n",
    "* **3 Logistic Regression (8 Points)**\n",
    "    * 3.1 True/False Questions about Logistic Regression (8 Points)\n",
    "* **4 Model Selection (14 Points)**\n",
    "    * 4.1 Data Preprocessing (3 Points)\n",
    "    * 4.2 Performing K-Fold Cross-Validation \"By Hand\"\n",
    "        * 4.2 a) Implement k-fold Cross Validation (4 Points)\n",
    "        * 4.2 b) Run k-fold Cross Validation for 4 Regressors and Discuss the Results. (4 Points)\n",
    "    * 4.3 Hyperparameter Tuning (3 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38005d",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5163ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that the notebook will reload external python modules;\n",
    "# i.e. when you change something in your .py script, you do not need to re-import the .py script file.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905218d7",
   "metadata": {},
   "source": [
    "Making all the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c42f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from src.utils import plot_validation_results, plot_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032c810",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A2_script.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8edf0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from A2_script import MyDecisionTreeRegressor, MyRandomForestRegressor # replace this line with the next line after renaming\n",
    "from A2_NiharikaShrivastava_e0954756 import MyDecisionTreeRegressor, MyRandomForestRegressor # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408cb8f",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d6f3b",
   "metadata": {},
   "source": [
    "## 1 Decision Trees\n",
    "\n",
    "Decision Trees are a very common classification and regression model on their own, but even more so as the core building block of ensemble models (Random Forests and Gradient Boosted Trees) which are among the most popular prediction models in many practical settings. As such, it is particularly important to gain a solid understanding of how they work, rather than treating them as a \"black box\", which we will do by implementing the core parts of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350025e",
   "metadata": {},
   "source": [
    "### 1.1 Implementing a Decision Tree Regressor\n",
    "\n",
    "In this section, you will implement your own Decision Tree regressor. To keep the complexity low, it is simplified as follows:\n",
    "\n",
    "* Only numerical features are supported\n",
    "* Only binary splits are supported\n",
    "\n",
    "(incidentally, this is in line with scitkit-learn's [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) implementation)\n",
    "\n",
    "In the file `A2a_script.py`, you can find the skeleton code for the class `MyDecisionTreeRegressor`. Note that many parts of the implementation are given to you. Have a good look at the provided code to understand its different components.\n",
    "\n",
    "**In the following subtasks, you will complete the methods where indicated by the comments within each method.** No need to worry, we will guide you through the implementation step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aabe69",
   "metadata": {},
   "source": [
    "#### Calculating the RSS Scores of a Node and a Split (nothing for you to do here!)\n",
    "\n",
    "Recall from lecture that a criterion score is necessary in order to decide which split to make at a node. The criterion score can be interpreted as measuring the quality (or \"purity\") of the response variable within a node. In lecture, we introduced the use of the Gini index and entropy for measuring the purity of a node. \n",
    "\n",
    "In this assignment, we instead focus on regression trees, which are similar but slightly easier to implement. The quality measure used here is the *Residual Sum of Squares (RSS)*, or the sum of squared differences between the response variables of samples in a node, and the mean response variable in the node. Formally, the RSS score of a node is defined as:\n",
    "\n",
    "$$RSS_{node} = \\sum_{i\\in R_{node}} (y_i - \\mu_{R_{node}})^2$$\n",
    "\n",
    "where $y_i$ is the response variable for sample $i$, $R_{node}$ is the set of sample indices at the node, and $\\mu_{R_{node}}$ is the mean of the response variable for samples in $R_{node}$.\n",
    "\n",
    "Analogously, the RSS score of a split is defined as summing the RSS score of the nodes resulting from the split:\n",
    "\n",
    "$$RSS_{split} = \\sum_{k=1}^K\\sum_{i\\in R_k} (y_i - \\mu_{R_k})^2$$\n",
    "\n",
    "where $K$ is the number of child nodes (here $K=2$ since all splits are binary splits), $R_k$ is the set of sample indices in child node $k$, and $\\mu_{R_k}$ is the mean of the response values in $R_k$.\n",
    "\n",
    "The intuition behind the RSS criterion is analogous to that of Gini index / entropy: the RSS at a node measures the amount of variation in a node, so if we find a split that achieves a low RSS, this means we have effectively separated the high values of the response variable from the low values of the response variable, which is a good split.\n",
    "\n",
    "Since these are very basic arithmetic operations, we give you the two methods `calc_rss_score_node` and `calc_rss_score_split` for free. Have a look at both methods to convince yourself that these methods implement the formulas given above. Again, since we consider only binary splits, method `calc_rss_score_split` simplifies to calling `calc_rss_score_node` twice on both child nodes and summing up both scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c7665",
   "metadata": {},
   "source": [
    "#### 1.1 a) Calculating All Possible Thresholds (2 Points)\n",
    "\n",
    "We have seen in the lecture that we can split numerical features using thresholds that separate the feature values into all values less or equal to a threshold and larger than the threshold.\n",
    "\n",
    "**Implement the method `calc_thresholds()`!** In principle, there are different possible approaches. Here, we consider a simple approach, where we first sort the unique values of a feature, then the possible thresholds are the midpoints between every two adjacent values, as shown in the following example. You can test your implementation using the code cell below; it shows for three different inputs the expected outputs. (Hint: [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) might make life easier; note that it returns the values in sorted order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55389384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds: [1.5, 2.5, 3.5]\n",
      "Thresholds: [1.5, 2.5, 3.5, 4.5, 5.5]\n",
      "Thresholds: [0.5, 4.5]\n",
      "Thresholds: []\n"
     ]
    }
   ],
   "source": [
    "values1 = np.array([4, 1, 2, 1, 1, 3])\n",
    "values2 = np.array([1, 2, 3, 4, 5, 6])\n",
    "values3 = np.array([8, 0, 0, 1, 0, 0])\n",
    "values4 = np.array([1, 1, 1])\n",
    "\n",
    "my_regressor = MyDecisionTreeRegressor()\n",
    "\n",
    "thresholds = my_regressor.calc_thresholds(values1)  # Expected output: [1.5 2.5 3.5]\n",
    "print('Thresholds: {}'.format(thresholds))\n",
    "\n",
    "thresholds = my_regressor.calc_thresholds(values2)  # Expected output: [1.5 2.5 3.5 4.5 5.5]\n",
    "print('Thresholds: {}'.format(thresholds))\n",
    "\n",
    "thresholds = my_regressor.calc_thresholds(values3)  # Expected output: [0.5 4.5]\n",
    "print('Thresholds: {}'.format(thresholds))\n",
    "\n",
    "thresholds = my_regressor.calc_thresholds(values4)  # Expected output: []\n",
    "print('Thresholds: {}'.format(thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb1743",
   "metadata": {},
   "source": [
    "#### Creating a Split (nothing for you to do here!)\n",
    "\n",
    "We provide you the method `create_split()` to split a list `x` of feature values with respect to a threshold. Note that the method returns not the values themselves but the indices of the values. By giving you this method, we can ensure that everyone should get the exact same result, as well as that the results will match with the Decision Tree implementation of scikit-learn. For example, `create_split()` puts all samples `<=` then the threshold into the left child node (and all others into the right child node). However, there would be no principle difference to put them in the right child node (and all others into the left child node). It's purely a convention to ensure consistency. Again, have a good look at `create_split()` to understand its inputs, inner workings, and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658383ea",
   "metadata": {},
   "source": [
    "#### 1.1 b) Finding the Best Split (4 Points)\n",
    "\n",
    "Finding the best split is at the heart of training of a Decision Tree. As we saw in the lecture, the best split is defined by which\n",
    "\n",
    "* feature *and*\n",
    "\n",
    "* threshold\n",
    "\n",
    "result in a split of the data samples yielding the best score (here: the split with the lowest RSS score). We already have the methods to calculate the RSS scores of a node and a split, as well as to calculate the thresholds to be considered for a given feature. Finding the best split essentially boils simply down to checking for each feature and all corresponding thresholds to check which split gets the lowest RSS score.\n",
    "\n",
    "To test your implementation, let's first define a small toy dataset of 20 data samples. The two input features are the `weight` (in kg) and `height` (in cm) of a person, and the output value (i.e., the response variable) is the blood sugar level (in mmol/L)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da84f196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy dataset -- #samples: 20, #features: 2\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([68, 71, 92, 59, 80, 81, 75, 88, 45, 64, 59, 87, 80, 73, 55, 92, 93, 72, 49, 57])\n",
    "heights = np.array([175, 175, 170, 168, 184, 184, 167, 155, 152, 163, 190, 161, 160, 174, 159, 183, 165, 181, 179, 154])\n",
    "\n",
    "X_toy = np.stack((weights, heights), axis=1)\n",
    "y_toy = np.array([7.8, 7.7, 11.0, 7.9, 6.8, 7.9, 6.3, 9.5, 8.1, 9.0, 6.0, 10.1, 7.0, 7.8, 7.7, 10.2, 9.8, 7.0, 6.4, 7.6])\n",
    "\n",
    "print('Toy dataset -- #samples: {}, #features: {}'.format(X_toy.shape[0], X_toy.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf6e1b",
   "metadata": {},
   "source": [
    "**Implement method `calc_best_split`** to find the best split with respect to all features and corresponding thresholds. You obviously can and should make use of the existing methods you have implemented or are already provided to you. The skeleton code of method `calc_best_split()` shows you which values need to be returned. The 4 variables you need to return are those that:\n",
    "\n",
    "* define a split (`best_feature_idx`, `best_threshold`)\n",
    "\n",
    "* decide whether to split or not (`best_score`)\n",
    "\n",
    "* recursively build the Decision Tree (`best_split`, which is a tuple containing the left and right child indices)\n",
    "\n",
    "You can use the code cell below to test your implementation of the method. The following markdown cell shows the expected outcomes. As your implementation of the Decision Tree regressor does not contain any randomness, your results should match the expected outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066632a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_regressor = MyDecisionTreeRegressor()\n",
    "\n",
    "score, threshold, feature_idx, split = my_regressor.calc_best_split(X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed177dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature index:   0 (0=weight, 1=height)\n",
      "Best threshold:       84.0 (for the best feature)\n",
      "#samples (left)       15\n",
      "#samples (right)      5\n"
     ]
    }
   ],
   "source": [
    "print(\"Best feature index:   {} (0=weight, 1=height)\".format(feature_idx))\n",
    "print(\"Best threshold:       {} (for the best feature)\".format(threshold))\n",
    "print(\"#samples (left)       {}\".format(len(split[0])))\n",
    "print(\"#samples (right)      {}\".format(len(split[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ff588",
   "metadata": {},
   "source": [
    "The code cell above should yield the following output:\n",
    "    \n",
    "```\n",
    "Best feature index:   0 (0=weight, 1=height)\n",
    "Best threshold:       84.0 (for the best feature)\n",
    "#samples (left)       15\n",
    "#samples (right)      5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230b76a",
   "metadata": {},
   "source": [
    "You can now also compare your implementation with scitkit-learn's [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) implementation. Since we are only interested in the first split here -- that is, we are only interested in the root node and its two children -- we can set `max_depth=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca1e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sk_regressor = DecisionTreeRegressor(max_depth=1).fit(X_toy, y_toy)\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(sk_regressor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb7c59",
   "metadata": {},
   "source": [
    "The information in the box for the root node reflects the feature index and threshold for the best split; the boxes for the two child nodes show you the number of samples in both children. Of course, all that information should match your results from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852275e0",
   "metadata": {},
   "source": [
    "#### 1.1 c) Training the Decision Tree Regressor (4 Points)\n",
    "\n",
    "With all the methods for scoring and splitting nodes in place, we can now train our Decision Tree. To this end, let's first load a small dataset. We use the [Hitters](https://www.kaggle.com/floser/hitters) dataset which aims to predict the salaries of baseball players based on their statistics. You can check the website for more details about the different features. In the following, we just consider a subset of all features to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e137bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 210\n",
      "Size of test: 53\n"
     ]
    }
   ],
   "source": [
    "subset = ['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Years', 'Assists', 'Errors']\n",
    "\n",
    "df = pd.read_csv('data/a2-hitters.csv')\n",
    "df = df.dropna(subset=subset+['Salary']) # We ignore all samples with NA values; it's not important here\n",
    "\n",
    "X = df[subset].to_numpy()\n",
    "y = df[['Salary']].to_numpy().squeeze()\n",
    "\n",
    "# Note: sklearn.model_selection.train_test_split also shuffles the data!\n",
    "# So we need to set random_state to ensure consistent results\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f0b07",
   "metadata": {},
   "source": [
    "Have a good look at methods `fit()` and `_fit()` and get a good understanding! They basically contain all the code required for recursively splitting nodes. So you don't have to worry about that. The only things missing from the `_fit()` method are the conditions for when to stop the recursion. Some conditions can be checked before calculating the best split, some afterwards (cf. lecture slides, including the ones to avoid overfitting).\n",
    "\n",
    "**Implement the conditions to stop the recursion in method `_fit()` as indicated by `max_depth` and `min_samples_split`**! \n",
    "\n",
    "For example, if `max_depth`=1, this condition should stop the recursion even if we are in the top level node (which has `depth`=0). Similarly, we should stop the recursion if the current node has less than `min_samples_split` samples. The existing code has some stopping conditions already implemented; you can use them as examples.\n",
    "\n",
    "Each condition can be implemented as a simple `if` statement that will `return` if true. (Note: The 2 parameters `max_depth` and `min_samples_split` are adopted from scikit-learn's [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html); only exception: `min_samples_split` is only interpreted as *int*, scitkit-learn also has a separate interpretation if `min_sample_split` is a *float*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc58326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare node counts for different values of max_depth\n",
      "3 vs 3\n",
      "13 vs 13\n",
      "41 vs 41\n",
      "\n",
      "Compare node counts for different values of min_samples_split\n",
      "99 vs 99\n",
      "41 vs 41\n",
      "31 vs 31\n"
     ]
    }
   ],
   "source": [
    "print('Compare node counts for different values of max_depth')\n",
    "for max_depth in [1, 3, 5]:\n",
    "    my_regressor = MyDecisionTreeRegressor(max_depth=max_depth).fit(X_train, y_train)\n",
    "    sk_regressor = DecisionTreeRegressor(max_depth=max_depth).fit(X_train, y_train)\n",
    "    print('{} vs {}'.format(sk_regressor.tree_.node_count, my_regressor.get_node_count()))\n",
    "\n",
    "print('\\nCompare node counts for different values of min_samples_split')\n",
    "for min_samples_split in [10, 20, 30]:\n",
    "    my_regressor = MyDecisionTreeRegressor(min_samples_split=min_samples_split).fit(X_train, y_train)\n",
    "    sk_regressor = DecisionTreeRegressor(min_samples_split=min_samples_split).fit(X_train, y_train)\n",
    "    print('{} vs {}'.format(sk_regressor.tree_.node_count, my_regressor.get_node_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee33a5",
   "metadata": {},
   "source": [
    "Of course, the pairs of node counts should always match. (Hint: In case of (small) discrepancies, you first might want to check the stop conditions in the `_fit()` method. Simply a `<` instead of a `<=` can change the outcome. You can consult with the documentation of scikit-learn's [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) to identify the correct conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ef0a1",
   "metadata": {},
   "source": [
    "#### Predicting Output Values (nothing for you to do here!)\n",
    "\n",
    "With the code for training the Decision Tree, the code for predicting output values is very straightforward. We therefore provide you with the methods `predict()` and `predict_sample()` with the former simply calling the latter for each given sample. The prediction of a single sample has to consider only two cases: (a) if the current node is not a leaf, we need to decide whether to recursively check the left or the right child node; (b) if the current node is a leaf, the predicted value is simply the mean of values in the node. Have a good look at `predict_sample()` to convince yourself that this method implements these two cases.\n",
    "\n",
    "With the implementation of `MyDecisionTreeRegressor` complete, you can now compare your implementation with the one from scikit-learn in terms of the prediction values. The code cell below trains two Decision Trees, one using your implementation and the one from scikit-learn. Feel free to modify the values of `max_depth` and `min_samples_split`. For comparison, we print the predicted values of the first five samples in the test set `X_test` derived from both decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58cc3184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244.75       725.92588889 244.75       725.92588889 207.46666667]\n",
      "[244.75       725.92588889 244.75       725.92588889 207.46666667]\n"
     ]
    }
   ],
   "source": [
    "max_depth = 5\n",
    "min_samples_split = 20\n",
    "\n",
    "my_regressor = MyDecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split).fit(X_train, y_train)\n",
    "sk_regressor = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split).fit(X_train, y_train)\n",
    "\n",
    "# Let's compare the first 5 predicted values\n",
    "print(my_regressor.predict(X_test)[:5])\n",
    "print(sk_regressor.predict(X_test)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606a212",
   "metadata": {},
   "source": [
    "If you get identical predictions from both Decision Trees (and different values for `max_depth` and `min_samples_split`), your implementation should be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f485d30",
   "metadata": {},
   "source": [
    "### 1.2 True/False Questions about Decision Trees (6 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93efb6",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(3).\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       | Brief Explanation |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| (1)  | The decision tree learning algorithm always finds an optimal decision tree, i.e., one that minimizes the number of questions needed to classify a case. | <font color='blue'> False </font> | <font color='blue'> There are cases where finding the optimal decision tree is computationally infeasible since the number of possible decision trees grows exponentially with the number of attributes and instances. Furthermore, the decision tree learning algorithm often relies on heuristics and approximations to reduce the search space, such as greedy algorithms that choose the best attribute to split at each node without considering the long-term consequences of this choice. This can lead to suboptimal trees that require more questions to classify a case than the optimal tree. </font> |\n",
    "| (2)  | The performance of a Decision Tree classifier or regressors generally benefits from normalization/standardization of the input data. | <font color='blue'> False </font>| <font color='blue'> No, the algorithm performance is exactly the same if the entire input data is transformed. This applies to any order-preserving transformations: e.g. normalization, standardization, rescaling. </font> |\n",
    "| (3)  | Small variations in the input data can yield very different Decision Trees.  | <font color='blue'> True </font>| <font color='blue'> Since Decision Trees formulation uses greedy heuristics such as finding the best split without considering the long-term consequences of this choice, variations in the input features can cause a different node to become the best split resulting in a completely different Decision tree. </font> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4fadb4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11d038",
   "metadata": {},
   "source": [
    "## 2 Tree Ensembles\n",
    "\n",
    "With the implementation of a Decision Tree regressor in place, the goal of this task now is to show that the extension to ensemble models  like Random Forests and Gradient Boosting Trees is rather straightforward one. In the following you will implement\n",
    "\n",
    "* a Random Forest regressor *and*\n",
    "* a Gradient Boosting Tree regressor\n",
    "\n",
    "For both implementations you can directly adopt the algorithms covered in the lecture. Once you have completed this task, we hope that you see even more advanced models no longer as a complicated black box :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2a5ff",
   "metadata": {},
   "source": [
    "### 2.1. Implementing a Random Forest Regressor\n",
    "\n",
    "We saw that a Random Forest trains a whole set of Decision Trees in parallel. To yield different Decision Trees each time, two sampling strategies are performed:\n",
    "\n",
    "* **Bootstrap Sampling:** randomly sample N data points with replacement (N = total number of data points); *and*\n",
    "\n",
    "* **Feature Sampling:** randomly choose only a subset of features to be used for training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a338275",
   "metadata": {},
   "source": [
    "#### 2.1 a) Implement Bagging (2 Points)\n",
    "**Implement `bootstrap_sampling()` to generate a bootstrap sample for a given dataset!** \n",
    "\n",
    "The input is represented by feature array `X`, and target array `y` containing the output values. You can use the code cell below to test your implementation. The cell computes five bootstrap samples and prints the shapes of data matrix `X_bootstrap` and output values vector `y_bootstrap`, as well as the vector of the first five samples output values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889bbb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2) (20,) [ 7.  10.2  7.8  7.9  7.9  9.5  9.   7.6  6.4  6.8]\n",
      "(20, 2) (20,) [ 9.   7.6  9.8  7.6  7.9 10.2 10.2  7.8  6.4  7.9]\n",
      "(20, 2) (20,) [ 6.   7.9 10.1  6.4 11.   7.8  7.8  6.8  7.9  6.3]\n",
      "(20, 2) (20,) [ 7.9  6.3 10.1  7.7  6.4  7.8  7.7  7.9  7.   6. ]\n",
      "(20, 2) (20,) [10.2  7.9 10.2  7.8  9.8  7.   7.9  9.   7.9  7.8]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(0)\n",
    "\n",
    "my_random_forest = MyRandomForestRegressor()\n",
    "\n",
    "for _ in range(5):\n",
    "    X_bootstrap, y_bootstrap = my_random_forest.bootstrap_sampling(X_toy, y_toy)\n",
    "    print(X_bootstrap.shape, y_bootstrap.shape, y_bootstrap[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598f4a2",
   "metadata": {},
   "source": [
    "The expected output is as follows:\n",
    "\n",
    "```\n",
    "(20, 2) (20,) [ 7.  10.2  7.8  7.9  7.9  9.5  9.   7.6  6.4  6.8]\n",
    "(20, 2) (20,) [ 9.   7.6  9.8  7.6  7.9 10.2 10.2  7.8  6.4  7.9]\n",
    "(20, 2) (20,) [ 6.   7.9 10.1  6.4 11.   7.8  7.8  6.8  7.9  6.3]\n",
    "(20, 2) (20,) [ 7.9  6.3 10.1  7.7  6.4  7.8  7.7  7.9  7.   6. ]\n",
    "(20, 2) (20,) [10.2  7.9 10.2  7.8  9.8  7.   7.9  9.   7.9  7.8]\n",
    "```\n",
    "\n",
    "Most importantly, the size of each bootstrap sample should reflect the size of the original dataset -- that is, each bootstrap sample needs to contain 20 data samples, each with two features for the toy dataset (weight and height)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ce4e3",
   "metadata": {},
   "source": [
    "#### 2.1 b) Implement Feature Sampling (2 Points)\n",
    "**Implement method `feature_sampling()`!** \n",
    "The input is a feature array `X`; to specify the number of features we use a simple approach where `max_features` specifies the ratio of features to be considered. Apart from the new dataset `X_sample` the method also returns the indices of the selected features; you will need this information when implementing the `fit()` method for training the Random Forest.\n",
    "\n",
    "Have a look at the `__init__()` method; note that `MyRandomForestRegressor` already supports the `max_features` parameter which is aligned with the [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) from scikit-learn. For the implementation of `feature_sampling()`, however, we keep it simple, and consider `max_features` as a *float* which value between 0 and 1, reflecting the ratio of features to include in the sample. For example, if `max_features=0.4`, only 40% of all features are part of the sample. If this number is not an integer, please always round up (Hint: [`np.ceil`](https://numpy.org/doc/stable/reference/generated/numpy.ceil.html)).\n",
    "\n",
    "You can use the code cell below to test your implementation. Since the toy dataset has only 2 features, feature sampling with `max_features=0.5` and lower will return only one feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dada045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2) [1 0]\n",
      "(20, 2) [0 1]\n",
      "(20, 2) [0 1]\n",
      "(20, 2) [1 0]\n",
      "(20, 2) [0 1]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(0)\n",
    "\n",
    "my_random_forest = MyRandomForestRegressor(max_features=0.7)\n",
    "\n",
    "for _ in range(5):\n",
    "    X_sample, indices_sampled = my_random_forest.feature_sampling(X_toy)\n",
    "    print(X_sample.shape, indices_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cebb0",
   "metadata": {},
   "source": [
    "The expected output is as follows:\n",
    "\n",
    "```\n",
    "(20, 1) [1]\n",
    "(20, 1) [0]\n",
    "(20, 1) [0]\n",
    "(20, 1) [1]\n",
    "(20, 1) [0]\n",
    "```\n",
    "\n",
    "Since we round up, the result will also be the same for, say `max_features=0.00001`. If you set `max_fetures` to a value larger than 0.5, all two features will be selected and each line in the output should be `(20, 2) [0 1]` or `(20, 2) [1 0]`. Values for `max_features` less or equal to `0`  or larger than `1.0` are of course invalid. But you do not need to make any checks in your implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa3e54",
   "metadata": {},
   "source": [
    "#### 2.1 c) Training the Random Forest (2 Points)\n",
    "\n",
    "With **Bootstrap Sampling (Bagging)** and **Feature Sampling** -- and of course the implementation of your Decision Tree regressor -- everything is in place to finally train the Random Forest. As we saw in the lecture, training a Random Forest is simply training multiple Decision Trees based on different sampled datasets.\n",
    "\n",
    "**Implement method `fit()` to train the Random Forest!** Have a good look at the given code snippet, and note that you need to keep track of the tuple `(regressor, indices_sampled)` for each estimator (i.e., Decision Tree). You will need `indices_sampled` later when making predictions, as you can only predict using those features that were also used during the training of a particular regressor.\n",
    "\n",
    "You can use the code cell below to test your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4c0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.1 8.1] [1]\n",
      "[7.8 9.5] [0]\n",
      "[7.8 9.5] [0]\n",
      "[8.1 8.1] [1]\n",
      "[7.8 9.5] [0]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(1)\n",
    "\n",
    "my_random_forest = MyRandomForestRegressor(n_estimators=100, max_features=0.5).fit(X_toy, y_toy)\n",
    "\n",
    "for i in range(5):\n",
    "    print(my_random_forest.estimators[i][0].predict(np.array([[73, 180], [90, 170]])), my_random_forest.estimators[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de7b46",
   "metadata": {},
   "source": [
    "The expected output is as follows:\n",
    "\n",
    "```\n",
    "[8.1 8.1] [1]\n",
    "[7.8 9.5] [0]\n",
    "[7.8 9.5] [0]\n",
    "[8.1 8.1] [1]\n",
    "[7.8 9.5] [0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f77954",
   "metadata": {},
   "source": [
    "#### 2.1 d) Predicting the Output Values (2 Points)\n",
    "\n",
    "With a trained Random Forest, all that's left is to predict the output values for new data points. We do this by using each estimator (i.e., Decision Tree) to predict the value, and then calculate the average over predictions. Again, since we can only use those features with which an individual Decision Tree was trained on, you need information about `indices_sampled` here (cf. `fit()` method).\n",
    "\n",
    "**Implement method `predict()` to predict the output values for new data points! (2 Points)** The input is represented by a feature array `X` containing all new data points. If `X` contains N data points, the result should be an array containing all N predictions.\n",
    "\n",
    "You can use the code cell below to test your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4437d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.995 10.435]\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(1)\n",
    "\n",
    "my_random_forest = MyRandomForestRegressor(n_estimators=100, max_features=1.0).fit(X_toy, y_toy)\n",
    "\n",
    "print(my_random_forest.predict(np.array([[73, 180], [90, 170]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a97287",
   "metadata": {},
   "source": [
    "The expected output is as follows:\n",
    "\n",
    "```\n",
    "[ 6.995 10.435]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e8ca2",
   "metadata": {},
   "source": [
    "#### Additional Tests (nothing for you to do here!)\n",
    "\n",
    "You have now implemented your Random Forest regressor. This means that you can now also compare your implementation with the one from scikit-learn. Due to the random sampling, it is basically not possible to ensure the same results. Although we set `np.random.seed()`, your implementation and the one from scikit-learn are too different for that to matter. To lower the effect of randomization we can \"switch off\" feature sampling by always using all features (`max_features=1.0`).\n",
    "\n",
    "Try different values for `n_estimators`. You should see that the more estimators you use the more similar the results. This shouldn't be surprising as we make the predictions as this tends to stabilize the results. Just don't go too high as your implementation is far from optimized :).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7b4f5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312.67143 560.75831 285.995   550.56497 343.265  ]\n",
      "[292.24643 560.37498 299.13    550.5833  309.775  ]\n",
      "\n",
      "CPU times: user 22.1 s, sys: 210 ms, total: 22.3 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(1)\n",
    "\n",
    "n_estimators = 100\n",
    "\n",
    "my_random_forest = MyRandomForestRegressor(n_estimators=n_estimators, max_features=1.0).fit(X_train, y_train)\n",
    "sk_random_forest = RandomForestRegressor(n_estimators=n_estimators, max_features=1.0).fit(X_train, y_train)\n",
    "\n",
    "print(my_random_forest.predict(X_test)[:5])\n",
    "print(sk_random_forest.predict(X_test)[:5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283558f0",
   "metadata": {},
   "source": [
    "For `n_estimators = 100`, it should take about 30s to run, and the expected output is as follows:\n",
    "\n",
    "```\n",
    "[312.67143 560.75831 285.995   550.56497 343.265  ]\n",
    "[292.24643 560.37498 299.13    550.5833  309.775  ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41601ba",
   "metadata": {},
   "source": [
    "### 2.2 Questions about Tree Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b353a",
   "metadata": {},
   "source": [
    "#### 2.2 a) Random Forest: Bagging Only vs. Bagging + Feature Sampling (2 Points)\n",
    "\n",
    "The code cell below trains a two series of 20 Decision Trees each. One series uses only Bagging (i.e., Bootstrap Sampling) for the training data; the other series uses both Bagging and Feature Sampling. The output shows the results for \"Bagging only\" in the left column and \"Bagging + Feature Sampling\" in the right column. `root index` represents the feature index chosen as the root node (i.e., for the first split); `#nodes` represents the total number of nodes in the trained Decision Tree.\n",
    "\n",
    "**Note:** There is nothing for you to implement here, but feel free to increase the number of Decision Trees beyond 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b19f52e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging only\t\t\t\tBagging + Feature Sampling\n",
      "#root index: 1,  #nodes: 259\t\t#root index: 1,  #nodes: 263\n",
      "#root index: 4,  #nodes: 253\t\t#root index: 0,  #nodes: 263\n",
      "#root index: 1,  #nodes: 245\t\t#root index: 2,  #nodes: 231\n",
      "#root index: 5,  #nodes: 251\t\t#root index: 2,  #nodes: 239\n",
      "#root index: 1,  #nodes: 247\t\t#root index: 5,  #nodes: 231\n",
      "#root index: 4,  #nodes: 243\t\t#root index: 1,  #nodes: 257\n",
      "#root index: 5,  #nodes: 241\t\t#root index: 1,  #nodes: 249\n",
      "#root index: 0,  #nodes: 249\t\t#root index: 5,  #nodes: 249\n",
      "#root index: 1,  #nodes: 259\t\t#root index: 1,  #nodes: 269\n",
      "#root index: 5,  #nodes: 231\t\t#root index: 0,  #nodes: 243\n",
      "#root index: 5,  #nodes: 273\t\t#root index: 5,  #nodes: 235\n",
      "#root index: 5,  #nodes: 255\t\t#root index: 5,  #nodes: 249\n",
      "#root index: 1,  #nodes: 231\t\t#root index: 2,  #nodes: 237\n",
      "#root index: 1,  #nodes: 263\t\t#root index: 1,  #nodes: 271\n",
      "#root index: 5,  #nodes: 253\t\t#root index: 5,  #nodes: 259\n",
      "#root index: 1,  #nodes: 261\t\t#root index: 1,  #nodes: 279\n",
      "#root index: 5,  #nodes: 235\t\t#root index: 1,  #nodes: 241\n",
      "#root index: 5,  #nodes: 255\t\t#root index: 5,  #nodes: 247\n",
      "#root index: 5,  #nodes: 259\t\t#root index: 5,  #nodes: 255\n",
      "#root index: 1,  #nodes: 229\t\t#root index: 4,  #nodes: 239\n"
     ]
    }
   ],
   "source": [
    "# We need to set the seed as the sampling is random, and we want to ensure consistent results\n",
    "np.random.seed(10)\n",
    "\n",
    "my_random_forest_bagging = MyRandomForestRegressor(max_features=1.0)\n",
    "my_random_forest_sampling = MyRandomForestRegressor(max_features=0.2)\n",
    "\n",
    "print(\"Bagging only\\t\\t\\t\\tBagging + Feature Sampling\")\n",
    "for _ in range(20):\n",
    "    # Create a new bootstrap sample\n",
    "    X_t, y_t = my_random_forest_bagging.bootstrap_sampling(X_train, y_train)\n",
    "    regressor_bagging = DecisionTreeRegressor().fit(X_t, y_t)\n",
    "    \n",
    "    X_t, indices_sampled = my_random_forest_sampling.feature_sampling(X_t)\n",
    "    regressor_sampling = DecisionTreeRegressor().fit(X_t, y_t)    \n",
    "    \n",
    "    # Print core features of trained Decision Tree\n",
    "    # (feature index of root node, total of number of nodes in Decision Tree)\n",
    "    print('#root index: {},  #nodes: {}\\t\\t#root index: {},  #nodes: {}'\n",
    "          .format(regressor_bagging.tree_.feature[0], regressor_bagging.tree_.node_count,\n",
    "                  indices_sampled[regressor_sampling.tree_.feature[0]], regressor_sampling.tree_.node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba8e1c",
   "metadata": {},
   "source": [
    "**Interpret the result!** Comparing the resulting series of Decision Trees when using **Bagging only** and **Bagging + Feature Sampling**, what differences do you observe in the root indices chosen between the left and right cases? Please explain your observations. (You do not need to comment on the #nodes for now)\n",
    "\n",
    "(It might be useful to remember that the size of `X_train` is 210 data samples with 8 features; note also that each [`sklearn.tree.DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) has been trained with their default parameters)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447fe56",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "1. In Bagging (B), all 8 features are used to compute the root index. In Bagging + Feature Sampling (BF), 2 features are used to compute the root index.\n",
    "    \n",
    "2. 9/20 (45%) individual trees selected the same root index irrespective of the methods, using the same boostratpped data.\n",
    "\n",
    "```\n",
    "#root index: 1,  #nodes: 259\t\t#root index: 1,  #nodes: 263\n",
    "#root index: 1,  #nodes: 259\t\t#root index: 1,  #nodes: 269\n",
    "#root index: 5,  #nodes: 273\t\t#root index: 5,  #nodes: 235\n",
    "#root index: 5,  #nodes: 255\t\t#root index: 5,  #nodes: 249\n",
    "#root index: 1,  #nodes: 263\t\t#root index: 1,  #nodes: 271\n",
    "#root index: 5,  #nodes: 253\t\t#root index: 5,  #nodes: 259\n",
    "#root index: 1,  #nodes: 261\t\t#root index: 1,  #nodes: 279\n",
    "#root index: 5,  #nodes: 255\t\t#root index: 5,  #nodes: 247\n",
    "#root index: 5,  #nodes: 259\t\t#root index: 5,  #nodes: 255\n",
    "```\n",
    "    \n",
    "3. Therefore, this means that (BF) arrived at the same decision as (B) with lesser computation for the best split (80% less computation).\n",
    "    \n",
    "4. Moreover, (B) selects root index mainly from `{0, 1, 4, 5}` after comparing all eight features. This means that these might be the Top-4 best splits in most cases. However, (BF) selects the root index from `{0, 1, 2, 4, 5}` after random sampling and comparison between resulting two features. The only difference is the addition of feature `2` in BF. This shows that BF is able to deduce the Top-4 best splits as well with extremely less computation, along with avoiding overfitting the data.\n",
    "        \n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5490faa",
   "metadata": {},
   "source": [
    "#### 2.2 b) Random Forest: Regression vs. Classification (2 Points)\n",
    "\n",
    "The markdown cell below shows a screenshot showing similar results as you have seen in 2.2 a). The only difference here is that these results stem from a classification task (and not a regression task). More specifically, the simple [IRIS](https://archive.ics.uci.edu/ml/datasets/iris) dataset was: it's small and clean, and has only numerical features. The dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant described by 4 features. This means that each Decision Tree has been trained with 150 data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7e8d6",
   "metadata": {},
   "source": [
    "<img src=\"data/a2-rf-regression-vs-classification.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3586c",
   "metadata": {},
   "source": [
    "**Interpret the result!** Again, state and explain the differences between the left and right cases, both in the root index and #nodes. Particularly, compare these results with the results from the regression task in 2.3 a). Explain why the number of nodes now varies greatly between the left and right cases, while it hardly varied for the earlier case (regression).\n",
    "\n",
    "(As in 2.2 a) each [`sklearn.tree.DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) has been trained with their default parameters)\n",
    "\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e364b98",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "1. Main root indexes are {2, 3}. However, when 0 is chosen due to feature sampling, the #nodes is extremely high since its a bad split choice (not a clean split).\n",
    "\n",
    "2. avoiding overfitting? lesser data than regression \n",
    "\n",
    "3. results are either yes or no, instead of ranges, therefore need cleaner split? \n",
    "        \n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760b761",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85e0ae",
   "metadata": {},
   "source": [
    "## 3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166fdac",
   "metadata": {},
   "source": [
    "### 3.1 Questions about Logistic Regression (8 Points)\n",
    "\n",
    "In the table below are 4 statements that are either True or False. Complete the table to specify whether a statement is True or False, and provide a brief explanation for your answer (Your explanation is more important than a simple True/False answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e14e16",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(4).\n",
    "\n",
    "| No. | Statement                                                                                                   | True or False?       | Brief Explanation |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| (1)  | If the dataset is linearly separable, logistic regression will eventually achieve a loss of 0 | <font color = 'blue'> True </font>| <font color = 'blue'> The minimization of the loss function (J) of logistic regression for a linearly separable dataset is a convex optimization problem (J has convexity), where the solution will always reach the global minima provided the learning rate is chosen appropriately, i.e., loss of 0. </font> |\n",
    "| (2)  | No matter how the parameter vector $\\theta$ is initialized, logistic regression will eventually always converge to the same solution (for the same dataset), if we train it to convergence | <font color = 'blue'> True </font> | <font color = 'blue'> Since the loss function (J) of logistic regression has convexity, it will always reach the global minima eventually, i.e., it will always have the same solution no matter what it's initialization is. However, the time taken to reach the global minima might be different. </font> |\n",
    "| (3)  | Since logistic regression considers the interaction between features, it always performs better then a (single) Decision Tree |<font color = 'blue'> False </font> | <font color = 'blue'> Logistic regression models the interaction between features by using non-linear transformations of the input variables. However, it assumes a linear relationship between the transformed input variables and the target variable. In contrast, decision trees can model non-linear relationships between the input variables and the target variable by recursively partitioning the input space based on the input features. Therefore, it is not always true that logistic regression performs better than a single decision tree. The performance of each algorithm depends on the nature of the problem and the characteristics of the data. For example, logistic regression may be better suited for problems where the relationship between the input variables and the target variable is approximately linear, while decision trees may be better suited for problems where the relationship is non-linear and can be captured by a series of simple rules. </font> |\n",
    "| (4)  | When incorporating Regularization into logistic regression, you would generally see larger training loss | <font color = 'blue'> True </font> |  <font color = 'blue'> Generalization avoids overfitting to the training dataset by penalizing the loss function at each iteration. Therefore, the training loss is generally larger. </font>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264c7fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5308410",
   "metadata": {},
   "source": [
    "## 4 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaff2cd",
   "metadata": {},
   "source": [
    "The topic \"Classification & Regression\" covered a whole series of different models. In this section, we look at the basic data mining task of finding the best model for a given dataset: which model performs best with which hyperparameters. To keep it simple and keep the implementation work to a minimum, we make full use of scikit-learn (see additional hints in the subtasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1cc7dd",
   "metadata": {},
   "source": [
    "#### Prepare Dataset\n",
    "\n",
    "#### Load Dataset from File\n",
    "\n",
    "We use a [WHO Life Expectancy](https://www.kaggle.com/kumarajarshi/life-expectancy-who) dataset for this task. Note that we cleaned the dataset for you (i.e., there are no dirty records in there)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433a87f",
   "metadata": {},
   "source": [
    "Let's load the file and have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34eba01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>under-five deaths</th>\n",
       "      <th>...</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness  1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "      <th>Life expectancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>Developing</td>\n",
       "      <td>263.0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>71.279624</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>19.1</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>8.16</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>584.259210</td>\n",
       "      <td>33736494.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.479</td>\n",
       "      <td>10.1</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>Developing</td>\n",
       "      <td>271.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.523582</td>\n",
       "      <td>62.0</td>\n",
       "      <td>492</td>\n",
       "      <td>18.6</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>8.18</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>612.696514</td>\n",
       "      <td>327582.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.476</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>Developing</td>\n",
       "      <td>268.0</td>\n",
       "      <td>66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>73.219243</td>\n",
       "      <td>64.0</td>\n",
       "      <td>430</td>\n",
       "      <td>18.1</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>8.13</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>631.744976</td>\n",
       "      <td>31731688.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.470</td>\n",
       "      <td>9.9</td>\n",
       "      <td>59.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>Developing</td>\n",
       "      <td>272.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>78.184215</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2787</td>\n",
       "      <td>17.6</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>8.52</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>669.959000</td>\n",
       "      <td>3696958.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>9.8</td>\n",
       "      <td>59.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>Developing</td>\n",
       "      <td>275.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.097109</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3013</td>\n",
       "      <td>17.2</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>7.87</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>63.537231</td>\n",
       "      <td>2978599.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.454</td>\n",
       "      <td>9.5</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year      Status  Adult Mortality  infant deaths  Alcohol  \\\n",
       "0  2015  Developing            263.0             62     0.01   \n",
       "1  2014  Developing            271.0             64     0.01   \n",
       "2  2013  Developing            268.0             66     0.01   \n",
       "3  2012  Developing            272.0             69     0.01   \n",
       "4  2011  Developing            275.0             71     0.01   \n",
       "\n",
       "   percentage expenditure  Hepatitis B  Measles   BMI  under-five deaths  ...  \\\n",
       "0               71.279624         65.0     1154  19.1                 83  ...   \n",
       "1               73.523582         62.0      492  18.6                 86  ...   \n",
       "2               73.219243         64.0      430  18.1                 89  ...   \n",
       "3               78.184215         67.0     2787  17.6                 93  ...   \n",
       "4                7.097109         68.0     3013  17.2                 97  ...   \n",
       "\n",
       "   Total expenditure  Diphtheria  HIV/AIDS         GDP  Population  \\\n",
       "0               8.16        65.0       0.1  584.259210  33736494.0   \n",
       "1               8.18        62.0       0.1  612.696514    327582.0   \n",
       "2               8.13        64.0       0.1  631.744976  31731688.0   \n",
       "3               8.52        67.0       0.1  669.959000   3696958.0   \n",
       "4               7.87        68.0       0.1   63.537231   2978599.0   \n",
       "\n",
       "   thinness  1-19 years  thinness 5-9 years  Income composition of resources  \\\n",
       "0                  17.2                17.3                            0.479   \n",
       "1                  17.5                17.5                            0.476   \n",
       "2                  17.7                17.7                            0.470   \n",
       "3                  17.9                18.0                            0.463   \n",
       "4                  18.2                18.2                            0.454   \n",
       "\n",
       "   Schooling  Life expectancy  \n",
       "0       10.1             65.0  \n",
       "1       10.0             59.9  \n",
       "2        9.9             59.9  \n",
       "3        9.8             59.5  \n",
       "4        9.5             59.2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/a2-life-expectancy-cleaned.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825696a4",
   "metadata": {},
   "source": [
    "For your convenience, we split the dataframe into two, one containing the input features, the other containing the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcda3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.iloc[:,0:-1]\n",
    "df_y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5019219",
   "metadata": {},
   "source": [
    "### 4.1 Data Preprocessing (3 Points)\n",
    "\n",
    "As usual, the first step is data preprocessing (informed by an EDA). As mentioned, above there's not much to do as this dataset does not contain any \"dirty\" records, particularly, there are no NA values in any of the columns/features. As such, there should be no need to remove any samples.\n",
    "\n",
    "**Preprocess the data:** Note that we have already imported [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/preprocessing.html). You should convert the categorical feature (`Status`) to a binary feature, and also scale the data with correct usage of `StandardScaler`. Note that we have provided **two code blocks**; for each of the operations you need to do, you should choose appropriately as to whether your operations should be done in the former or latter blocks (recalling discussion in lecture on avoiding leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc33921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ############################################################### \n",
    "labelencoder= preprocessing.LabelEncoder()\n",
    "df_X['Status'] = labelencoder.fit_transform(df_X['Status'])\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55c4defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to numpy arrays\n",
    "X, y = df_X.to_numpy(), df_y.to_numpy()\n",
    "\n",
    "# Split dataset in training and test data (20% test data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5e89352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28b5d6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1319, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0605adb",
   "metadata": {},
   "source": [
    "#### Training and Evaluating off-the-shelf Models\n",
    "\n",
    "Packages like `scikit-learn` make it almost trivial to train a variety of models with a minimum number of lines of codes. In the following code cell, we train 6 different models with their default values using the training data, and evaluate them over the test set. We use the Root Mean Squared Error (RMSE) score as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2086d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE scores for test data for all regressors\n",
      "===========================================\n",
      "KNeighborsRegressor:\t3.05\n",
      "LinearRegression:\t3.53\n",
      "DecisionTreeRegressor:\t2.69\n",
      "AdaBoostRegressor:\t3.06\n",
      "RandomForestRegressor:\t1.81\n",
      "GradientBoostingRegressor:\t2.09\n"
     ]
    }
   ],
   "source": [
    "print('RMSE scores for test data for all regressors')\n",
    "print('===========================================')\n",
    "for model in [KNeighborsRegressor(), LinearRegression(), DecisionTreeRegressor(),\n",
    "              AdaBoostRegressor(), RandomForestRegressor(), GradientBoostingRegressor()]:    \n",
    "    try:\n",
    "        regressor = model.fit(X_train, y_train)\n",
    "        # Predict values for test samples\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Calculate the RMSE\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # Handle exception (e.g., a regressor is still None)\n",
    "        rmse = '---'\n",
    "    # Print regressor name and the RMSE score\n",
    "    print('{}:\\t{:.3}'.format(type(regressor).__name__, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06e0a4",
   "metadata": {},
   "source": [
    "### 4.2 Performing K-Fold Cross-Validation \"By Hand\"\n",
    "\n",
    "The code below shows the basic loop for an evaluation using k-fold cross-validation. The only bits missing are the steps to (a) create the k folds and (b) to construct the training set of (k-1) folds and the validation set of 1 fold. For this task, use the `DecisionTreeRegressor` by default as it is the fastest to evaluate. \n",
    "\n",
    "#### 4.2 a) Implement k-fold Cross Validation (4 Points)\n",
    "For testing and debugging, feel free to reduce `num_folds` (e.g., 5) and `param_choices` (e.g., `[1, 2, 3]`) in the beginning, as shown in the comments.\n",
    "\n",
    "(Hint: Have a look at [`np.array_split`](https://numpy.org/doc/stable/reference/generated/numpy.array_split.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785cd0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_folds = 5\n",
    "param_choices = [1, 2, 3, 5]\n",
    "\n",
    "X_train_folds = []\n",
    "y_train_folds = []\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "# Hint: you can use np.array_split to split the sample indices into folds here\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "param_to_scores = {}\n",
    "\n",
    "for param in param_choices:\n",
    "    \n",
    "    ## We want to keep track of the training scores and validation scores\n",
    "    rmse_train, rmse_valid = [], []\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        X_train_fold, X_valid_fold = None, None\n",
    "        y_train_fold, y_valid_fold = None, None\n",
    "\n",
    "        #########################################################################################\n",
    "        ### Your code starts here ###############################################################\n",
    "        \n",
    "        # Hint: consider the np.setdiff1d function to construct the training folds here (optional; \n",
    "        # other ways are fine too)\n",
    "        \n",
    "        ### Your code ends here #################################################################\n",
    "        #########################################################################################           \n",
    "\n",
    "        \n",
    "        ## Train all the regressors one-by-one and discuss the results\n",
    "#         regressor = KNeighborsRegressor(n_neighbors=param).fit(X_train_fold, y_train_fold)\n",
    "        regressor = DecisionTreeRegressor(max_depth=param).fit(X_train_fold, y_train_fold)\n",
    "        #regressor = RandomForestRegressor(max_depth=param).fit(X_train_fold, y_train_fold)\n",
    "        #regressor = GradientBoostingRegressor(max_depth=param).fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        \n",
    "        ## Predict labels for for training validation set\n",
    "        y_pred_fold_train = regressor.predict(X_train_fold)\n",
    "        y_pred_fold_valid = regressor.predict(X_valid_fold)\n",
    "      \n",
    "        ## Keep track of training and validation scores\n",
    "        rmse_train.append(mean_squared_error(y_train_fold, y_pred_fold_train, squared=False))\n",
    "        rmse_valid.append(mean_squared_error(y_valid_fold, y_pred_fold_valid, squared=False))\n",
    "        \n",
    "    ## Keep track of all num_folds scores for current param (for plotting)\n",
    "    param_to_scores[param] = (rmse_train, rmse_valid)\n",
    "    \n",
    "    ## Print statement for some immediate feedback\n",
    "    print('param = {}, RMSE (training) = {:.3f}, RMSE (validation) = {:.3f} (stdev: {:.3f})'.format(param, np.mean(rmse_train), np.mean(rmse_valid), np.std(rmse_valid)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb9583",
   "metadata": {},
   "source": [
    "If correct, your answers (for the debug case) should look like this:\n",
    "```\n",
    "param = 1, RMSE (training) = 0.000, RMSE (validation) = 3.101 (stdev: 0.240)\n",
    "param = 2, RMSE (training) = 1.581, RMSE (validation) = 2.941 (stdev: 0.211)\n",
    "param = 3, RMSE (training) = 1.998, RMSE (validation) = 2.982 (stdev: 0.107)\n",
    "param = 5, RMSE (training) = 2.463, RMSE (validation) = 3.112 (stdev: 0.091)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861f476",
   "metadata": {},
   "source": [
    "#### Visualization of Results\n",
    "\n",
    "We provide you with 2 methods to visualize the results. `plot_validation_results()` shows all `num_folds` scores for each parameter setting together with the means and standard deviations of the validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75571983",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_results(param_to_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc6d09",
   "metadata": {},
   "source": [
    "The method `plot_scores()` shows the training and validation scores for each parameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f45988",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(param_to_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9844e",
   "metadata": {},
   "source": [
    "The code above for the k-fold cross-validation already contains the lines for the training of 4 different regressors:\n",
    "\n",
    "* [`KNeighborsRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) with `n_neighbors` as hyperparameter\n",
    "\n",
    "* [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) with `max_depth` as hyperparameter\n",
    "\n",
    "* [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) with `max_depth` as hyperparameter\n",
    "\n",
    "* [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) with `max_depth` as hyperparameter\n",
    "\n",
    "We focus on these 4 models since we look at only one hyperparameter and consider only integer values. This `n_neighbors` (for KNN) and `max_depth` for the tree-based models are important hyperparameters (assume integer values).\n",
    "\n",
    "\n",
    "#### 4.2 b) Run k-fold Cross Validation for 4 Regressors and Discuss the Results. (6 Points)¶\n",
    "\n",
    "Based on the plots, 1) compare the methods in terms of validation performance, and explain your findings; 2) explain how the hyperparameters of each method relate to overfitting and underfitting. Hint: Use the methods `plot_scores()` and `plot_validation_results` to visualize the results; save the 2 plots for each regressor as images so you can easily compare them side by side (there's no need to submit the images later!)\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f65903",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "(Your answer here)\n",
    "        \n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca12daa",
   "metadata": {},
   "source": [
    "### 4.3 Hyperparameter Tuning (3 Points)\n",
    "\n",
    "The results of the different models in off-the-shelf implementations vary quite a bit, but of course, we used the only default parameters of each implementation which might or might not be good for our dataset and task. In practice, you would perform hyperparameter tuning for all or at least most models. However, this is unnecessarily since the tuning process is very similar for each model. So we do it only for one model: **GradientBoostingRegressor**, which is very similar to the boosting approach discussed in lecture.\n",
    "\n",
    "**Important hints:**\n",
    "\n",
    "* Use [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)! It automatically performs k-fold cross-validation (by default: k=5, which is fine) for all specified combinations of hyperparameter values. With [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), finding the best model (i.e., the model with the best hyperparameter models) should only require only very few lines of code!\n",
    "* For all other arguments of [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) (other than the model and the parameter grid), you can use the defaults.\n",
    "* [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) with (`base_estimator=DecisionTreeRegressor()`) provides a whole range of hyperparameters. Pick **2-3 hyperparameters** to tune the model (any choice of hyperparameters and grid values is fine; feel free to pick them based on your preference).\n",
    "* Setting a large grid (e.g. more than 10 parameter combinations) may cause `GridSearchCV` to run slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = None\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "######################################################################################### \n",
    "\n",
    "# Store the parameters of the best model\n",
    "best_params = model.best_params_\n",
    "\n",
    "# Predict class labels of test data on the model with the best found parameters\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "best_mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print('Best Gradient Boosting regressor: {} (RMSE: {:.3f})'.format(best_params, best_mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
