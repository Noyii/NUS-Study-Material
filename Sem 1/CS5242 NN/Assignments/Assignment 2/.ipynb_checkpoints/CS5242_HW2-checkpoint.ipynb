{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXMX8yuAyl55"
   },
   "source": [
    "# Welcome to CS 5242 **Homework 2**\n",
    "\n",
    "ASSIGNMENT DEADLINE â° : **05 Sept 2022** \n",
    "\n",
    "In this assignment, the task is to implement Multi-Layer Perceptron (MLP) for handwritten digit classification from scratch.\n",
    "\n",
    "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
    "\n",
    "> In this assignment, there is no need to use GPU.\n",
    "\n",
    "### **Grades Policy**\n",
    "\n",
    "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
    "\n",
    "### **Cautions**\n",
    "\n",
    "**DO NOT** use external libraries like PyTorch or TensorFlow in your implementation.\n",
    "\n",
    "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "### **Contact**\n",
    "\n",
    "Please feel free to contact us if you have any question about this homework or need any further information.\n",
    "\n",
    "Slack (Recommend): Shenggan Cheng\n",
    "\n",
    "TA Email: shenggan@comp.nus.edu.sg\n",
    "\n",
    "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242ay20222-oiw1784/shared_invite/zt-1eiv24k1t-0J9EI7vz3uQmAHa68qU0aw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLeZHcOVBp4U"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Start by running the cell below to set up all required software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIgu_q2HBg-E",
    "outputId": "c92ee59b-7279-4014-ecb2-d1daa719a6f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqkIbfgfE1oB"
   },
   "source": [
    "Download and extract MNIST Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSoSnxC4EzzJ",
    "outputId": "d331fc01-0d84-4b9c-fffe-9ef188339b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 124872\n",
      "-rw-r--r-- 1 1000 1000  18289443 Jul 15  2019 mnist_test.csv\n",
      "-rw-r--r-- 1 1000 1000 109575994 Jul 15  2019 mnist_train.csv\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data \\\n",
    "    && wget -q https://raw.githubusercontent.com/Shenggan/cs5242_resource/main/homework2/mnist_data.tar.gz -O ./data/mnist_data.tar.gz \\\n",
    "    && tar xf data/mnist_data.tar.gz -C ./data \\\n",
    "    && rm -rf ./data/mnist_data.tar.gz \\\n",
    "    && ls -l ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtXcchT5H2PH"
   },
   "source": [
    "Import the neccesary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I2Yodsn4H6CB"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yrZD7DDExF4"
   },
   "source": [
    "Everything is ready, you can move on and ***Good Luck !*** ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DwqHhL9CSxs"
   },
   "source": [
    "## Explore MNIST Dataset\n",
    "\n",
    "One of the first steps when working with a new data set is exploring.Our data here is from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of `28x28` pixel images of handwritten digits from `0-9`, which has an important place in the history of machine learning.\n",
    "\n",
    "Our data have been downloaded in the directory `data`: `mnist_train.csv` and `mnist_test.csv`, so the complete file path (from the main directory) are `data/mnist_train.csv` and `data/mnist_test.csv`. \n",
    "\n",
    "With that in mind, let's define a helper function `load_data` that we can use to quickly in our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DIHIpxpaHyOo"
   },
   "outputs": [],
   "source": [
    "def load_data(dir_name):\n",
    "    \"\"\"\n",
    "    Function for loading MNIST data stored in comma delimited files. Labels for \n",
    "    each image are the first entry in each row.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dit_name : str\n",
    "         Path to where data is contained\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.array, np.float32\n",
    "        A (N x 784) matrix of samples\n",
    "    Y : np.array, np.float32\n",
    "        A (N x 1) matrix of labels for each sample\n",
    "    \"\"\"\n",
    "    X, Y = None, None\n",
    "\n",
    "    # Hint:\n",
    "    # 1) open the file and iterate through each line\n",
    "    # 2) in every line, split with ',' and you can get 785 elements\n",
    "    # 3) as the first number in each sample is the label (0-9), extract that from the rest and return\n",
    "\n",
    "    # Some numpy function you maybe need to use: `np.array`, `numpy.ndarray.astype`, `numpy.asarray`\n",
    "    # A kindly example, \n",
    "    # tmp_str = \"1,2,3,4\"\n",
    "    # array = np.array(line.split(',')).astype(np.float32)\n",
    "    # print(array)\n",
    "\n",
    "    # === Complete the code (1')\n",
    "    X_train, Y_train = [], []\n",
    "    data = open(dir_name)\n",
    "\n",
    "    for row in data:\n",
    "      array = np.array(row.split(',')).astype(np.float32)\n",
    "      Y_train.append(array[0])\n",
    "      X_train.append(array[1:])\n",
    "\n",
    "    X = np.array(X_train)\n",
    "    Y = np.array(Y_train)\n",
    "    # === Complete the code\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m4HSdfdqIPeN"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = load_data(\"./data/mnist_train.csv\")\n",
    "X_test, Y_test = load_data(\"./data/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aH3JKg3I2bZ"
   },
   "source": [
    "An easy to quickly get an idea for how your data looks is to examine the shape of the matrix it's stored using the `.shape` attribute of numpy arrays. We see that the shape of `X_train` is `60000 x 784`, which tells us there are `60000` samples (images) each with dimension `784`. Each sample, typically presented as a 28 x 28 image, is unrolled into a 1-dimensional vector 28 x 28 = 784 contained within each row of `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNCf79ShI6H9",
    "outputId": "e0c1629c-bdcc-4362-d414-a0666c8f3e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is: 60000 x 784\n",
      "The shape of the test set is: 10000 x 784\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the training set is: {X_train.shape[0]} x {X_train.shape[1]}\")\n",
    "print(f\"The shape of the test set is: {X_test.shape[0]} x {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf5GmYOTzvVI"
   },
   "source": [
    "Now lets take a look at how the samples are represented, we can do this by calling `Y_train[index]` and `X_train[index]` (here I choose `index=0` to look at the very first sample). We first notice `Y_train[0]=5.0`, meaning this entry is the digit `5`. We will confirm this shortly by visualizing some of these samples. We then notice each entry is an integer (cast into `np.float32` in our `load_data` function) ranging from `0-255`. This representation is common when working with images. The numerical entries are interpreted as pixel intensities typically shown in gray-scale ranging between `0` (black) and `255` (white). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZWVJHTXzzp2",
    "outputId": "13a6af35-837b-4a18-e5f3-1f9753b4c72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train[0]: 5.0\n",
      "X_train[0] range from 0.0 to 255.0.\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(f\"Y_train[{index}]: {Y_train[index]}\")\n",
    "print(f\"X_train[{index}] range from {min(X_train[index])} to {max(X_train[index])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b6KGKdP7JCmc"
   },
   "outputs": [],
   "source": [
    "# rescale data between 0 - 1.0\n",
    "def scale_data(X):\n",
    "    X_scaled = None\n",
    "    # === Complete the code (0.5')\n",
    "    X_scaled = X/255.0\n",
    "    # === Complete the code\n",
    "    return X_scaled\n",
    "\n",
    "X_train = scale_data(X_train)\n",
    "X_test = scale_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "2G0EUVRs2DCh",
    "outputId": "ca09a992-ef6a-41d4-a0e4-a595fc4f80c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_onehot = np.zeros((Y_train.size, int(Y_train.max()) + 1))\n",
    "y_test_onehot = np.zeros((Y_test.size, int(Y_test.max()) + 1))\n",
    "\n",
    "# one-hot encode train (y_train) and test (y_test) set labels\n",
    "# === Complete the code (0.5')\n",
    "y_train_onehot[np.arange(Y_train.size), Y_train.astype(int)] = 1\n",
    "y_test_onehot[np.arange(Y_test.size), Y_test.astype(int)] = 1\n",
    "# === Complete the code\n",
    "\n",
    "num_images = 4\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(15, 10))\n",
    "for image, label, ax in zip(X_train[:num_images], y_train_onehot[:num_images], axes):\n",
    "    ax.imshow(image.reshape(28, 28), cmap='gray', vmin=0, vmax=1.0)\n",
    "    ax.set_title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTHgDB8MCguR"
   },
   "source": [
    "## Implementation of Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS0HX6A_o5mL"
   },
   "source": [
    "We'll first go through and write the code for each piece of an MLP (**without bias**) in generic Python functions. We'll then wrap everything in an `MLP` class, which will allow us to easily access all the MLP functionality in a user friendly manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Wa7cQBpEql"
   },
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7V2F14ppBdi"
   },
   "source": [
    "Implementing the forward pass of the neural network is relatively simply as everything is basically just a series of matrix multiplicaitons and activation functions. A two-layer MLP example we looked at when working through the math had a forward pass which mathematically took the form:\n",
    "$$\\sigma(\\mathbf{x}W^{(1)})W^{(2)}$$\n",
    "\n",
    "We'll of course want to make things a bit more modular by allowing the user to choose some of the neural network paramters, such as the number of hidden layers `L` and the nunber of neurons per layer `N_l`. For simplicity we'll restrict that all the hidden layers are treated with **sigmoid** activaitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nFN2g3MrXWw"
   },
   "source": [
    "The last step of process for classification tasks is actually producing a prediction from these numbers in the output layer. Typically, in the case of multi-label classification, this is done using a *softmax* activation function which effectively converts the output neurons into probabilities for each label. This has the mathematical form, $$\\text{softmax}(\\mathbf{z})_j = \\frac{\\exp(z_j)}{\\sum_{k=0}^{K=9}\\exp{(z_k)}}$$.\n",
    "\n",
    "The softmax activation function has the property of the outputs summing to 1 $\\sum_{k=1}^{K=9}\\text{softmax}(\\mathbf{z}^{(2)})_k = 1$, allowing each output $\\mathbf{a}^{(2)}_i = \\text{softmax}(\\mathbf{z}^{(2)})_i$ to be interpreted as the probability that the input is actually a digit `0-9`. Note, the output of a MLP does not need to have a softmax activation, for example in a regression setting a softmax activation would not make much sense. When evaluating the classification accuracy of the neural network, the input is typically classified according to the output label with the highest probability, \n",
    "\n",
    "$$\\text{prediction}(\\mathbf{x}) = \\text{argmax}\\  \\text{softmax}(\\mathbf{z}^{(2)}) = \\text{argmax}\\  \\text{softmax}((\\sigma(\\mathbf{x}W^{(1)})W^{(2)}))$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF57FyDHJ2zl"
   },
   "source": [
    "We can start by simply defining our activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eRZKTMJqJ2LQ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of `x`, calculated element-wise\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float or array_like\n",
    "        input\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid(x) : float or array_like\n",
    "        sigmoid applied to `x` element-wise\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    # === Complete the code (0.5')\n",
    "    result = 1 / (1 + np.exp(-x))\n",
    "    # === Complete the code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FAhoW7rfI1s-"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of `x`,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        (N x dim) array with N samples by p dimensions. dim=10 for MNIST classification. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    softmax(x) : float or array_like\n",
    "        softmax applied to `x` along the first axis.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    # === Complete the code (0.5')\n",
    "    result = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    # === Complete the code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJO2LaDmsmXq"
   },
   "source": [
    "The inputs `x`, weight matricies `w`, and activations are in principle all we need to define the forward pass; however, for efficency reasons we'll want to store the outputs of the hidden layer neurons when performing the forward pass. Storing these values will help us later more quickly calculate the gradients during the backward pass. The `init_layers` functions will initalize these hidden layers as NumPy arrays, doing this before we begin training will help us save some overhead we would otherwise inccur reinitalizing these hidden layers before each forward pass. These hidden layer values will be stored in multi-dimensional matricies, called *tensors*. One dimension of these tensors will be the `batch size` which will indicate the number of samples simultaneously passed to MLP during one training loop (feed forward + backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0M-gOFpTsl3N"
   },
   "outputs": [],
   "source": [
    "def init_layers(batch_size, layer_sizes):\n",
    "    \"\"\"\n",
    "    Initalize arrays to store the hidden layer ouputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        Number of samples to concurrently feed through the network.\n",
    "    layer_sizes : array_like\n",
    "        Array of length `N_l`. Each entry is the number of neurons in each layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hidden_layers : list\n",
    "        List of empty arrays used to hold hidden layer outputs. \n",
    "    \"\"\"\n",
    "    hidden_layers = [np.empty((batch_size, layer_size)) for layer_size in layer_sizes]\n",
    "    return hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiNL3Ubcsq8N"
   },
   "source": [
    "In order to perform a forward pass our input `x` is consequtively multiplied by weight matricies passed into the associated activaiton functions. The paramters in these weight matricies will ultimately be learned through backpropagation, but each weight matrix must first be initalized to random values. There are a number of different methods for doing this initalization, but for the moment we'll use a simple approach of just drawing the numerical values from a normal distribution with mean zero and standard deviation 1. We could have also reasonably choosen to simply draw from a uniform distribution on the range `[-1,1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2ojQ3Hl1svff"
   },
   "outputs": [],
   "source": [
    "def init_weights(layer_sizes):\n",
    "    \"\"\"\n",
    "    Initalize the paramters of the weight matricies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_sizes : array_like\n",
    "        Array of length `N_l`. Each entry is the number of neurons in each layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : array_like\n",
    "        Randomly initalized weight matricies based on the layer sizes. \n",
    "    \"\"\"\n",
    "    weights = list()\n",
    "    for i in range(layer_sizes.shape[0] - 1):\n",
    "        weights.append(np.random.uniform(-1, 1, size=[layer_sizes[i], layer_sizes[i + 1]]))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHnQDrnMpSFL"
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7ek23WJOGHs"
   },
   "source": [
    "Initalizing all the weights, layers, and activations prior to the forward pass makes much of the backward pass implimentation actually quite simple. For convinience we'll define a `sigmoid_derivative` function, which simply computes the derivative of the sigmoid activation $\\sigma^{\\prime}$. We'll use this when computing the gradients during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZxfTWU_6oeE7"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(sigmoid_out):\n",
    "    \"\"\"\n",
    "    Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigmoid_out : array_like\n",
    "        Output values processed by a sigmoid function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid_derivative(h) : array_like\n",
    "        Derivative of sigmoid, based on value of sigmoid.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    # === Complete the code (0.5')\n",
    "    result = sigmoid_out * (1-sigmoid_out)\n",
    "    # === Complete the code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9XZokeNpnUf"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA9w06GhtWxt"
   },
   "source": [
    "You have **three tasks** in this section.\n",
    "\n",
    "1. You need to implement `__feed_forward` function.\n",
    "2. You need to implement `__back_prop` function.\n",
    "3. You need to implement the main loop of `train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_-ERNpYWpYc4"
   },
   "outputs": [],
   "source": [
    "def to_categorical(x, class_num):\n",
    "    # Transform probabilities into categorical predictions row-wise, by simply taking the max probability\n",
    "    categorical = np.zeros((x.shape[0], class_num))\n",
    "    categorical[np.arange(x.shape[0]), x.argmax(axis=1)] = 1\n",
    "    return categorical\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    # Compute the accuracy along the rows, averaging along the number of samples\n",
    "    return np.all(y_pred == y, axis=1).mean()\n",
    "\n",
    "def loss(y_pred, y):\n",
    "    # Compute the loss along the rows, averaging along the number of samples\n",
    "    loss_ = ((-np.log(y_pred)) * y).sum(axis=1).mean()\n",
    "    return loss_\n",
    "\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, X, Y, X_val, Y_val, L=1, N_l=128):\n",
    "        self.X = X\n",
    "        self.Y = np.squeeze(np.eye(10)[Y.astype(int).reshape(-1)])\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = np.squeeze(np.eye(10)[Y_val.astype(int).reshape(-1)])\n",
    "        self.L = L\n",
    "        self.N_l = N_l\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes = np.array([self.X.shape[1]] + [N_l] * L + [self.Y.shape[1]])\n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [\n",
    "            self.train_loss, self.train_acc, self.val_loss, self.val_acc, self.train_time,\n",
    "            self.tot_time\n",
    "        ]\n",
    "\n",
    "        self.class_num = self.Y.shape[1]\n",
    "\n",
    "    def __init_weights(self):\n",
    "        # Initialize the weights of the network given the sizes of the layers\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0] - 1):\n",
    "            self.weights.append(\n",
    "                np.random.uniform(-1, 1, size=[self.layer_sizes[i], self.layer_sizes[i + 1]]))\n",
    "        self.weights = np.asarray(self.weights)\n",
    "\n",
    "    def __init_layers(self, batch_size):\n",
    "        # Initialize and allocate arrays for the hidden layer activations\n",
    "        self.__h = [np.empty((batch_size, layer)) for layer in self.layer_sizes]\n",
    "\n",
    "    def __feed_forward(self, batch):\n",
    "        # Perform a forward pass of `batch` samples (N_samples x N_features)\n",
    "        out_ = None\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        \n",
    "        # === Complete the code (1.5')\n",
    "        for i in range(len(self.weights) - 1):\n",
    "          h_l = sigmoid(np.matmul(h_l, self.weights[i]))\n",
    "          self.__h[i+1] = h_l\n",
    "\n",
    "        # Doing this outside the loop since we don't want sigmoid for last linear layer\n",
    "        # y = softmax( sigmoid(Xw1) * w2) --> if L=1\n",
    "        h_l = np.matmul(h_l, self.weights[-1])\n",
    "        self.__h[-1] = softmax(h_l)\n",
    "        out_ = self.__h[-1]\n",
    "        # === Complete the code\n",
    "        self.__out = out_\n",
    "\n",
    "    def __back_prop(self, batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        # === Complete the code (1.5')\n",
    "\n",
    "          ### Suppose we have 1 hidden layer\n",
    "          # p = softmax( sigm(Xw0).w1 )\n",
    "          # z = sigm(Xw0).w1\n",
    "          # m = Xw0\n",
    "\n",
    "          # Thus, \n",
    "          # p = softmax(z)\n",
    "          # z = m.w1\n",
    "\n",
    "          # L = -y.log(p)\n",
    "          # dL/dw1 = dL/dp * dp/dz * dz/dw1\n",
    "          #        = (p - y) * dz/dw1\n",
    "          #        = (p - y) * sigm(Xw0)\n",
    "          #        = (p - y) * h_l[1]\n",
    "          # dL/dw0 = dL/dp * dp/dz * dz/dm * dm/dw0\n",
    "          #        = (p - y) * w1 * sigm_deriv(Xw0) * X\n",
    "          #        = (p - y) * w1 * sigm_deriv(h_l[1]) * h_l[0]\n",
    "\n",
    "        loss_gradient = self.__out - batch_y\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "          self.weights[-i-1] -= self.lr * np.matmul(self.__h[-i-2].transpose(), loss_gradient) / self.batch_size\n",
    "          loss_gradient = np.matmul(loss_gradient, self.weights[-i-1].transpose()) * sigmoid_derivative(self.__h[-i-2])     \n",
    "        # === Complete the code\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Generate a categorical, one-hot, prediction given an input X\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return to_categorical(self.__out, self.class_num)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
    "        prediction = self.predict(X)\n",
    "        return accuracy(prediction, Y)\n",
    "\n",
    "    def train(self, batch_size=8, epochs=25, lr=1.0):\n",
    "        # Train the model with a given batch size, epochs, and learning rate. Store and print relevant metrics.\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle], self.n_samples / self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle], self.n_samples / self.batch_size)\n",
    "            for batch_x, batch_y in zip(X_batches, Y_batches):\n",
    "                # === Complete the code (1.5')\n",
    "                self.__feed_forward(batch_x)\n",
    "                train_loss += loss(self.__out, batch_y)\n",
    "                train_acc += accuracy(to_categorical(self.__out, self.class_num), batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                # === Complete the code\n",
    "\n",
    "            train_loss = (train_loss / len(X_batches))\n",
    "            train_acc = (train_acc / len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "\n",
    "            train_time = round(time.time() - start, 3)\n",
    "            self.train_time.append(train_time)\n",
    "\n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = loss(self.__out, self.Y_val)\n",
    "            val_acc = accuracy(to_categorical(self.__out, self.class_num), self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "\n",
    "            tot_time = round(time.time() - start, 3)\n",
    "            self.tot_time.append(tot_time)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrUiDpPynrLF"
   },
   "source": [
    "Great, now let's give this a try. Let's create a really simple MLP with only a single hidden layer `L=2` with 64 neurons `N_l=64`. We'll train with a `batch_size=8` for `epochs=20` and a learning rate `lr=0.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkkmE_mQpb4j",
    "outputId": "36c6b7de-acd4-4923-8812-b170f5a3c4cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.398 | acc = 0.875 | val_loss = 0.288 | val_acc = 0.918 | train_time = 7.449 | tot_time = 7.561\n",
      "Epoch 2: loss = 0.198 | acc = 0.94 | val_loss = 0.175 | val_acc = 0.948 | train_time = 6.389 | tot_time = 6.496\n",
      "Epoch 3: loss = 0.148 | acc = 0.955 | val_loss = 0.148 | val_acc = 0.956 | train_time = 6.415 | tot_time = 6.515\n",
      "Epoch 4: loss = 0.122 | acc = 0.962 | val_loss = 0.139 | val_acc = 0.957 | train_time = 6.446 | tot_time = 6.541\n",
      "Epoch 5: loss = 0.104 | acc = 0.968 | val_loss = 0.163 | val_acc = 0.951 | train_time = 6.316 | tot_time = 6.414\n",
      "Epoch 6: loss = 0.088 | acc = 0.972 | val_loss = 0.135 | val_acc = 0.961 | train_time = 6.135 | tot_time = 6.231\n",
      "Epoch 7: loss = 0.078 | acc = 0.975 | val_loss = 0.122 | val_acc = 0.964 | train_time = 6.123 | tot_time = 6.233\n",
      "Epoch 8: loss = 0.068 | acc = 0.978 | val_loss = 0.121 | val_acc = 0.966 | train_time = 7.234 | tot_time = 7.332\n",
      "Epoch 9: loss = 0.057 | acc = 0.982 | val_loss = 0.124 | val_acc = 0.967 | train_time = 6.183 | tot_time = 6.281\n",
      "Epoch 10: loss = 0.052 | acc = 0.983 | val_loss = 0.136 | val_acc = 0.964 | train_time = 6.233 | tot_time = 6.331\n",
      "Epoch 11: loss = 0.045 | acc = 0.986 | val_loss = 0.125 | val_acc = 0.966 | train_time = 7.22 | tot_time = 7.317\n",
      "Epoch 12: loss = 0.041 | acc = 0.987 | val_loss = 0.138 | val_acc = 0.963 | train_time = 6.127 | tot_time = 6.223\n",
      "Epoch 13: loss = 0.035 | acc = 0.989 | val_loss = 0.125 | val_acc = 0.969 | train_time = 6.148 | tot_time = 6.261\n",
      "Epoch 14: loss = 0.033 | acc = 0.989 | val_loss = 0.133 | val_acc = 0.967 | train_time = 6.176 | tot_time = 6.272\n",
      "Epoch 15: loss = 0.027 | acc = 0.991 | val_loss = 0.14 | val_acc = 0.966 | train_time = 6.067 | tot_time = 6.166\n",
      "Epoch 16: loss = 0.024 | acc = 0.992 | val_loss = 0.131 | val_acc = 0.968 | train_time = 6.063 | tot_time = 6.157\n",
      "Epoch 17: loss = 0.022 | acc = 0.993 | val_loss = 0.131 | val_acc = 0.969 | train_time = 6.227 | tot_time = 6.325\n",
      "Epoch 18: loss = 0.019 | acc = 0.994 | val_loss = 0.134 | val_acc = 0.969 | train_time = 6.115 | tot_time = 6.211\n",
      "Epoch 19: loss = 0.016 | acc = 0.996 | val_loss = 0.142 | val_acc = 0.966 | train_time = 6.204 | tot_time = 6.305\n",
      "Epoch 20: loss = 0.013 | acc = 0.996 | val_loss = 0.148 | val_acc = 0.966 | train_time = 6.064 | tot_time = 6.16\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X_train, Y_train, X_test, Y_test, L=2, N_l=64)\n",
    "model.train(batch_size=8, epochs=20, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "b8IawSzBSRNI",
    "outputId": "dd06ff02-954c-47bf-baf6-9ec8730db0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(model.train_loss,label=\"Train loss\")\n",
    "ax[0].plot(model.val_loss,label=\"Val loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "\n",
    "ax[1].plot(model.train_acc,label=\"Train acc\")\n",
    "ax[1].plot(model.val_acc,label=\"Val acc\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9rgEKrTCseN"
   },
   "source": [
    "## Discussion (2 points)\n",
    "\n",
    "In this section, you are free to choose some of the following directions to explore, and try to summarize some patterns and conclusions.\n",
    "\n",
    "* Try to use different learning rates and generalize the impact of learning rate on training.\n",
    "* Try to use different number of layers of MLPs or different number of neurons of MLPs (including different ways of weight initialization) and explore the impact on the final classification performance.\n",
    "* Try to use a portion of the data for training (e.g., 10%, 20%, 50%) to explore the training convergence and final classification performance with different amounts of data.\n",
    "\n",
    "> **NOTE:** *Good Disscusion* include experimental setup, presentation of experimental results including visualization, analysis and interpretation of phenomena, and summary of conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Analysis\n",
    "\n",
    "From the graph of Loss vs Epoch, we notice that the validation loss decreases with decrease in training loss until ~epoch 8. After this, both of them start to diverge. This means that after epoch 8, the model started to overfit on the training data and hence generalization of the model was lost, resulting in greater loss values for the validation data.\n",
    "\n",
    "Thus, we will take optimum no. of epochs to be 8 for further analysis below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pF4oSHgumbT"
   },
   "source": [
    "## 1. Change learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IpuUw__XX2BE"
   },
   "outputs": [],
   "source": [
    "# === Complete the code (2')\n",
    "import math\n",
    "\n",
    "def optimize_lr(max_lr=1):\n",
    "  step_size = 0.1\n",
    "  learning_rate = 0.1 ## Initial rate\n",
    "  val_loss_vector = []\n",
    "  train_loss_vector = []\n",
    "  lr_vector = []\n",
    "\n",
    "  # keeping num_epochs constant\n",
    "  while learning_rate < max_lr:\n",
    "    model = MLP(X_train, Y_train, X_test, Y_test, L=2, N_l=64)\n",
    "    model.train(batch_size=8, epochs=8, lr=learning_rate)\n",
    "    \n",
    "    if model.val_loss[-1] == float('inf') or math.isnan(model.val_loss[-1]):\n",
    "      break\n",
    "\n",
    "    lr_vector.append(learning_rate)\n",
    "    val_loss_vector.append(model.val_loss[-1])\n",
    "    train_loss_vector.append(model.train_loss[-1])\n",
    "    learning_rate += step_size # Increase rate linearly for each iteration\n",
    "\n",
    "    print(\"\\n\\n ------------------------------------------- \\n\\n\")\n",
    "\n",
    "  return lr_vector, val_loss_vector, train_loss_vector\n",
    "# === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZvJdbBGExV8G",
    "outputId": "2037e723-8942-4473-9489-bdeef5dd3095"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.475 | acc = 0.853 | val_loss = 0.273 | val_acc = 0.918 | train_time = 6.182 | tot_time = 6.281\n",
      "Epoch 2: loss = 0.243 | acc = 0.928 | val_loss = 0.211 | val_acc = 0.937 | train_time = 6.188 | tot_time = 6.288\n",
      "Epoch 3: loss = 0.191 | acc = 0.943 | val_loss = 0.203 | val_acc = 0.938 | train_time = 6.129 | tot_time = 6.227\n",
      "Epoch 4: loss = 0.159 | acc = 0.952 | val_loss = 0.166 | val_acc = 0.95 | train_time = 6.105 | tot_time = 6.205\n",
      "Epoch 5: loss = 0.138 | acc = 0.959 | val_loss = 0.157 | val_acc = 0.952 | train_time = 6.071 | tot_time = 6.17\n",
      "Epoch 6: loss = 0.121 | acc = 0.964 | val_loss = 0.148 | val_acc = 0.956 | train_time = 6.154 | tot_time = 6.264\n",
      "Epoch 7: loss = 0.108 | acc = 0.967 | val_loss = 0.137 | val_acc = 0.961 | train_time = 6.131 | tot_time = 6.231\n",
      "Epoch 8: loss = 0.097 | acc = 0.971 | val_loss = 0.133 | val_acc = 0.962 | train_time = 6.119 | tot_time = 6.23\n",
      "Epoch 9: loss = 0.088 | acc = 0.973 | val_loss = 0.127 | val_acc = 0.964 | train_time = 6.108 | tot_time = 6.205\n",
      "Epoch 10: loss = 0.08 | acc = 0.976 | val_loss = 0.125 | val_acc = 0.964 | train_time = 6.129 | tot_time = 6.229\n",
      "Epoch 11: loss = 0.074 | acc = 0.977 | val_loss = 0.131 | val_acc = 0.96 | train_time = 6.089 | tot_time = 6.188\n",
      "Epoch 12: loss = 0.067 | acc = 0.98 | val_loss = 0.124 | val_acc = 0.964 | train_time = 6.15 | tot_time = 6.249\n",
      "Epoch 13: loss = 0.06 | acc = 0.982 | val_loss = 0.132 | val_acc = 0.963 | train_time = 7.379 | tot_time = 7.478\n",
      "Epoch 14: loss = 0.057 | acc = 0.983 | val_loss = 0.124 | val_acc = 0.964 | train_time = 6.175 | tot_time = 6.276\n",
      "Epoch 15: loss = 0.051 | acc = 0.985 | val_loss = 0.124 | val_acc = 0.966 | train_time = 6.161 | tot_time = 6.267\n",
      "Epoch 16: loss = 0.048 | acc = 0.986 | val_loss = 0.133 | val_acc = 0.962 | train_time = 6.227 | tot_time = 6.327\n",
      "Epoch 17: loss = 0.044 | acc = 0.988 | val_loss = 0.122 | val_acc = 0.966 | train_time = 6.126 | tot_time = 6.228\n",
      "Epoch 18: loss = 0.041 | acc = 0.989 | val_loss = 0.131 | val_acc = 0.965 | train_time = 6.078 | tot_time = 6.177\n",
      "Epoch 19: loss = 0.038 | acc = 0.99 | val_loss = 0.123 | val_acc = 0.967 | train_time = 6.095 | tot_time = 6.194\n",
      "Epoch 20: loss = 0.035 | acc = 0.99 | val_loss = 0.128 | val_acc = 0.966 | train_time = 6.093 | tot_time = 6.191\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.391 | acc = 0.879 | val_loss = 0.23 | val_acc = 0.929 | train_time = 6.132 | tot_time = 6.231\n",
      "Epoch 2: loss = 0.203 | acc = 0.939 | val_loss = 0.208 | val_acc = 0.935 | train_time = 6.122 | tot_time = 6.224\n",
      "Epoch 3: loss = 0.156 | acc = 0.952 | val_loss = 0.161 | val_acc = 0.95 | train_time = 6.154 | tot_time = 6.253\n",
      "Epoch 4: loss = 0.13 | acc = 0.96 | val_loss = 0.147 | val_acc = 0.956 | train_time = 6.085 | tot_time = 6.181\n",
      "Epoch 5: loss = 0.109 | acc = 0.966 | val_loss = 0.146 | val_acc = 0.958 | train_time = 6.119 | tot_time = 6.228\n",
      "Epoch 6: loss = 0.093 | acc = 0.971 | val_loss = 0.149 | val_acc = 0.958 | train_time = 6.221 | tot_time = 6.323\n",
      "Epoch 7: loss = 0.083 | acc = 0.974 | val_loss = 0.131 | val_acc = 0.962 | train_time = 6.197 | tot_time = 6.301\n",
      "Epoch 8: loss = 0.072 | acc = 0.977 | val_loss = 0.13 | val_acc = 0.962 | train_time = 6.192 | tot_time = 6.288\n",
      "Epoch 9: loss = 0.063 | acc = 0.98 | val_loss = 0.13 | val_acc = 0.963 | train_time = 6.105 | tot_time = 6.204\n",
      "Epoch 10: loss = 0.056 | acc = 0.982 | val_loss = 0.134 | val_acc = 0.962 | train_time = 6.109 | tot_time = 6.206\n",
      "Epoch 11: loss = 0.05 | acc = 0.985 | val_loss = 0.135 | val_acc = 0.965 | train_time = 6.13 | tot_time = 6.228\n",
      "Epoch 12: loss = 0.044 | acc = 0.986 | val_loss = 0.126 | val_acc = 0.967 | train_time = 6.204 | tot_time = 6.314\n",
      "Epoch 13: loss = 0.038 | acc = 0.988 | val_loss = 0.129 | val_acc = 0.966 | train_time = 6.082 | tot_time = 6.179\n",
      "Epoch 14: loss = 0.035 | acc = 0.989 | val_loss = 0.139 | val_acc = 0.963 | train_time = 6.04 | tot_time = 6.141\n",
      "Epoch 15: loss = 0.032 | acc = 0.99 | val_loss = 0.146 | val_acc = 0.964 | train_time = 7.24 | tot_time = 7.342\n",
      "Epoch 16: loss = 0.028 | acc = 0.992 | val_loss = 0.13 | val_acc = 0.967 | train_time = 6.178 | tot_time = 6.277\n",
      "Epoch 17: loss = 0.024 | acc = 0.993 | val_loss = 0.141 | val_acc = 0.965 | train_time = 6.065 | tot_time = 6.185\n",
      "Epoch 18: loss = 0.02 | acc = 0.994 | val_loss = 0.134 | val_acc = 0.967 | train_time = 6.207 | tot_time = 6.303\n",
      "Epoch 19: loss = 0.017 | acc = 0.996 | val_loss = 0.137 | val_acc = 0.966 | train_time = 6.081 | tot_time = 6.181\n",
      "Epoch 20: loss = 0.015 | acc = 0.996 | val_loss = 0.143 | val_acc = 0.965 | train_time = 6.175 | tot_time = 6.274\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.383 | acc = 0.881 | val_loss = 0.21 | val_acc = 0.937 | train_time = 6.18 | tot_time = 6.277\n",
      "Epoch 2: loss = 0.197 | acc = 0.94 | val_loss = 0.172 | val_acc = 0.947 | train_time = 6.187 | tot_time = 6.284\n",
      "Epoch 3: loss = 0.153 | acc = 0.954 | val_loss = 0.168 | val_acc = 0.951 | train_time = 6.192 | tot_time = 6.291\n",
      "Epoch 4: loss = 0.128 | acc = 0.961 | val_loss = 0.179 | val_acc = 0.949 | train_time = 6.044 | tot_time = 6.141\n",
      "Epoch 5: loss = 0.108 | acc = 0.966 | val_loss = 0.143 | val_acc = 0.959 | train_time = 6.238 | tot_time = 6.338\n",
      "Epoch 6: loss = 0.094 | acc = 0.97 | val_loss = 0.131 | val_acc = 0.965 | train_time = 6.071 | tot_time = 6.171\n",
      "Epoch 7: loss = 0.082 | acc = 0.975 | val_loss = 0.149 | val_acc = 0.959 | train_time = 6.141 | tot_time = 6.239\n",
      "Epoch 8: loss = 0.075 | acc = 0.976 | val_loss = 0.134 | val_acc = 0.964 | train_time = 6.038 | tot_time = 6.14\n",
      "Epoch 9: loss = 0.065 | acc = 0.979 | val_loss = 0.124 | val_acc = 0.966 | train_time = 6.127 | tot_time = 6.228\n",
      "Epoch 10: loss = 0.059 | acc = 0.981 | val_loss = 0.148 | val_acc = 0.962 | train_time = 6.071 | tot_time = 6.17\n",
      "Epoch 11: loss = 0.051 | acc = 0.984 | val_loss = 0.143 | val_acc = 0.963 | train_time = 6.172 | tot_time = 6.276\n",
      "Epoch 12: loss = 0.048 | acc = 0.984 | val_loss = 0.168 | val_acc = 0.956 | train_time = 6.165 | tot_time = 6.27\n",
      "Epoch 13: loss = 0.044 | acc = 0.986 | val_loss = 0.163 | val_acc = 0.96 | train_time = 6.139 | tot_time = 6.238\n",
      "Epoch 14: loss = 0.039 | acc = 0.987 | val_loss = 0.156 | val_acc = 0.96 | train_time = 6.155 | tot_time = 6.252\n",
      "Epoch 15: loss = 0.034 | acc = 0.989 | val_loss = 0.14 | val_acc = 0.966 | train_time = 6.102 | tot_time = 6.204\n",
      "Epoch 16: loss = 0.032 | acc = 0.989 | val_loss = 0.137 | val_acc = 0.965 | train_time = 6.072 | tot_time = 6.171\n",
      "Epoch 17: loss = 0.027 | acc = 0.991 | val_loss = 0.141 | val_acc = 0.966 | train_time = 6.158 | tot_time = 6.257\n",
      "Epoch 18: loss = 0.024 | acc = 0.992 | val_loss = 0.147 | val_acc = 0.965 | train_time = 7.195 | tot_time = 7.293\n",
      "Epoch 19: loss = 0.025 | acc = 0.992 | val_loss = 0.153 | val_acc = 0.965 | train_time = 6.098 | tot_time = 6.197\n",
      "Epoch 20: loss = 0.02 | acc = 0.994 | val_loss = 0.15 | val_acc = 0.967 | train_time = 6.069 | tot_time = 6.175\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.494 | acc = 0.863 | val_loss = 0.261 | val_acc = 0.926 | train_time = 6.117 | tot_time = 6.216\n",
      "Epoch 2: loss = 0.257 | acc = 0.929 | val_loss = 0.301 | val_acc = 0.924 | train_time = 6.043 | tot_time = 6.139\n",
      "Epoch 3: loss = 0.206 | acc = 0.942 | val_loss = 0.211 | val_acc = 0.943 | train_time = 6.038 | tot_time = 6.138\n",
      "Epoch 4: loss = 0.179 | acc = 0.95 | val_loss = 0.186 | val_acc = 0.95 | train_time = 6.157 | tot_time = 6.258\n",
      "Epoch 5: loss = 0.154 | acc = 0.957 | val_loss = 0.167 | val_acc = 0.956 | train_time = 6.171 | tot_time = 6.27\n",
      "Epoch 6: loss = 0.133 | acc = 0.962 | val_loss = 0.161 | val_acc = 0.957 | train_time = 6.15 | tot_time = 6.251\n",
      "Epoch 7: loss = 0.125 | acc = 0.965 | val_loss = 0.158 | val_acc = 0.961 | train_time = 6.053 | tot_time = 6.151\n",
      "Epoch 8: loss = 0.117 | acc = 0.966 | val_loss = 0.176 | val_acc = 0.957 | train_time = 6.051 | tot_time = 6.148\n",
      "Epoch 9: loss = 0.111 | acc = 0.969 | val_loss = 0.15 | val_acc = 0.96 | train_time = 6.06 | tot_time = 6.158\n",
      "Epoch 10: loss = 0.098 | acc = 0.972 | val_loss = 0.151 | val_acc = 0.964 | train_time = 6.147 | tot_time = 6.25\n",
      "Epoch 11: loss = 0.092 | acc = 0.973 | val_loss = 0.171 | val_acc = 0.96 | train_time = 6.093 | tot_time = 6.192\n",
      "Epoch 12: loss = 0.085 | acc = 0.976 | val_loss = 0.156 | val_acc = 0.96 | train_time = 6.086 | tot_time = 6.183\n",
      "Epoch 13: loss = 0.075 | acc = 0.977 | val_loss = 0.154 | val_acc = 0.964 | train_time = 6.056 | tot_time = 6.157\n",
      "Epoch 14: loss = 0.077 | acc = 0.978 | val_loss = 0.149 | val_acc = 0.966 | train_time = 6.123 | tot_time = 6.237\n",
      "Epoch 15: loss = 0.08 | acc = 0.976 | val_loss = 0.179 | val_acc = 0.959 | train_time = 6.103 | tot_time = 6.206\n",
      "Epoch 16: loss = 0.068 | acc = 0.98 | val_loss = 0.154 | val_acc = 0.964 | train_time = 6.136 | tot_time = 6.235\n",
      "Epoch 17: loss = 0.064 | acc = 0.981 | val_loss = 0.152 | val_acc = 0.967 | train_time = 6.031 | tot_time = 6.127\n",
      "Epoch 18: loss = 0.057 | acc = 0.982 | val_loss = 0.175 | val_acc = 0.959 | train_time = 6.219 | tot_time = 6.317\n",
      "Epoch 19: loss = 0.063 | acc = 0.981 | val_loss = 0.167 | val_acc = 0.966 | train_time = 7.322 | tot_time = 7.422\n",
      "Epoch 20: loss = 0.057 | acc = 0.983 | val_loss = 0.179 | val_acc = 0.962 | train_time = 6.15 | tot_time = 6.25\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.677 | acc = 0.841 | val_loss = 0.302 | val_acc = 0.925 | train_time = 6.056 | tot_time = 6.16\n",
      "Epoch 2: loss = 0.345 | acc = 0.916 | val_loss = 0.27 | val_acc = 0.936 | train_time = 8.109 | tot_time = 8.206\n",
      "Epoch 3: loss = 0.283 | acc = 0.93 | val_loss = 0.284 | val_acc = 0.935 | train_time = 6.147 | tot_time = 6.258\n",
      "Epoch 4: loss = 0.249 | acc = 0.938 | val_loss = 0.272 | val_acc = 0.935 | train_time = 6.104 | tot_time = 6.205\n",
      "Epoch 5: loss = 0.231 | acc = 0.943 | val_loss = 0.243 | val_acc = 0.943 | train_time = 6.136 | tot_time = 6.234\n",
      "Epoch 6: loss = 0.215 | acc = 0.948 | val_loss = 0.283 | val_acc = 0.93 | train_time = 6.028 | tot_time = 6.125\n",
      "Epoch 7: loss = 0.2 | acc = 0.951 | val_loss = 0.21 | val_acc = 0.95 | train_time = 6.017 | tot_time = 6.113\n",
      "Epoch 8: loss = 0.178 | acc = 0.956 | val_loss = 0.193 | val_acc = 0.956 | train_time = 6.081 | tot_time = 6.178\n",
      "Epoch 9: loss = 0.18 | acc = 0.956 | val_loss = 0.189 | val_acc = 0.958 | train_time = 6.156 | tot_time = 6.254\n",
      "Epoch 10: loss = 0.167 | acc = 0.958 | val_loss = 0.175 | val_acc = 0.962 | train_time = 6.134 | tot_time = 6.235\n",
      "Epoch 11: loss = 0.159 | acc = 0.959 | val_loss = 0.176 | val_acc = 0.961 | train_time = 6.148 | tot_time = 6.247\n",
      "Epoch 12: loss = 0.146 | acc = 0.963 | val_loss = 0.272 | val_acc = 0.94 | train_time = 6.283 | tot_time = 6.381\n",
      "Epoch 13: loss = 0.15 | acc = 0.963 | val_loss = 0.241 | val_acc = 0.947 | train_time = 6.253 | tot_time = 6.353\n",
      "Epoch 14: loss = 0.136 | acc = 0.966 | val_loss = 0.185 | val_acc = 0.96 | train_time = 6.256 | tot_time = 6.359\n",
      "Epoch 15: loss = 0.129 | acc = 0.966 | val_loss = 0.214 | val_acc = 0.952 | train_time = 6.125 | tot_time = 6.222\n",
      "Epoch 16: loss = 0.125 | acc = 0.969 | val_loss = 0.211 | val_acc = 0.954 | train_time = 5.967 | tot_time = 6.064\n",
      "Epoch 17: loss = 0.119 | acc = 0.97 | val_loss = 0.195 | val_acc = 0.957 | train_time = 5.982 | tot_time = 6.079\n",
      "Epoch 18: loss = 0.118 | acc = 0.97 | val_loss = 0.183 | val_acc = 0.962 | train_time = 6.022 | tot_time = 6.121\n",
      "Epoch 19: loss = 0.12 | acc = 0.969 | val_loss = 0.242 | val_acc = 0.951 | train_time = 6.011 | tot_time = 6.11\n",
      "Epoch 20: loss = 0.112 | acc = 0.971 | val_loss = 0.242 | val_acc = 0.95 | train_time = 6.014 | tot_time = 6.114\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 1.109 | acc = 0.806 | val_loss = 0.675 | val_acc = 0.876 | train_time = 7.183 | tot_time = 7.286\n",
      "Epoch 2: loss = 0.55 | acc = 0.892 | val_loss = 0.629 | val_acc = 0.885 | train_time = 6.149 | tot_time = 6.248\n",
      "Epoch 3: loss = 0.492 | acc = 0.903 | val_loss = 0.48 | val_acc = 0.91 | train_time = 5.986 | tot_time = 6.083\n",
      "Epoch 4: loss = 0.46 | acc = 0.909 | val_loss = 0.602 | val_acc = 0.878 | train_time = 6.115 | tot_time = 6.218\n",
      "Epoch 5: loss = 0.413 | acc = 0.918 | val_loss = 0.39 | val_acc = 0.925 | train_time = 6.053 | tot_time = 6.151\n",
      "Epoch 6: loss = 0.381 | acc = 0.924 | val_loss = 0.324 | val_acc = 0.939 | train_time = 5.967 | tot_time = 6.065\n",
      "Epoch 7: loss = 0.346 | acc = 0.929 | val_loss = 0.483 | val_acc = 0.907 | train_time = 5.987 | tot_time = 6.084\n",
      "Epoch 8: loss = 0.341 | acc = 0.929 | val_loss = 0.315 | val_acc = 0.94 | train_time = 6.018 | tot_time = 6.115\n",
      "Epoch 9: loss = 0.323 | acc = 0.935 | val_loss = 0.307 | val_acc = 0.937 | train_time = 6.064 | tot_time = 6.164\n",
      "Epoch 10: loss = 0.3 | acc = 0.937 | val_loss = 0.453 | val_acc = 0.912 | train_time = 5.985 | tot_time = 6.088\n",
      "Epoch 11: loss = 0.303 | acc = 0.938 | val_loss = 0.343 | val_acc = 0.932 | train_time = 5.969 | tot_time = 6.067\n",
      "Epoch 12: loss = 0.303 | acc = 0.938 | val_loss = 0.384 | val_acc = 0.929 | train_time = 6.025 | tot_time = 6.124\n",
      "Epoch 13: loss = 0.29 | acc = 0.94 | val_loss = 0.27 | val_acc = 0.948 | train_time = 5.957 | tot_time = 6.056\n",
      "Epoch 14: loss = 0.274 | acc = 0.944 | val_loss = 0.274 | val_acc = 0.943 | train_time = 6.018 | tot_time = 6.116\n",
      "Epoch 15: loss = 0.27 | acc = 0.944 | val_loss = 0.396 | val_acc = 0.927 | train_time = 6.053 | tot_time = 6.15\n",
      "Epoch 16: loss = 0.269 | acc = 0.945 | val_loss = 0.271 | val_acc = 0.949 | train_time = 5.952 | tot_time = 6.049\n",
      "Epoch 17: loss = 0.25 | acc = 0.948 | val_loss = 0.316 | val_acc = 0.94 | train_time = 6.079 | tot_time = 6.18\n",
      "Epoch 18: loss = 0.258 | acc = 0.946 | val_loss = 0.284 | val_acc = 0.945 | train_time = 6.026 | tot_time = 6.137\n",
      "Epoch 19: loss = 0.247 | acc = 0.949 | val_loss = 0.307 | val_acc = 0.945 | train_time = 6.05 | tot_time = 6.148\n",
      "Epoch 20: loss = 0.237 | acc = 0.949 | val_loss = 0.311 | val_acc = 0.938 | train_time = 5.944 | tot_time = 6.041\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 2.33 | acc = 0.702 | val_loss = 1.318 | val_acc = 0.766 | train_time = 8.277 | tot_time = 8.552\n",
      "Epoch 2: loss = 1.132 | acc = 0.822 | val_loss = 0.736 | val_acc = 0.865 | train_time = 11.213 | tot_time = 11.409\n",
      "Epoch 3: loss = 0.897 | acc = 0.85 | val_loss = 0.809 | val_acc = 0.869 | train_time = 6.174 | tot_time = 6.275\n",
      "Epoch 4: loss = 0.778 | acc = 0.868 | val_loss = 0.741 | val_acc = 0.872 | train_time = 5.932 | tot_time = 6.045\n",
      "Epoch 5: loss = 0.736 | acc = 0.875 | val_loss = 0.707 | val_acc = 0.879 | train_time = 5.948 | tot_time = 6.048\n",
      "Epoch 6: loss = 0.705 | acc = 0.879 | val_loss = 0.72 | val_acc = 0.862 | train_time = 6.02 | tot_time = 6.128\n",
      "Epoch 7: loss = 0.63 | acc = 0.89 | val_loss = 0.517 | val_acc = 0.91 | train_time = 6.074 | tot_time = 6.184\n",
      "Epoch 8: loss = 0.604 | acc = 0.895 | val_loss = 0.757 | val_acc = 0.878 | train_time = 5.993 | tot_time = 6.106\n",
      "Epoch 9: loss = 0.604 | acc = 0.894 | val_loss = 0.45 | val_acc = 0.916 | train_time = 6.037 | tot_time = 6.147\n",
      "Epoch 10: loss = 0.573 | acc = 0.9 | val_loss = 0.532 | val_acc = 0.918 | train_time = 6.059 | tot_time = 6.157\n",
      "Epoch 11: loss = 0.575 | acc = 0.9 | val_loss = 0.732 | val_acc = 0.835 | train_time = 6.017 | tot_time = 6.115\n",
      "Epoch 12: loss = 0.571 | acc = 0.901 | val_loss = 0.519 | val_acc = 0.916 | train_time = 6.039 | tot_time = 6.139\n",
      "Epoch 13: loss = 0.549 | acc = 0.904 | val_loss = 0.527 | val_acc = 0.908 | train_time = 6.895 | tot_time = 6.992\n",
      "Epoch 14: loss = 0.527 | acc = 0.908 | val_loss = 2.565 | val_acc = 0.73 | train_time = 5.965 | tot_time = 6.061\n",
      "Epoch 15: loss = 0.527 | acc = 0.907 | val_loss = 1.333 | val_acc = 0.763 | train_time = 6.028 | tot_time = 6.126\n",
      "Epoch 16: loss = 0.509 | acc = 0.91 | val_loss = 0.384 | val_acc = 0.926 | train_time = 5.985 | tot_time = 6.083\n",
      "Epoch 17: loss = 0.504 | acc = 0.91 | val_loss = 0.427 | val_acc = 0.931 | train_time = 6.052 | tot_time = 6.153\n",
      "Epoch 18: loss = 0.494 | acc = 0.913 | val_loss = 0.412 | val_acc = 0.933 | train_time = 6.007 | tot_time = 6.103\n",
      "Epoch 19: loss = 0.498 | acc = 0.911 | val_loss = 0.378 | val_acc = 0.93 | train_time = 5.966 | tot_time = 6.067\n",
      "Epoch 20: loss = 0.471 | acc = 0.916 | val_loss = 0.565 | val_acc = 0.901 | train_time = 6.083 | tot_time = 6.183\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 3.655 | acc = 0.62 | val_loss = 1.853 | val_acc = 0.73 | train_time = 6.055 | tot_time = 6.156\n",
      "Epoch 2: loss = 2.001 | acc = 0.744 | val_loss = 1.801 | val_acc = 0.752 | train_time = 6.16 | tot_time = 6.272\n",
      "Epoch 3: loss = 1.681 | acc = 0.781 | val_loss = 1.45 | val_acc = 0.788 | train_time = 6.15 | tot_time = 6.247\n",
      "Epoch 4: loss = 1.684 | acc = 0.779 | val_loss = 0.968 | val_acc = 0.864 | train_time = 6.43 | tot_time = 6.618\n",
      "Epoch 5: loss = 1.632 | acc = 0.78 | val_loss = 1.298 | val_acc = 0.819 | train_time = 6.55 | tot_time = 6.648\n",
      "Epoch 6: loss = 1.476 | acc = 0.801 | val_loss = 1.112 | val_acc = 0.857 | train_time = 6.037 | tot_time = 6.137\n",
      "Epoch 7: loss = 1.381 | acc = 0.811 | val_loss = 2.094 | val_acc = 0.64 | train_time = 5.972 | tot_time = 6.072\n",
      "Epoch 8: loss = 1.399 | acc = 0.813 | val_loss = 1.346 | val_acc = 0.779 | train_time = 5.966 | tot_time = 6.078\n",
      "Epoch 9: loss = 1.327 | acc = 0.816 | val_loss = 1.103 | val_acc = 0.848 | train_time = 5.899 | tot_time = 6.01\n",
      "Epoch 10: loss = 1.273 | acc = 0.826 | val_loss = 0.985 | val_acc = 0.857 | train_time = 5.964 | tot_time = 6.063\n",
      "Epoch 11: loss = 1.158 | acc = 0.838 | val_loss = 0.821 | val_acc = 0.884 | train_time = 5.95 | tot_time = 6.047\n",
      "Epoch 12: loss = 1.075 | acc = 0.847 | val_loss = 0.804 | val_acc = 0.887 | train_time = 6.039 | tot_time = 6.134\n",
      "Epoch 13: loss = 1.058 | acc = 0.848 | val_loss = 0.789 | val_acc = 0.884 | train_time = 6.019 | tot_time = 6.118\n",
      "Epoch 14: loss = 1.011 | acc = 0.853 | val_loss = 0.83 | val_acc = 0.878 | train_time = 5.963 | tot_time = 6.06\n",
      "Epoch 15: loss = 1.033 | acc = 0.849 | val_loss = 0.832 | val_acc = 0.882 | train_time = 6.034 | tot_time = 6.136\n",
      "Epoch 16: loss = 0.94 | acc = 0.862 | val_loss = 1.221 | val_acc = 0.801 | train_time = 6.068 | tot_time = 6.166\n",
      "Epoch 17: loss = 0.902 | acc = 0.867 | val_loss = 0.741 | val_acc = 0.89 | train_time = 5.965 | tot_time = 6.062\n",
      "Epoch 18: loss = 0.94 | acc = 0.86 | val_loss = 1.706 | val_acc = 0.728 | train_time = 5.939 | tot_time = 6.037\n",
      "Epoch 19: loss = 0.89 | acc = 0.867 | val_loss = 1.112 | val_acc = 0.808 | train_time = 5.957 | tot_time = 6.057\n",
      "Epoch 20: loss = 0.911 | acc = 0.865 | val_loss = 0.671 | val_acc = 0.899 | train_time = 5.998 | tot_time = 6.095\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 11.808 | acc = 0.312 | val_loss = 5.795 | val_acc = 0.433 | train_time = 5.984 | tot_time = 6.083\n",
      "Epoch 2: loss = 6.224 | acc = 0.478 | val_loss = 3.926 | val_acc = 0.485 | train_time = 6.082 | tot_time = 6.181\n",
      "Epoch 3: loss = 5.109 | acc = 0.531 | val_loss = 8.501 | val_acc = 0.272 | train_time = 5.991 | tot_time = 6.088\n",
      "Epoch 4: loss = 4.759 | acc = 0.548 | val_loss = 2.536 | val_acc = 0.68 | train_time = 5.952 | tot_time = 6.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss = 4.484 | acc = 0.56 | val_loss = 2.908 | val_acc = 0.67 | train_time = 5.955 | tot_time = 6.058\n",
      "Epoch 6: loss = 4.027 | acc = 0.592 | val_loss = 2.627 | val_acc = 0.659 | train_time = 6.013 | tot_time = 6.115\n",
      "Epoch 7: loss = 3.968 | acc = 0.597 | val_loss = 4.523 | val_acc = 0.518 | train_time = 7.156 | tot_time = 7.254\n",
      "Epoch 8: loss = 3.541 | acc = 0.638 | val_loss = 4.153 | val_acc = 0.454 | train_time = 6.041 | tot_time = 6.14\n",
      "Epoch 9: loss = 3.166 | acc = 0.661 | val_loss = 3.065 | val_acc = 0.704 | train_time = 6.057 | tot_time = 6.155\n",
      "Epoch 10: loss = 3.122 | acc = 0.666 | val_loss = 2.53 | val_acc = 0.722 | train_time = 6.007 | tot_time = 6.104\n",
      "Epoch 11: loss = 3.237 | acc = 0.656 | val_loss = 3.462 | val_acc = 0.574 | train_time = 6.023 | tot_time = 6.121\n",
      "Epoch 12: loss = 2.967 | acc = 0.679 | val_loss = 1.596 | val_acc = 0.794 | train_time = 6.049 | tot_time = 6.152\n",
      "Epoch 13: loss = 3.074 | acc = 0.67 | val_loss = 4.19 | val_acc = 0.6 | train_time = 6.053 | tot_time = 6.148\n",
      "Epoch 14: loss = 3.332 | acc = 0.65 | val_loss = 2.362 | val_acc = 0.766 | train_time = 6.083 | tot_time = 6.18\n",
      "Epoch 15: loss = 2.886 | acc = 0.687 | val_loss = 2.738 | val_acc = 0.683 | train_time = 6.122 | tot_time = 6.22\n",
      "Epoch 16: loss = 2.844 | acc = 0.688 | val_loss = 4.756 | val_acc = 0.544 | train_time = 6.127 | tot_time = 6.222\n",
      "Epoch 17: loss = 2.727 | acc = 0.701 | val_loss = 1.969 | val_acc = 0.742 | train_time = 6.128 | tot_time = 6.241\n",
      "Epoch 18: loss = 2.749 | acc = 0.694 | val_loss = 2.152 | val_acc = 0.738 | train_time = 6.081 | tot_time = 6.179\n",
      "Epoch 19: loss = 2.735 | acc = 0.693 | val_loss = 2.316 | val_acc = 0.703 | train_time = 6.133 | tot_time = 6.235\n",
      "Epoch 20: loss = 2.954 | acc = 0.673 | val_loss = 1.99 | val_acc = 0.744 | train_time = 6.192 | tot_time = 6.291\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 27.861 | acc = 0.108 | val_loss = 25.534 | val_acc = 0.114 | train_time = 6.078 | tot_time = 6.18\n",
      "Epoch 2: loss = 28.011 | acc = 0.11 | val_loss = 31.918 | val_acc = 0.089 | train_time = 6.095 | tot_time = 6.191\n",
      "Epoch 3: loss = 28.819 | acc = 0.101 | val_loss = 25.881 | val_acc = 0.098 | train_time = 5.91 | tot_time = 6.006\n",
      "Epoch 4: loss = 28.637 | acc = 0.101 | val_loss = 19.867 | val_acc = 0.097 | train_time = 5.949 | tot_time = 6.045\n",
      "Epoch 5: loss = 28.924 | acc = 0.099 | val_loss = 37.13 | val_acc = 0.098 | train_time = 5.944 | tot_time = 6.043\n",
      "Epoch 6: loss = 28.684 | acc = 0.102 | val_loss = 35.197 | val_acc = 0.101 | train_time = 5.968 | tot_time = 6.064\n",
      "Epoch 7: loss = 28.715 | acc = 0.101 | val_loss = 26.604 | val_acc = 0.101 | train_time = 5.934 | tot_time = 6.031\n",
      "Epoch 8: loss = 28.755 | acc = 0.101 | val_loss = 43.458 | val_acc = 0.097 | train_time = 5.887 | tot_time = 5.993\n",
      "Epoch 9: loss = 28.685 | acc = 0.101 | val_loss = 22.189 | val_acc = 0.103 | train_time = 7.094 | tot_time = 7.205\n",
      "Epoch 10: loss = 28.811 | acc = 0.101 | val_loss = 30.848 | val_acc = 0.101 | train_time = 5.953 | tot_time = 6.062\n",
      "Epoch 11: loss = 28.96 | acc = 0.1 | val_loss = 22.44 | val_acc = 0.097 | train_time = 5.966 | tot_time = 6.065\n",
      "Epoch 12: loss = 28.657 | acc = 0.102 | val_loss = 27.275 | val_acc = 0.103 | train_time = 5.969 | tot_time = 6.066\n",
      "Epoch 13: loss = 24.803 | acc = 0.147 | val_loss = 12.22 | val_acc = 0.322 | train_time = 5.935 | tot_time = 6.031\n",
      "Epoch 14: loss = 14.962 | acc = 0.263 | val_loss = 16.296 | val_acc = 0.284 | train_time = 6.012 | tot_time = 6.111\n",
      "Epoch 15: loss = 13.553 | acc = 0.281 | val_loss = 18.964 | val_acc = 0.122 | train_time = 6.081 | tot_time = 6.18\n",
      "Epoch 16: loss = 12.404 | acc = 0.319 | val_loss = 11.442 | val_acc = 0.375 | train_time = 6.033 | tot_time = 6.13\n",
      "Epoch 17: loss = 12.244 | acc = 0.325 | val_loss = 11.108 | val_acc = 0.284 | train_time = 6.071 | tot_time = 6.172\n",
      "Epoch 18: loss = 12.283 | acc = 0.328 | val_loss = 13.598 | val_acc = 0.369 | train_time = 6.054 | tot_time = 6.153\n",
      "Epoch 19: loss = 10.544 | acc = 0.36 | val_loss = 10.231 | val_acc = 0.34 | train_time = 6.038 | tot_time = 6.135\n",
      "Epoch 20: loss = 9.359 | acc = 0.399 | val_loss = 12.246 | val_acc = 0.405 | train_time = 6.091 | tot_time = 6.201\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_vector, val_loss_vector, train_loss_vector = optimize_lr()\n",
    "\n",
    "plt.plot(lr_vector, val_loss_vector, label='Validation Loss')\n",
    "plt.plot(lr_vector, train_loss_vector, label='Training Loss')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBoPH2ho-esz"
   },
   "source": [
    "### Experiment setup:\n",
    "1. We train the model for varying learning rates, starting from lr=0.1 and increasing it everytime by 0.1 up till max lr=1 or till model validation loss values are not nan/inf.\n",
    "\n",
    "2. We record the model validation and training loss for each of these learning rates and plot a graph of Loss vs Learning Rate to see its trend.\n",
    "\n",
    "3. Every other parameter remains constant.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "- From the graph in the above cell we see that the validation loss decreases with increasing learning rate, but then increases exponentially after a certain point.\n",
    "\n",
    "- Using the elbow method, we find optimal learning rate to be <= 0.6. After this, the model starts to diverge and hence loss increases.\n",
    "\n",
    "- Thus, increasing the learning rate increases the chances of the model converging faster, upto a certain point, after which it causes the model learning to diverge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVjLm0vcuzhJ"
   },
   "source": [
    "## Change no. of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-v_ba4Eayj2D"
   },
   "outputs": [],
   "source": [
    "# === Complete the code (2')\n",
    "def optimize_layers(max_layer=6):\n",
    "  layer = 1 ## Initial layer size\n",
    "  val_loss_vector = []\n",
    "  train_loss_vector = []\n",
    "  layer_vector = []\n",
    "\n",
    "  # keeping num_epochs, no. of neurons constant\n",
    "  while layer < max_layer:\n",
    "    model = MLP(X_train, Y_train, X_test, Y_test, L=layer, N_l=64)\n",
    "    model.train(batch_size=8, epochs=8, lr=0.04)\n",
    "    \n",
    "    if model.val_loss[-1] == float('inf') or math.isnan(model.val_loss[-1]):\n",
    "      break\n",
    "\n",
    "    layer_vector.append(layer)\n",
    "    val_loss_vector.append(model.val_loss[-1])\n",
    "    train_loss_vector.append(model.train_loss[-1])\n",
    "    layer += 1 # Increase layer linearly for each iteration\n",
    "\n",
    "    print(\"\\n\\n ------------------------------------------- \\n\\n\")\n",
    "\n",
    "  return layer_vector, val_loss_vector, train_loss_vector\n",
    "# === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ionNCRwPyj6n",
    "outputId": "27aa06a6-022f-44b1-b05a-ef75f816afec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.63 | acc = 0.808 | val_loss = 0.358 | val_acc = 0.893 | train_time = 5.421 | tot_time = 5.499\n",
      "Epoch 2: loss = 0.329 | acc = 0.903 | val_loss = 0.29 | val_acc = 0.914 | train_time = 5.378 | tot_time = 5.46\n",
      "Epoch 3: loss = 0.273 | acc = 0.919 | val_loss = 0.25 | val_acc = 0.926 | train_time = 5.346 | tot_time = 5.423\n",
      "Epoch 4: loss = 0.238 | acc = 0.93 | val_loss = 0.229 | val_acc = 0.933 | train_time = 5.362 | tot_time = 5.438\n",
      "Epoch 5: loss = 0.214 | acc = 0.936 | val_loss = 0.214 | val_acc = 0.937 | train_time = 5.412 | tot_time = 5.491\n",
      "Epoch 6: loss = 0.196 | acc = 0.942 | val_loss = 0.197 | val_acc = 0.942 | train_time = 5.35 | tot_time = 5.425\n",
      "Epoch 7: loss = 0.18 | acc = 0.947 | val_loss = 0.19 | val_acc = 0.944 | train_time = 5.339 | tot_time = 5.418\n",
      "Epoch 8: loss = 0.168 | acc = 0.949 | val_loss = 0.182 | val_acc = 0.947 | train_time = 5.399 | tot_time = 5.474\n",
      "Epoch 9: loss = 0.157 | acc = 0.953 | val_loss = 0.176 | val_acc = 0.95 | train_time = 5.454 | tot_time = 5.529\n",
      "Epoch 10: loss = 0.148 | acc = 0.956 | val_loss = 0.169 | val_acc = 0.951 | train_time = 5.337 | tot_time = 5.415\n",
      "Epoch 11: loss = 0.139 | acc = 0.958 | val_loss = 0.163 | val_acc = 0.952 | train_time = 5.366 | tot_time = 5.455\n",
      "Epoch 12: loss = 0.132 | acc = 0.961 | val_loss = 0.161 | val_acc = 0.952 | train_time = 6.488 | tot_time = 6.626\n",
      "Epoch 13: loss = 0.125 | acc = 0.963 | val_loss = 0.159 | val_acc = 0.954 | train_time = 5.603 | tot_time = 5.682\n",
      "Epoch 14: loss = 0.119 | acc = 0.965 | val_loss = 0.152 | val_acc = 0.955 | train_time = 5.371 | tot_time = 5.445\n",
      "Epoch 15: loss = 0.114 | acc = 0.966 | val_loss = 0.148 | val_acc = 0.957 | train_time = 5.36 | tot_time = 5.435\n",
      "Epoch 16: loss = 0.109 | acc = 0.968 | val_loss = 0.146 | val_acc = 0.957 | train_time = 5.477 | tot_time = 5.553\n",
      "Epoch 17: loss = 0.104 | acc = 0.969 | val_loss = 0.144 | val_acc = 0.958 | train_time = 5.419 | tot_time = 5.495\n",
      "Epoch 18: loss = 0.1 | acc = 0.971 | val_loss = 0.143 | val_acc = 0.957 | train_time = 5.409 | tot_time = 5.485\n",
      "Epoch 19: loss = 0.096 | acc = 0.972 | val_loss = 0.14 | val_acc = 0.958 | train_time = 5.488 | tot_time = 5.566\n",
      "Epoch 20: loss = 0.092 | acc = 0.973 | val_loss = 0.14 | val_acc = 0.959 | train_time = 5.394 | tot_time = 5.471\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.639 | acc = 0.803 | val_loss = 0.359 | val_acc = 0.891 | train_time = 6.085 | tot_time = 6.182\n",
      "Epoch 2: loss = 0.32 | acc = 0.904 | val_loss = 0.28 | val_acc = 0.913 | train_time = 6.113 | tot_time = 6.214\n",
      "Epoch 3: loss = 0.257 | acc = 0.923 | val_loss = 0.243 | val_acc = 0.928 | train_time = 6.085 | tot_time = 6.199\n",
      "Epoch 4: loss = 0.221 | acc = 0.934 | val_loss = 0.216 | val_acc = 0.934 | train_time = 6.082 | tot_time = 6.192\n",
      "Epoch 5: loss = 0.197 | acc = 0.942 | val_loss = 0.2 | val_acc = 0.94 | train_time = 5.97 | tot_time = 6.079\n",
      "Epoch 6: loss = 0.179 | acc = 0.946 | val_loss = 0.204 | val_acc = 0.936 | train_time = 6.18 | tot_time = 6.279\n",
      "Epoch 7: loss = 0.165 | acc = 0.95 | val_loss = 0.189 | val_acc = 0.943 | train_time = 6.024 | tot_time = 6.122\n",
      "Epoch 8: loss = 0.153 | acc = 0.954 | val_loss = 0.171 | val_acc = 0.947 | train_time = 6.017 | tot_time = 6.113\n",
      "Epoch 9: loss = 0.142 | acc = 0.957 | val_loss = 0.166 | val_acc = 0.949 | train_time = 6.04 | tot_time = 6.139\n",
      "Epoch 10: loss = 0.133 | acc = 0.96 | val_loss = 0.16 | val_acc = 0.951 | train_time = 6.025 | tot_time = 6.123\n",
      "Epoch 11: loss = 0.125 | acc = 0.962 | val_loss = 0.158 | val_acc = 0.951 | train_time = 6.068 | tot_time = 6.166\n",
      "Epoch 12: loss = 0.119 | acc = 0.964 | val_loss = 0.151 | val_acc = 0.954 | train_time = 6.089 | tot_time = 6.192\n",
      "Epoch 13: loss = 0.112 | acc = 0.967 | val_loss = 0.149 | val_acc = 0.954 | train_time = 6.086 | tot_time = 6.184\n",
      "Epoch 14: loss = 0.106 | acc = 0.968 | val_loss = 0.147 | val_acc = 0.956 | train_time = 6.065 | tot_time = 6.162\n",
      "Epoch 15: loss = 0.1 | acc = 0.97 | val_loss = 0.145 | val_acc = 0.958 | train_time = 6.541 | tot_time = 6.744\n",
      "Epoch 16: loss = 0.096 | acc = 0.971 | val_loss = 0.144 | val_acc = 0.957 | train_time = 6.713 | tot_time = 6.812\n",
      "Epoch 17: loss = 0.091 | acc = 0.973 | val_loss = 0.139 | val_acc = 0.958 | train_time = 6.077 | tot_time = 6.174\n",
      "Epoch 18: loss = 0.087 | acc = 0.974 | val_loss = 0.139 | val_acc = 0.959 | train_time = 5.971 | tot_time = 6.068\n",
      "Epoch 19: loss = 0.083 | acc = 0.975 | val_loss = 0.139 | val_acc = 0.959 | train_time = 6.09 | tot_time = 6.187\n",
      "Epoch 20: loss = 0.079 | acc = 0.977 | val_loss = 0.138 | val_acc = 0.96 | train_time = 6.0 | tot_time = 6.097\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.654 | acc = 0.801 | val_loss = 0.358 | val_acc = 0.893 | train_time = 6.597 | tot_time = 6.725\n",
      "Epoch 2: loss = 0.315 | acc = 0.905 | val_loss = 0.281 | val_acc = 0.915 | train_time = 6.65 | tot_time = 6.767\n",
      "Epoch 3: loss = 0.25 | acc = 0.924 | val_loss = 0.244 | val_acc = 0.927 | train_time = 6.692 | tot_time = 6.81\n",
      "Epoch 4: loss = 0.213 | acc = 0.936 | val_loss = 0.216 | val_acc = 0.934 | train_time = 6.561 | tot_time = 6.68\n",
      "Epoch 5: loss = 0.188 | acc = 0.943 | val_loss = 0.205 | val_acc = 0.938 | train_time = 6.669 | tot_time = 6.792\n",
      "Epoch 6: loss = 0.169 | acc = 0.949 | val_loss = 0.191 | val_acc = 0.942 | train_time = 6.58 | tot_time = 6.7\n",
      "Epoch 7: loss = 0.153 | acc = 0.954 | val_loss = 0.178 | val_acc = 0.946 | train_time = 6.52 | tot_time = 6.642\n",
      "Epoch 8: loss = 0.14 | acc = 0.958 | val_loss = 0.179 | val_acc = 0.946 | train_time = 6.713 | tot_time = 6.834\n",
      "Epoch 9: loss = 0.13 | acc = 0.962 | val_loss = 0.167 | val_acc = 0.95 | train_time = 6.666 | tot_time = 6.788\n",
      "Epoch 10: loss = 0.121 | acc = 0.965 | val_loss = 0.156 | val_acc = 0.951 | train_time = 6.555 | tot_time = 6.674\n",
      "Epoch 11: loss = 0.112 | acc = 0.967 | val_loss = 0.151 | val_acc = 0.953 | train_time = 6.557 | tot_time = 6.679\n",
      "Epoch 12: loss = 0.105 | acc = 0.969 | val_loss = 0.151 | val_acc = 0.953 | train_time = 6.656 | tot_time = 6.776\n",
      "Epoch 13: loss = 0.098 | acc = 0.971 | val_loss = 0.145 | val_acc = 0.955 | train_time = 6.57 | tot_time = 6.693\n",
      "Epoch 14: loss = 0.093 | acc = 0.972 | val_loss = 0.145 | val_acc = 0.954 | train_time = 6.67 | tot_time = 6.788\n",
      "Epoch 15: loss = 0.087 | acc = 0.974 | val_loss = 0.144 | val_acc = 0.956 | train_time = 6.538 | tot_time = 6.654\n",
      "Epoch 16: loss = 0.083 | acc = 0.976 | val_loss = 0.142 | val_acc = 0.956 | train_time = 7.736 | tot_time = 7.863\n",
      "Epoch 17: loss = 0.079 | acc = 0.977 | val_loss = 0.142 | val_acc = 0.956 | train_time = 6.628 | tot_time = 6.746\n",
      "Epoch 18: loss = 0.074 | acc = 0.978 | val_loss = 0.14 | val_acc = 0.957 | train_time = 6.551 | tot_time = 6.672\n",
      "Epoch 19: loss = 0.071 | acc = 0.98 | val_loss = 0.137 | val_acc = 0.958 | train_time = 6.57 | tot_time = 6.702\n",
      "Epoch 20: loss = 0.067 | acc = 0.981 | val_loss = 0.138 | val_acc = 0.958 | train_time = 6.664 | tot_time = 6.785\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.663 | acc = 0.794 | val_loss = 0.348 | val_acc = 0.896 | train_time = 7.173 | tot_time = 7.316\n",
      "Epoch 2: loss = 0.308 | acc = 0.907 | val_loss = 0.256 | val_acc = 0.923 | train_time = 7.159 | tot_time = 7.311\n",
      "Epoch 3: loss = 0.242 | acc = 0.927 | val_loss = 0.219 | val_acc = 0.934 | train_time = 7.213 | tot_time = 7.352\n",
      "Epoch 4: loss = 0.205 | acc = 0.939 | val_loss = 0.207 | val_acc = 0.939 | train_time = 7.069 | tot_time = 7.206\n",
      "Epoch 5: loss = 0.181 | acc = 0.946 | val_loss = 0.18 | val_acc = 0.945 | train_time = 7.164 | tot_time = 7.305\n",
      "Epoch 6: loss = 0.162 | acc = 0.951 | val_loss = 0.186 | val_acc = 0.944 | train_time = 7.131 | tot_time = 7.271\n",
      "Epoch 7: loss = 0.148 | acc = 0.955 | val_loss = 0.167 | val_acc = 0.947 | train_time = 7.269 | tot_time = 7.41\n",
      "Epoch 8: loss = 0.135 | acc = 0.959 | val_loss = 0.161 | val_acc = 0.952 | train_time = 7.316 | tot_time = 7.453\n",
      "Epoch 9: loss = 0.125 | acc = 0.962 | val_loss = 0.167 | val_acc = 0.949 | train_time = 7.215 | tot_time = 7.353\n",
      "Epoch 10: loss = 0.116 | acc = 0.965 | val_loss = 0.158 | val_acc = 0.952 | train_time = 7.266 | tot_time = 7.411\n",
      "Epoch 11: loss = 0.108 | acc = 0.968 | val_loss = 0.145 | val_acc = 0.956 | train_time = 7.488 | tot_time = 7.635\n",
      "Epoch 12: loss = 0.101 | acc = 0.97 | val_loss = 0.147 | val_acc = 0.956 | train_time = 7.229 | tot_time = 7.369\n",
      "Epoch 13: loss = 0.095 | acc = 0.971 | val_loss = 0.146 | val_acc = 0.955 | train_time = 7.195 | tot_time = 7.333\n",
      "Epoch 14: loss = 0.088 | acc = 0.973 | val_loss = 0.132 | val_acc = 0.958 | train_time = 7.144 | tot_time = 7.292\n",
      "Epoch 15: loss = 0.083 | acc = 0.976 | val_loss = 0.142 | val_acc = 0.957 | train_time = 8.313 | tot_time = 8.453\n",
      "Epoch 16: loss = 0.078 | acc = 0.977 | val_loss = 0.135 | val_acc = 0.959 | train_time = 7.112 | tot_time = 7.259\n",
      "Epoch 17: loss = 0.074 | acc = 0.978 | val_loss = 0.139 | val_acc = 0.958 | train_time = 7.106 | tot_time = 7.25\n",
      "Epoch 18: loss = 0.07 | acc = 0.98 | val_loss = 0.135 | val_acc = 0.958 | train_time = 7.086 | tot_time = 7.223\n",
      "Epoch 19: loss = 0.065 | acc = 0.981 | val_loss = 0.148 | val_acc = 0.953 | train_time = 7.122 | tot_time = 7.257\n",
      "Epoch 20: loss = 0.062 | acc = 0.982 | val_loss = 0.135 | val_acc = 0.961 | train_time = 7.04 | tot_time = 7.179\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.699 | acc = 0.78 | val_loss = 0.347 | val_acc = 0.894 | train_time = 7.703 | tot_time = 7.883\n",
      "Epoch 2: loss = 0.311 | acc = 0.905 | val_loss = 0.254 | val_acc = 0.923 | train_time = 7.958 | tot_time = 8.132\n",
      "Epoch 3: loss = 0.242 | acc = 0.926 | val_loss = 0.224 | val_acc = 0.934 | train_time = 7.798 | tot_time = 7.957\n",
      "Epoch 4: loss = 0.204 | acc = 0.938 | val_loss = 0.207 | val_acc = 0.938 | train_time = 7.906 | tot_time = 8.066\n",
      "Epoch 5: loss = 0.179 | acc = 0.946 | val_loss = 0.178 | val_acc = 0.946 | train_time = 7.82 | tot_time = 7.979\n",
      "Epoch 6: loss = 0.159 | acc = 0.951 | val_loss = 0.18 | val_acc = 0.947 | train_time = 7.852 | tot_time = 8.016\n",
      "Epoch 7: loss = 0.144 | acc = 0.956 | val_loss = 0.168 | val_acc = 0.95 | train_time = 7.791 | tot_time = 7.953\n",
      "Epoch 8: loss = 0.131 | acc = 0.96 | val_loss = 0.161 | val_acc = 0.95 | train_time = 7.682 | tot_time = 7.84\n",
      "Epoch 9: loss = 0.12 | acc = 0.963 | val_loss = 0.154 | val_acc = 0.953 | train_time = 7.685 | tot_time = 7.854\n",
      "Epoch 10: loss = 0.111 | acc = 0.966 | val_loss = 0.15 | val_acc = 0.956 | train_time = 7.685 | tot_time = 7.843\n",
      "Epoch 11: loss = 0.103 | acc = 0.968 | val_loss = 0.146 | val_acc = 0.955 | train_time = 7.645 | tot_time = 7.806\n",
      "Epoch 12: loss = 0.096 | acc = 0.971 | val_loss = 0.15 | val_acc = 0.955 | train_time = 9.018 | tot_time = 9.176\n",
      "Epoch 13: loss = 0.089 | acc = 0.973 | val_loss = 0.149 | val_acc = 0.957 | train_time = 7.665 | tot_time = 7.839\n",
      "Epoch 14: loss = 0.084 | acc = 0.975 | val_loss = 0.152 | val_acc = 0.955 | train_time = 7.686 | tot_time = 7.847\n",
      "Epoch 15: loss = 0.078 | acc = 0.976 | val_loss = 0.151 | val_acc = 0.956 | train_time = 7.59 | tot_time = 7.747\n",
      "Epoch 16: loss = 0.073 | acc = 0.979 | val_loss = 0.148 | val_acc = 0.956 | train_time = 7.682 | tot_time = 7.841\n",
      "Epoch 17: loss = 0.069 | acc = 0.98 | val_loss = 0.149 | val_acc = 0.956 | train_time = 7.693 | tot_time = 7.854\n",
      "Epoch 18: loss = 0.065 | acc = 0.98 | val_loss = 0.146 | val_acc = 0.956 | train_time = 7.614 | tot_time = 7.774\n",
      "Epoch 19: loss = 0.062 | acc = 0.982 | val_loss = 0.144 | val_acc = 0.959 | train_time = 7.597 | tot_time = 7.757\n",
      "Epoch 20: loss = 0.057 | acc = 0.983 | val_loss = 0.151 | val_acc = 0.958 | train_time = 7.625 | tot_time = 7.8\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_vector, val_loss_vector, train_loss_vector = optimize_layers()\n",
    "\n",
    "plt.plot(layer_vector, val_loss_vector, label='Validation Loss')\n",
    "plt.plot(lr_vector, train_loss_vector, label='Training Loss')\n",
    "plt.xlabel(\"No. of layers\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKTucdFyOiO-"
   },
   "source": [
    "### Experiment setup:\n",
    "1. We train the model for varying no. of layers, starting from layer=1 and increasing it everytime by 1 up till max lr=5 or till model validation loss values are not nan/inf.\n",
    "\n",
    "2. We record the model validation and training loss for each of these layers and plot a graph of Loss vs No. of layers to see its trend.\n",
    "\n",
    "3. Every other parameter remains constant.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "- From the graph in the above cell we see that the validation loss decreases with increasing learning rate, but then increases exponentially after a certain point.\n",
    "\n",
    "- Using the elbow method, we find optimal learning rate to be <= 0.6. After this, the model starts to diverge and hence loss increases.\n",
    "\n",
    "- Thus, increasing the learning rate increases the chances of the model converging faster, upto a certain point, after which it causes the model learning to diverge.\n",
    "\n",
    "- From the graph in the above cell we see that the validation loss decreases with increasing layer, but then increases exponentially after a certain point.\n",
    "\n",
    "- Using the elbow method, we find optimal no. of layers to be = 4. After this, the model starts to diverge and hence loss increases. This is because <>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BP9hpf0Z1kuD"
   },
   "source": [
    "## Change amount of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LbL5anyJ1kFT"
   },
   "outputs": [],
   "source": [
    "# === Complete the code (2')\n",
    "def analyse_training_data_amount():\n",
    "  val_loss_vector = []\n",
    "  train_loss_vector = []\n",
    "  tdata_vector = []\n",
    "  x = 6000\n",
    "\n",
    "  # keeping num_epochs, no. of neurons constant\n",
    "  while x <= 60000:\n",
    "    model = MLP(X_train[:x], Y_train[:x], X_test, Y_test, L=2, N_l=64)\n",
    "    model.train(batch_size=8, epochs=8, lr=0.04)\n",
    "    \n",
    "    if model.val_loss[-1] == float('inf') or math.isnan(model.val_loss[-1]):\n",
    "      break\n",
    "\n",
    "    tdata_vector.append(100 * x/60000)\n",
    "    val_loss_vector.append(model.val_loss[-1])\n",
    "    train_loss_vector.append(model.train_loss[-1])\n",
    "    x += 6000 # Increase data amount linearly for each iteration\n",
    "\n",
    "    print(\"\\n\\n ------------------------------------------- \\n\\n\")\n",
    "\n",
    "  return tdata_vector, val_loss_vector, train_loss_vector\n",
    "# === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l7IOdrYK1kKd",
    "outputId": "9db9d541-879c-4637-ad65-3c03e8ae6006"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 1.521 | acc = 0.519 | val_loss = 0.985 | val_acc = 0.69 | train_time = 0.599 | tot_time = 0.707\n",
      "Epoch 2: loss = 0.787 | acc = 0.762 | val_loss = 0.718 | val_acc = 0.774 | train_time = 0.602 | tot_time = 0.699\n",
      "Epoch 3: loss = 0.6 | acc = 0.816 | val_loss = 0.606 | val_acc = 0.813 | train_time = 0.625 | tot_time = 0.721\n",
      "Epoch 4: loss = 0.505 | acc = 0.848 | val_loss = 0.55 | val_acc = 0.829 | train_time = 0.616 | tot_time = 0.713\n",
      "Epoch 5: loss = 0.44 | acc = 0.866 | val_loss = 0.493 | val_acc = 0.847 | train_time = 0.607 | tot_time = 0.702\n",
      "Epoch 6: loss = 0.396 | acc = 0.881 | val_loss = 0.467 | val_acc = 0.856 | train_time = 0.607 | tot_time = 0.706\n",
      "Epoch 7: loss = 0.359 | acc = 0.895 | val_loss = 0.437 | val_acc = 0.865 | train_time = 0.594 | tot_time = 0.69\n",
      "Epoch 8: loss = 0.329 | acc = 0.904 | val_loss = 0.426 | val_acc = 0.87 | train_time = 0.596 | tot_time = 0.693\n",
      "Epoch 9: loss = 0.307 | acc = 0.911 | val_loss = 0.407 | val_acc = 0.875 | train_time = 0.618 | tot_time = 0.722\n",
      "Epoch 10: loss = 0.284 | acc = 0.92 | val_loss = 0.396 | val_acc = 0.879 | train_time = 0.646 | tot_time = 0.746\n",
      "Epoch 11: loss = 0.267 | acc = 0.924 | val_loss = 0.384 | val_acc = 0.881 | train_time = 0.609 | tot_time = 0.717\n",
      "Epoch 12: loss = 0.25 | acc = 0.93 | val_loss = 0.376 | val_acc = 0.884 | train_time = 0.637 | tot_time = 0.736\n",
      "Epoch 13: loss = 0.235 | acc = 0.932 | val_loss = 0.371 | val_acc = 0.885 | train_time = 0.627 | tot_time = 0.728\n",
      "Epoch 14: loss = 0.223 | acc = 0.94 | val_loss = 0.362 | val_acc = 0.887 | train_time = 0.609 | tot_time = 0.712\n",
      "Epoch 15: loss = 0.211 | acc = 0.942 | val_loss = 0.364 | val_acc = 0.887 | train_time = 0.604 | tot_time = 0.704\n",
      "Epoch 16: loss = 0.2 | acc = 0.944 | val_loss = 0.357 | val_acc = 0.889 | train_time = 0.619 | tot_time = 0.72\n",
      "Epoch 17: loss = 0.188 | acc = 0.949 | val_loss = 0.353 | val_acc = 0.892 | train_time = 0.611 | tot_time = 0.713\n",
      "Epoch 18: loss = 0.18 | acc = 0.951 | val_loss = 0.349 | val_acc = 0.891 | train_time = 0.597 | tot_time = 0.694\n",
      "Epoch 19: loss = 0.171 | acc = 0.954 | val_loss = 0.348 | val_acc = 0.892 | train_time = 0.621 | tot_time = 0.718\n",
      "Epoch 20: loss = 0.163 | acc = 0.956 | val_loss = 0.345 | val_acc = 0.894 | train_time = 0.63 | tot_time = 0.728\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 1.114 | acc = 0.654 | val_loss = 0.67 | val_acc = 0.8 | train_time = 1.218 | tot_time = 1.314\n",
      "Epoch 2: loss = 0.552 | acc = 0.838 | val_loss = 0.508 | val_acc = 0.848 | train_time = 1.197 | tot_time = 1.3\n",
      "Epoch 3: loss = 0.437 | acc = 0.87 | val_loss = 0.44 | val_acc = 0.866 | train_time = 1.204 | tot_time = 1.304\n",
      "Epoch 4: loss = 0.378 | acc = 0.887 | val_loss = 0.395 | val_acc = 0.879 | train_time = 1.216 | tot_time = 1.32\n",
      "Epoch 5: loss = 0.337 | acc = 0.901 | val_loss = 0.368 | val_acc = 0.885 | train_time = 1.185 | tot_time = 1.281\n",
      "Epoch 6: loss = 0.307 | acc = 0.909 | val_loss = 0.353 | val_acc = 0.89 | train_time = 1.196 | tot_time = 1.302\n",
      "Epoch 7: loss = 0.281 | acc = 0.918 | val_loss = 0.335 | val_acc = 0.897 | train_time = 1.181 | tot_time = 1.289\n",
      "Epoch 8: loss = 0.26 | acc = 0.924 | val_loss = 0.32 | val_acc = 0.9 | train_time = 1.198 | tot_time = 1.295\n",
      "Epoch 9: loss = 0.242 | acc = 0.927 | val_loss = 0.314 | val_acc = 0.903 | train_time = 1.226 | tot_time = 1.323\n",
      "Epoch 10: loss = 0.226 | acc = 0.933 | val_loss = 0.301 | val_acc = 0.907 | train_time = 1.217 | tot_time = 1.314\n",
      "Epoch 11: loss = 0.212 | acc = 0.937 | val_loss = 0.296 | val_acc = 0.91 | train_time = 1.21 | tot_time = 1.307\n",
      "Epoch 12: loss = 0.199 | acc = 0.942 | val_loss = 0.289 | val_acc = 0.911 | train_time = 1.204 | tot_time = 1.301\n",
      "Epoch 13: loss = 0.187 | acc = 0.945 | val_loss = 0.284 | val_acc = 0.911 | train_time = 1.186 | tot_time = 1.284\n",
      "Epoch 14: loss = 0.176 | acc = 0.948 | val_loss = 0.284 | val_acc = 0.913 | train_time = 1.209 | tot_time = 1.307\n",
      "Epoch 15: loss = 0.167 | acc = 0.952 | val_loss = 0.28 | val_acc = 0.912 | train_time = 1.2 | tot_time = 1.298\n",
      "Epoch 16: loss = 0.159 | acc = 0.954 | val_loss = 0.274 | val_acc = 0.914 | train_time = 1.214 | tot_time = 1.312\n",
      "Epoch 17: loss = 0.149 | acc = 0.958 | val_loss = 0.272 | val_acc = 0.916 | train_time = 1.232 | tot_time = 1.329\n",
      "Epoch 18: loss = 0.142 | acc = 0.96 | val_loss = 0.267 | val_acc = 0.917 | train_time = 1.244 | tot_time = 1.343\n",
      "Epoch 19: loss = 0.135 | acc = 0.964 | val_loss = 0.268 | val_acc = 0.917 | train_time = 1.216 | tot_time = 1.316\n",
      "Epoch 20: loss = 0.128 | acc = 0.966 | val_loss = 0.26 | val_acc = 0.92 | train_time = 1.213 | tot_time = 1.322\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 1.053 | acc = 0.666 | val_loss = 0.571 | val_acc = 0.83 | train_time = 1.802 | tot_time = 1.902\n",
      "Epoch 2: loss = 0.495 | acc = 0.85 | val_loss = 0.432 | val_acc = 0.867 | train_time = 1.798 | tot_time = 1.896\n",
      "Epoch 3: loss = 0.392 | acc = 0.881 | val_loss = 0.385 | val_acc = 0.88 | train_time = 1.821 | tot_time = 1.918\n",
      "Epoch 4: loss = 0.338 | acc = 0.897 | val_loss = 0.34 | val_acc = 0.898 | train_time = 1.842 | tot_time = 1.939\n",
      "Epoch 5: loss = 0.301 | acc = 0.909 | val_loss = 0.316 | val_acc = 0.906 | train_time = 1.795 | tot_time = 1.896\n",
      "Epoch 6: loss = 0.272 | acc = 0.919 | val_loss = 0.296 | val_acc = 0.912 | train_time = 1.795 | tot_time = 1.891\n",
      "Epoch 7: loss = 0.25 | acc = 0.927 | val_loss = 0.291 | val_acc = 0.912 | train_time = 1.76 | tot_time = 1.867\n",
      "Epoch 8: loss = 0.232 | acc = 0.932 | val_loss = 0.276 | val_acc = 0.916 | train_time = 1.815 | tot_time = 1.91\n",
      "Epoch 9: loss = 0.216 | acc = 0.938 | val_loss = 0.26 | val_acc = 0.922 | train_time = 1.771 | tot_time = 1.867\n",
      "Epoch 10: loss = 0.201 | acc = 0.941 | val_loss = 0.257 | val_acc = 0.923 | train_time = 1.777 | tot_time = 1.873\n",
      "Epoch 11: loss = 0.188 | acc = 0.946 | val_loss = 0.253 | val_acc = 0.923 | train_time = 1.791 | tot_time = 1.887\n",
      "Epoch 12: loss = 0.177 | acc = 0.95 | val_loss = 0.24 | val_acc = 0.927 | train_time = 1.79 | tot_time = 1.889\n",
      "Epoch 13: loss = 0.167 | acc = 0.952 | val_loss = 0.238 | val_acc = 0.93 | train_time = 1.813 | tot_time = 1.924\n",
      "Epoch 14: loss = 0.158 | acc = 0.956 | val_loss = 0.234 | val_acc = 0.93 | train_time = 1.845 | tot_time = 1.945\n",
      "Epoch 15: loss = 0.149 | acc = 0.958 | val_loss = 0.231 | val_acc = 0.93 | train_time = 1.895 | tot_time = 2.013\n",
      "Epoch 16: loss = 0.142 | acc = 0.961 | val_loss = 0.226 | val_acc = 0.933 | train_time = 2.46 | tot_time = 2.645\n",
      "Epoch 17: loss = 0.134 | acc = 0.963 | val_loss = 0.225 | val_acc = 0.934 | train_time = 2.175 | tot_time = 2.273\n",
      "Epoch 18: loss = 0.127 | acc = 0.964 | val_loss = 0.216 | val_acc = 0.934 | train_time = 1.822 | tot_time = 1.923\n",
      "Epoch 19: loss = 0.121 | acc = 0.967 | val_loss = 0.217 | val_acc = 0.935 | train_time = 1.837 | tot_time = 1.934\n",
      "Epoch 20: loss = 0.115 | acc = 0.969 | val_loss = 0.212 | val_acc = 0.935 | train_time = 1.84 | tot_time = 1.937\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.893 | acc = 0.721 | val_loss = 0.504 | val_acc = 0.844 | train_time = 2.353 | tot_time = 2.45\n",
      "Epoch 2: loss = 0.441 | acc = 0.869 | val_loss = 0.382 | val_acc = 0.88 | train_time = 2.371 | tot_time = 2.467\n",
      "Epoch 3: loss = 0.352 | acc = 0.895 | val_loss = 0.338 | val_acc = 0.896 | train_time = 2.429 | tot_time = 2.526\n",
      "Epoch 4: loss = 0.303 | acc = 0.909 | val_loss = 0.313 | val_acc = 0.903 | train_time = 2.448 | tot_time = 2.546\n",
      "Epoch 5: loss = 0.27 | acc = 0.921 | val_loss = 0.285 | val_acc = 0.913 | train_time = 2.445 | tot_time = 2.547\n",
      "Epoch 6: loss = 0.244 | acc = 0.927 | val_loss = 0.272 | val_acc = 0.913 | train_time = 2.425 | tot_time = 2.52\n",
      "Epoch 7: loss = 0.223 | acc = 0.934 | val_loss = 0.256 | val_acc = 0.921 | train_time = 2.42 | tot_time = 2.521\n",
      "Epoch 8: loss = 0.207 | acc = 0.938 | val_loss = 0.245 | val_acc = 0.924 | train_time = 2.457 | tot_time = 2.558\n",
      "Epoch 9: loss = 0.191 | acc = 0.943 | val_loss = 0.238 | val_acc = 0.926 | train_time = 2.433 | tot_time = 2.53\n",
      "Epoch 10: loss = 0.179 | acc = 0.946 | val_loss = 0.231 | val_acc = 0.928 | train_time = 2.426 | tot_time = 2.525\n",
      "Epoch 11: loss = 0.168 | acc = 0.951 | val_loss = 0.221 | val_acc = 0.933 | train_time = 2.4 | tot_time = 2.497\n",
      "Epoch 12: loss = 0.157 | acc = 0.953 | val_loss = 0.218 | val_acc = 0.932 | train_time = 2.435 | tot_time = 2.532\n",
      "Epoch 13: loss = 0.148 | acc = 0.957 | val_loss = 0.215 | val_acc = 0.934 | train_time = 2.373 | tot_time = 2.472\n",
      "Epoch 14: loss = 0.14 | acc = 0.959 | val_loss = 0.208 | val_acc = 0.936 | train_time = 2.382 | tot_time = 2.489\n",
      "Epoch 15: loss = 0.132 | acc = 0.961 | val_loss = 0.214 | val_acc = 0.934 | train_time = 2.398 | tot_time = 2.493\n",
      "Epoch 16: loss = 0.126 | acc = 0.963 | val_loss = 0.202 | val_acc = 0.938 | train_time = 2.364 | tot_time = 2.46\n",
      "Epoch 17: loss = 0.119 | acc = 0.966 | val_loss = 0.201 | val_acc = 0.938 | train_time = 2.416 | tot_time = 2.515\n",
      "Epoch 18: loss = 0.113 | acc = 0.968 | val_loss = 0.203 | val_acc = 0.938 | train_time = 2.362 | tot_time = 2.463\n",
      "Epoch 19: loss = 0.107 | acc = 0.97 | val_loss = 0.196 | val_acc = 0.94 | train_time = 2.423 | tot_time = 2.522\n",
      "Epoch 20: loss = 0.102 | acc = 0.971 | val_loss = 0.191 | val_acc = 0.942 | train_time = 2.442 | tot_time = 2.539\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.854 | acc = 0.736 | val_loss = 0.475 | val_acc = 0.859 | train_time = 2.997 | tot_time = 3.094\n",
      "Epoch 2: loss = 0.411 | acc = 0.876 | val_loss = 0.362 | val_acc = 0.891 | train_time = 2.979 | tot_time = 3.076\n",
      "Epoch 3: loss = 0.325 | acc = 0.904 | val_loss = 0.308 | val_acc = 0.905 | train_time = 3.027 | tot_time = 3.125\n",
      "Epoch 4: loss = 0.278 | acc = 0.916 | val_loss = 0.278 | val_acc = 0.916 | train_time = 2.983 | tot_time = 3.082\n",
      "Epoch 5: loss = 0.246 | acc = 0.926 | val_loss = 0.262 | val_acc = 0.919 | train_time = 2.947 | tot_time = 3.05\n",
      "Epoch 6: loss = 0.222 | acc = 0.932 | val_loss = 0.245 | val_acc = 0.923 | train_time = 2.993 | tot_time = 3.091\n",
      "Epoch 7: loss = 0.203 | acc = 0.939 | val_loss = 0.232 | val_acc = 0.928 | train_time = 3.007 | tot_time = 3.104\n",
      "Epoch 8: loss = 0.188 | acc = 0.943 | val_loss = 0.223 | val_acc = 0.93 | train_time = 3.011 | tot_time = 3.12\n",
      "Epoch 9: loss = 0.175 | acc = 0.948 | val_loss = 0.218 | val_acc = 0.934 | train_time = 2.99 | tot_time = 3.098\n",
      "Epoch 10: loss = 0.164 | acc = 0.952 | val_loss = 0.209 | val_acc = 0.934 | train_time = 3.025 | tot_time = 3.124\n",
      "Epoch 11: loss = 0.153 | acc = 0.954 | val_loss = 0.205 | val_acc = 0.937 | train_time = 2.987 | tot_time = 3.087\n",
      "Epoch 12: loss = 0.144 | acc = 0.958 | val_loss = 0.2 | val_acc = 0.938 | train_time = 3.005 | tot_time = 3.103\n",
      "Epoch 13: loss = 0.136 | acc = 0.961 | val_loss = 0.195 | val_acc = 0.94 | train_time = 3.039 | tot_time = 3.138\n",
      "Epoch 14: loss = 0.129 | acc = 0.962 | val_loss = 0.192 | val_acc = 0.94 | train_time = 3.039 | tot_time = 3.138\n",
      "Epoch 15: loss = 0.122 | acc = 0.964 | val_loss = 0.188 | val_acc = 0.942 | train_time = 3.06 | tot_time = 3.158\n",
      "Epoch 16: loss = 0.115 | acc = 0.966 | val_loss = 0.189 | val_acc = 0.942 | train_time = 3.033 | tot_time = 3.129\n",
      "Epoch 17: loss = 0.109 | acc = 0.969 | val_loss = 0.18 | val_acc = 0.945 | train_time = 3.018 | tot_time = 3.114\n",
      "Epoch 18: loss = 0.104 | acc = 0.97 | val_loss = 0.18 | val_acc = 0.946 | train_time = 3.036 | tot_time = 3.136\n",
      "Epoch 19: loss = 0.098 | acc = 0.972 | val_loss = 0.177 | val_acc = 0.945 | train_time = 3.023 | tot_time = 3.121\n",
      "Epoch 20: loss = 0.094 | acc = 0.974 | val_loss = 0.177 | val_acc = 0.946 | train_time = 3.051 | tot_time = 3.152\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.723 | acc = 0.78 | val_loss = 0.399 | val_acc = 0.879 | train_time = 3.667 | tot_time = 3.766\n",
      "Epoch 2: loss = 0.37 | acc = 0.888 | val_loss = 0.319 | val_acc = 0.902 | train_time = 3.702 | tot_time = 3.799\n",
      "Epoch 3: loss = 0.301 | acc = 0.91 | val_loss = 0.282 | val_acc = 0.913 | train_time = 3.629 | tot_time = 3.729\n",
      "Epoch 4: loss = 0.259 | acc = 0.922 | val_loss = 0.254 | val_acc = 0.922 | train_time = 3.645 | tot_time = 3.742\n",
      "Epoch 5: loss = 0.231 | acc = 0.931 | val_loss = 0.242 | val_acc = 0.926 | train_time = 4.635 | tot_time = 4.73\n",
      "Epoch 6: loss = 0.208 | acc = 0.937 | val_loss = 0.234 | val_acc = 0.928 | train_time = 3.576 | tot_time = 3.672\n",
      "Epoch 7: loss = 0.191 | acc = 0.942 | val_loss = 0.214 | val_acc = 0.936 | train_time = 3.608 | tot_time = 3.706\n",
      "Epoch 8: loss = 0.176 | acc = 0.947 | val_loss = 0.21 | val_acc = 0.936 | train_time = 3.589 | tot_time = 3.685\n",
      "Epoch 9: loss = 0.163 | acc = 0.952 | val_loss = 0.199 | val_acc = 0.94 | train_time = 3.565 | tot_time = 3.665\n",
      "Epoch 10: loss = 0.152 | acc = 0.954 | val_loss = 0.191 | val_acc = 0.943 | train_time = 3.62 | tot_time = 3.719\n",
      "Epoch 11: loss = 0.143 | acc = 0.958 | val_loss = 0.188 | val_acc = 0.943 | train_time = 3.544 | tot_time = 3.642\n",
      "Epoch 12: loss = 0.134 | acc = 0.961 | val_loss = 0.182 | val_acc = 0.946 | train_time = 3.601 | tot_time = 3.698\n",
      "Epoch 13: loss = 0.127 | acc = 0.963 | val_loss = 0.179 | val_acc = 0.946 | train_time = 3.654 | tot_time = 3.766\n",
      "Epoch 14: loss = 0.12 | acc = 0.965 | val_loss = 0.176 | val_acc = 0.946 | train_time = 3.561 | tot_time = 3.657\n",
      "Epoch 15: loss = 0.113 | acc = 0.967 | val_loss = 0.175 | val_acc = 0.947 | train_time = 3.627 | tot_time = 3.724\n",
      "Epoch 16: loss = 0.107 | acc = 0.969 | val_loss = 0.169 | val_acc = 0.95 | train_time = 3.703 | tot_time = 3.816\n",
      "Epoch 17: loss = 0.102 | acc = 0.971 | val_loss = 0.169 | val_acc = 0.95 | train_time = 3.651 | tot_time = 3.749\n",
      "Epoch 18: loss = 0.097 | acc = 0.972 | val_loss = 0.17 | val_acc = 0.949 | train_time = 3.671 | tot_time = 3.77\n",
      "Epoch 19: loss = 0.093 | acc = 0.974 | val_loss = 0.164 | val_acc = 0.951 | train_time = 3.617 | tot_time = 3.728\n",
      "Epoch 20: loss = 0.088 | acc = 0.975 | val_loss = 0.163 | val_acc = 0.952 | train_time = 3.627 | tot_time = 3.725\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.734 | acc = 0.777 | val_loss = 0.405 | val_acc = 0.876 | train_time = 4.199 | tot_time = 4.299\n",
      "Epoch 2: loss = 0.364 | acc = 0.89 | val_loss = 0.317 | val_acc = 0.904 | train_time = 4.241 | tot_time = 4.341\n",
      "Epoch 3: loss = 0.297 | acc = 0.91 | val_loss = 0.279 | val_acc = 0.916 | train_time = 4.249 | tot_time = 4.347\n",
      "Epoch 4: loss = 0.257 | acc = 0.923 | val_loss = 0.246 | val_acc = 0.925 | train_time = 4.278 | tot_time = 4.376\n",
      "Epoch 5: loss = 0.228 | acc = 0.933 | val_loss = 0.229 | val_acc = 0.931 | train_time = 4.216 | tot_time = 4.314\n",
      "Epoch 6: loss = 0.207 | acc = 0.939 | val_loss = 0.221 | val_acc = 0.934 | train_time = 4.205 | tot_time = 4.305\n",
      "Epoch 7: loss = 0.19 | acc = 0.942 | val_loss = 0.206 | val_acc = 0.936 | train_time = 4.264 | tot_time = 4.365\n",
      "Epoch 8: loss = 0.176 | acc = 0.949 | val_loss = 0.193 | val_acc = 0.94 | train_time = 4.204 | tot_time = 4.302\n",
      "Epoch 9: loss = 0.163 | acc = 0.951 | val_loss = 0.19 | val_acc = 0.942 | train_time = 4.223 | tot_time = 4.322\n",
      "Epoch 10: loss = 0.152 | acc = 0.955 | val_loss = 0.183 | val_acc = 0.945 | train_time = 4.264 | tot_time = 4.363\n",
      "Epoch 11: loss = 0.143 | acc = 0.957 | val_loss = 0.178 | val_acc = 0.945 | train_time = 4.195 | tot_time = 4.293\n",
      "Epoch 12: loss = 0.134 | acc = 0.96 | val_loss = 0.169 | val_acc = 0.949 | train_time = 4.244 | tot_time = 4.342\n",
      "Epoch 13: loss = 0.127 | acc = 0.963 | val_loss = 0.169 | val_acc = 0.949 | train_time = 4.208 | tot_time = 4.309\n",
      "Epoch 14: loss = 0.12 | acc = 0.965 | val_loss = 0.162 | val_acc = 0.951 | train_time = 4.196 | tot_time = 4.308\n",
      "Epoch 15: loss = 0.113 | acc = 0.967 | val_loss = 0.166 | val_acc = 0.95 | train_time = 4.258 | tot_time = 4.356\n",
      "Epoch 16: loss = 0.107 | acc = 0.969 | val_loss = 0.162 | val_acc = 0.952 | train_time = 4.266 | tot_time = 4.364\n",
      "Epoch 17: loss = 0.102 | acc = 0.971 | val_loss = 0.158 | val_acc = 0.952 | train_time = 5.08 | tot_time = 5.276\n",
      "Epoch 18: loss = 0.097 | acc = 0.972 | val_loss = 0.157 | val_acc = 0.952 | train_time = 4.556 | tot_time = 4.652\n",
      "Epoch 19: loss = 0.092 | acc = 0.974 | val_loss = 0.157 | val_acc = 0.953 | train_time = 4.223 | tot_time = 4.322\n",
      "Epoch 20: loss = 0.088 | acc = 0.975 | val_loss = 0.152 | val_acc = 0.955 | train_time = 4.163 | tot_time = 4.264\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.694 | acc = 0.783 | val_loss = 0.396 | val_acc = 0.878 | train_time = 4.738 | tot_time = 4.833\n",
      "Epoch 2: loss = 0.35 | acc = 0.895 | val_loss = 0.306 | val_acc = 0.906 | train_time = 4.805 | tot_time = 4.909\n",
      "Epoch 3: loss = 0.281 | acc = 0.915 | val_loss = 0.262 | val_acc = 0.921 | train_time = 4.766 | tot_time = 4.878\n",
      "Epoch 4: loss = 0.241 | acc = 0.927 | val_loss = 0.23 | val_acc = 0.932 | train_time = 4.817 | tot_time = 4.92\n",
      "Epoch 5: loss = 0.213 | acc = 0.935 | val_loss = 0.217 | val_acc = 0.934 | train_time = 4.795 | tot_time = 4.891\n",
      "Epoch 6: loss = 0.193 | acc = 0.942 | val_loss = 0.205 | val_acc = 0.937 | train_time = 4.739 | tot_time = 4.835\n",
      "Epoch 7: loss = 0.177 | acc = 0.947 | val_loss = 0.194 | val_acc = 0.942 | train_time = 4.794 | tot_time = 4.89\n",
      "Epoch 8: loss = 0.163 | acc = 0.951 | val_loss = 0.183 | val_acc = 0.946 | train_time = 4.732 | tot_time = 4.827\n",
      "Epoch 9: loss = 0.151 | acc = 0.954 | val_loss = 0.179 | val_acc = 0.948 | train_time = 4.701 | tot_time = 4.806\n",
      "Epoch 10: loss = 0.141 | acc = 0.958 | val_loss = 0.176 | val_acc = 0.948 | train_time = 4.96 | tot_time = 5.058\n",
      "Epoch 11: loss = 0.133 | acc = 0.961 | val_loss = 0.168 | val_acc = 0.949 | train_time = 4.756 | tot_time = 4.853\n",
      "Epoch 12: loss = 0.125 | acc = 0.963 | val_loss = 0.165 | val_acc = 0.95 | train_time = 4.781 | tot_time = 4.879\n",
      "Epoch 13: loss = 0.118 | acc = 0.965 | val_loss = 0.162 | val_acc = 0.952 | train_time = 4.907 | tot_time = 5.021\n",
      "Epoch 14: loss = 0.112 | acc = 0.967 | val_loss = 0.156 | val_acc = 0.953 | train_time = 4.805 | tot_time = 4.907\n",
      "Epoch 15: loss = 0.106 | acc = 0.969 | val_loss = 0.154 | val_acc = 0.952 | train_time = 4.82 | tot_time = 4.92\n",
      "Epoch 16: loss = 0.101 | acc = 0.97 | val_loss = 0.152 | val_acc = 0.955 | train_time = 4.774 | tot_time = 4.873\n",
      "Epoch 17: loss = 0.095 | acc = 0.972 | val_loss = 0.148 | val_acc = 0.955 | train_time = 4.824 | tot_time = 4.922\n",
      "Epoch 18: loss = 0.091 | acc = 0.973 | val_loss = 0.147 | val_acc = 0.956 | train_time = 4.774 | tot_time = 4.883\n",
      "Epoch 19: loss = 0.087 | acc = 0.975 | val_loss = 0.147 | val_acc = 0.958 | train_time = 4.84 | tot_time = 4.938\n",
      "Epoch 20: loss = 0.083 | acc = 0.976 | val_loss = 0.145 | val_acc = 0.957 | train_time = 4.824 | tot_time = 4.92\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.62 | acc = 0.81 | val_loss = 0.356 | val_acc = 0.894 | train_time = 5.485 | tot_time = 5.581\n",
      "Epoch 2: loss = 0.324 | acc = 0.903 | val_loss = 0.275 | val_acc = 0.918 | train_time = 5.453 | tot_time = 5.556\n",
      "Epoch 3: loss = 0.263 | acc = 0.922 | val_loss = 0.241 | val_acc = 0.929 | train_time = 5.419 | tot_time = 5.53\n",
      "Epoch 4: loss = 0.228 | acc = 0.931 | val_loss = 0.224 | val_acc = 0.936 | train_time = 6.59 | tot_time = 6.689\n",
      "Epoch 5: loss = 0.203 | acc = 0.94 | val_loss = 0.205 | val_acc = 0.94 | train_time = 5.462 | tot_time = 5.56\n",
      "Epoch 6: loss = 0.184 | acc = 0.945 | val_loss = 0.195 | val_acc = 0.942 | train_time = 5.461 | tot_time = 5.558\n",
      "Epoch 7: loss = 0.169 | acc = 0.95 | val_loss = 0.186 | val_acc = 0.946 | train_time = 5.46 | tot_time = 5.562\n",
      "Epoch 8: loss = 0.156 | acc = 0.954 | val_loss = 0.181 | val_acc = 0.948 | train_time = 5.587 | tot_time = 5.685\n",
      "Epoch 9: loss = 0.145 | acc = 0.957 | val_loss = 0.175 | val_acc = 0.948 | train_time = 5.442 | tot_time = 5.539\n",
      "Epoch 10: loss = 0.136 | acc = 0.96 | val_loss = 0.172 | val_acc = 0.949 | train_time = 5.47 | tot_time = 5.568\n",
      "Epoch 11: loss = 0.127 | acc = 0.963 | val_loss = 0.164 | val_acc = 0.951 | train_time = 5.365 | tot_time = 5.468\n",
      "Epoch 12: loss = 0.12 | acc = 0.965 | val_loss = 0.16 | val_acc = 0.952 | train_time = 5.45 | tot_time = 5.548\n",
      "Epoch 13: loss = 0.113 | acc = 0.967 | val_loss = 0.16 | val_acc = 0.952 | train_time = 5.449 | tot_time = 5.547\n",
      "Epoch 14: loss = 0.107 | acc = 0.969 | val_loss = 0.158 | val_acc = 0.952 | train_time = 5.398 | tot_time = 5.495\n",
      "Epoch 15: loss = 0.101 | acc = 0.97 | val_loss = 0.155 | val_acc = 0.952 | train_time = 5.39 | tot_time = 5.486\n",
      "Epoch 16: loss = 0.096 | acc = 0.972 | val_loss = 0.152 | val_acc = 0.953 | train_time = 5.41 | tot_time = 5.508\n",
      "Epoch 17: loss = 0.092 | acc = 0.974 | val_loss = 0.152 | val_acc = 0.953 | train_time = 5.48 | tot_time = 5.576\n",
      "Epoch 18: loss = 0.087 | acc = 0.974 | val_loss = 0.152 | val_acc = 0.953 | train_time = 5.372 | tot_time = 5.468\n",
      "Epoch 19: loss = 0.083 | acc = 0.976 | val_loss = 0.15 | val_acc = 0.954 | train_time = 5.496 | tot_time = 5.601\n",
      "Epoch 20: loss = 0.079 | acc = 0.977 | val_loss = 0.148 | val_acc = 0.955 | train_time = 5.381 | tot_time = 5.48\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n",
      "Epoch 1: loss = 0.595 | acc = 0.818 | val_loss = 0.349 | val_acc = 0.892 | train_time = 6.011 | tot_time = 6.111\n",
      "Epoch 2: loss = 0.321 | acc = 0.903 | val_loss = 0.271 | val_acc = 0.916 | train_time = 6.093 | tot_time = 6.194\n",
      "Epoch 3: loss = 0.259 | acc = 0.922 | val_loss = 0.235 | val_acc = 0.93 | train_time = 6.08 | tot_time = 6.182\n",
      "Epoch 4: loss = 0.223 | acc = 0.933 | val_loss = 0.21 | val_acc = 0.937 | train_time = 6.054 | tot_time = 6.15\n",
      "Epoch 5: loss = 0.198 | acc = 0.94 | val_loss = 0.194 | val_acc = 0.94 | train_time = 6.026 | tot_time = 6.127\n",
      "Epoch 6: loss = 0.178 | acc = 0.947 | val_loss = 0.18 | val_acc = 0.945 | train_time = 5.946 | tot_time = 6.044\n",
      "Epoch 7: loss = 0.163 | acc = 0.951 | val_loss = 0.169 | val_acc = 0.948 | train_time = 7.26 | tot_time = 7.358\n",
      "Epoch 8: loss = 0.15 | acc = 0.955 | val_loss = 0.171 | val_acc = 0.947 | train_time = 6.0 | tot_time = 6.097\n",
      "Epoch 9: loss = 0.139 | acc = 0.959 | val_loss = 0.159 | val_acc = 0.952 | train_time = 5.998 | tot_time = 6.097\n",
      "Epoch 10: loss = 0.13 | acc = 0.961 | val_loss = 0.153 | val_acc = 0.951 | train_time = 5.994 | tot_time = 6.098\n",
      "Epoch 11: loss = 0.122 | acc = 0.964 | val_loss = 0.148 | val_acc = 0.953 | train_time = 6.032 | tot_time = 6.129\n",
      "Epoch 12: loss = 0.115 | acc = 0.965 | val_loss = 0.144 | val_acc = 0.956 | train_time = 6.027 | tot_time = 6.127\n",
      "Epoch 13: loss = 0.108 | acc = 0.968 | val_loss = 0.141 | val_acc = 0.956 | train_time = 6.079 | tot_time = 6.175\n",
      "Epoch 14: loss = 0.103 | acc = 0.969 | val_loss = 0.144 | val_acc = 0.956 | train_time = 6.042 | tot_time = 6.145\n",
      "Epoch 15: loss = 0.097 | acc = 0.971 | val_loss = 0.138 | val_acc = 0.956 | train_time = 6.067 | tot_time = 6.164\n",
      "Epoch 16: loss = 0.092 | acc = 0.973 | val_loss = 0.135 | val_acc = 0.956 | train_time = 5.986 | tot_time = 6.082\n",
      "Epoch 17: loss = 0.087 | acc = 0.974 | val_loss = 0.134 | val_acc = 0.958 | train_time = 6.063 | tot_time = 6.163\n",
      "Epoch 18: loss = 0.083 | acc = 0.975 | val_loss = 0.133 | val_acc = 0.958 | train_time = 6.029 | tot_time = 6.126\n",
      "Epoch 19: loss = 0.079 | acc = 0.977 | val_loss = 0.129 | val_acc = 0.96 | train_time = 6.032 | tot_time = 6.132\n",
      "Epoch 20: loss = 0.075 | acc = 0.978 | val_loss = 0.13 | val_acc = 0.959 | train_time = 5.984 | tot_time = 6.082\n",
      "\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dcnO4QEQhLWBBKWICDIEhRBBRUrakVtFXC5Ltx7/dlWLdrN1i7qbXtta3utlS62Ra0bIBXFrbYqi+BGIiA7sidhC1sIhOzf3x9zoANMIEAmZ5K8n4/HPDJnm/kwHuc953zP+X7NOYeIiMixovwuQEREIpMCQkREQlJAiIhISAoIEREJSQEhIiIhxfhdQENJS0tzWVlZfpchItKk5Ofn73LOpYda1mwCIisri7y8PL/LEBFpUsxsc13LwnqKyczGmtkaM1tnZg+EWH6XmS0zsyVmtsDM+nnzs8zskDd/iZn9MZx1iojI8cJ2BGFm0cAU4DKgEFhkZrOdcyuDVnvROfdHb/1xwG+Asd6y9c65QeGqT0RETiycRxDnAuuccxucc5XANOCa4BWcc/uDJhMB3dYtIhIhwtkG0RUoCJouBM47diUz+wZwPxAHXBK0KNvMFgP7gR865z4Ise2dwJ0A3bp1a7jKRSTsqqqqKCwspLy83O9SWoSEhAQyMjKIjY2t9za+N1I756YAU8zsJuCHwG3ANqCbc263mQ0FXjWz/sccceCcewp4CiA3N1dHHyJNSGFhIUlJSWRlZWFmfpfTrDnn2L17N4WFhWRnZ9d7u3CeYioCMoOmM7x5dZkGXAvgnKtwzu32nucD64GcMNUpIj4oLy8nNTVV4dAIzIzU1NRTPloLZ0AsAnqbWbaZxQETgdnBK5hZ76DJq4AvvPnpXiM3ZtYD6A1sCGOtIuIDhUPjOZ3POmynmJxz1WZ2N/AOEA1Mdc6tMLNHgDzn3GzgbjMbA1QBewmcXgK4CHjEzKqAWuAu59yecNRZUlbFXxdu5OqBnendMSkcbyEi0iSFtQ3COfcW8NYx834c9PybdWz3d+Dv4aztsBrn+NO89RSXVvC/XxnQGG8pIhGiTZs2HDhwwO8yjhJJNbX4vpjaJ8bxlSFdmbW4kL0HK/0uR0QkYrT4gAC4fUQ25VW1vLRoi9+liIjPlixZwvDhwxk4cCDXXXcde/fuBeCJJ56gX79+DBw4kIkTJwIwb948Bg0axKBBgxg8eDClpaVHvdYDDzzAlClTjkw/9NBDPPbYYxw4cIBLL72UIUOGMGDAAF577bXj6pg7dy5f/vKXj0zffffdPPPMMwDk5+czatQohg4dyuWXX862bdvqrPFM+H6ZayTo0ymJC3ql8bcPN/PfF/YgNlq5KdKYHn59BSu37j/5iqegX5dkfnJ1/1Pe7tZbb+V3v/sdo0aN4sc//jEPP/wwjz/+OI8++igbN24kPj6effv2AfDYY48xZcoURo4cyYEDB0hISDjqtSZMmMDkyZP5xje+AcCMGTN45513SEhIYNasWSQnJ7Nr1y6GDx/OuHHj6tWQXFVVxT333MNrr71Geno606dP58EHH2Tq1KkhazwT+ib0TLogi+37y3l7+Xa/SxERn5SUlLBv3z5GjRoFwG233cb8+fMBGDhwIDfffDPPP/88MTGB39YjR47k/vvv54knnmDfvn1H5h82ePBgdu7cydatW1m6dCkpKSlkZmbinOMHP/gBAwcOZMyYMRQVFbFjx4561bhmzRqWL1/OZZddxqBBg/jpT39KYWFhnTWeCR1BeEbndCA7LZGpCzYy7pwufpcj0qKczi/9xvbmm28yf/58Xn/9dX72s5+xbNkyHnjgAa666ireeustRo4cyTvvvMNZZ5111HY33HADM2fOZPv27UyYMAGAF154geLiYvLz84mNjSUrK+u4exRiYmKora09Mn14uXOO/v3789FHH9WrxjMJCh1BeKKijDtGZrGkYB+fbdnrdzki4oO2bduSkpLCBx8EevZ57rnnGDVqFLW1tRQUFHDxxRfzi1/8gpKSEg4cOMD69esZMGAA3/ve9xg2bBirV68+7jUnTJjAtGnTmDlzJjfccAMQOFLp0KEDsbGxzJkzh82bj+9xu3v37qxcuZKKigr27dvHe++9B0CfPn0oLi4+EhBVVVWsWLGizhrPhI4ggnx1SAa/emcNUxdsZMhNKX6XIyJhVlZWRkZGxpHp+++/n2effZa77rqLsrIyevTowdNPP01NTQ233HILJSUlOOe49957adeuHT/60Y+YM2cOUVFR9O/fnyuuuOK49+jfvz+lpaV07dqVzp07A3DzzTdz9dVXM2DAAHJzc4876gDIzMxk/PjxnH322WRnZzN48GAA4uLimDlzJvfeey8lJSVUV1czefJkcnJyQtZ4Jsy55tGFUW5urmuIAYN+/tYq/rpgIx9892K6tGvVAJWJSCirVq2ib9++fpfRooT6zM0s3zmXG2p9nWI6xq3nd8c5x98+qnOQJRGRFkEBcYyMlNaMPbsTL326hbLKar/LERHxjQIihEkjsyk5VMUrn52o81kROVPN5RR3U3A6n7UCIoSh3VMYmNGWpxdupLZWO7BIOCQkJLB7926FRCM4PB7EsTfynYyuYgrBzJg0MpvJ05cw/4tiRvfp4HdJIs1ORkYGhYWFFBcX+11Ki3B4RLlToYCow5UDOvPzt1bx9MJNCgiRMIiNjT2l0c2k8ekUUx3iYqL4j+Hdmbe2mHU7S0++gYhIM6OAOIGbzutGXEwUTy/c5HcpIiKNTgFxAqlt4rluUFf+/lkh+8o0VoSItCwKiJO444KswFgRnxb4XYqISKNSQJzEWZ2SGdkrlb99tImqmtqTri8i0lwoIOph0shstpWU8w+NFSEiLYgCoh4u7tOBrNTWTF240e9SREQajQKiHgJjRWSzeIvGihCRlkMBUU/XD80gKSFGl7yKSIuhgKinxPgYJg7L5K1l29hWcsjvckREwk4BcQpuPT9LY0WISIuhgDgFme1bc3n/Trz4yRYOVdb4XY6ISFgpIE7RpAsCY0XMWqyxIkSkeVNAnKLc7ikM6NqWqQs3qh97EWnWFBCnyMy4Y2QW63Ye4IMvdvldjohI2CggTsNVAzuTnhSvG+dEpFlTQJyG+Jho/mN4d+auKWbdzgN+lyMiEhYKiNN0eKyIZz7UUYSINE8KiNOU1iaeawd14e/5RRorQkSaJQXEGbhjZDaHqmqYtkhjRYhI86OAOAN9Oyczomcqz36osSJEpPlRQJyhw2NFvLNCY0WISPOigDhDl5zVge6prZm6QI3VItK8hDUgzGysma0xs3Vm9kCI5XeZ2TIzW2JmC8ysX9Cy73vbrTGzy8NZ55mIijLuGJHFZ1v2saRgn9/liIg0mLAFhJlFA1OAK4B+wI3BAeB50Tk3wDk3CPgl8Btv237ARKA/MBb4vfd6Een63EyS4mN4WjfOiUgzEs4jiHOBdc65Dc65SmAacE3wCs65/UGTicDhzo2uAaY55yqccxuBdd7rRaQ28TFMGJbJm59vY3tJud/liIg0iHAGRFcg+PrPQm/eUczsG2a2nsARxL2nuO2dZpZnZnnFxcUNVvjpuG1EFrXO8dzHm3ytQ0SkofjeSO2cm+Kc6wl8D/jhKW77lHMu1zmXm56eHp4C6ymzfWsu69dRY0WISLMRzoAoAjKDpjO8eXWZBlx7mttGhEkjs9lbVsWrSyK+VBGRkwpnQCwCeptZtpnFEWh0nh28gpn1Dpq8CvjCez4bmGhm8WaWDfQGPg1jrQ3i3Oz29O+SzNQFGitCRJq+sAWEc64auBt4B1gFzHDOrTCzR8xsnLfa3Wa2wsyWAPcDt3nbrgBmACuBfwDfcM5F/HkbM2PSyGy+2HmABes0VoSING3WXH7p5ubmury8PL/LoKK6hpGPzmFA12SeviNiL7wSEQHAzPKdc7mhlvneSN3cHB4rYs6aYtYXa6wIEWm6FBBhcPPwbsRFR/HMwk1+lyIictoUEGGQ1iaeawZ1YWZ+ISVlVX6XIyJyWhQQYfLvsSK2+F2KiMhpUUCESb8uyZzfIzBWRLXGihCRJkgBEUaTLshma0k5/1y5w+9SREROmQIijDRWhIg0ZQqIMIqOMm4fkUXe5r0s1VgRItLEKCDC7AaNFSEiTZQCIszaxMdwQ24mb3y+jR37NVaEiDQdCohGcPuILGqc47mPNvtdiohIvSkgGkG31NZc1rcjL3yymfKqiO9zUEQEUEA0mkkXeGNFLNZYESLSNCggGsl52e3p1zmZqQs1VoSINA0KiEZiZky6IJu1Ow6wcN1uv8sRETkpBUQjuvqczqS1iWOqLnkVkSZAAdGI4mOiuWV4d95fvZMNGitCRCKcAqKR3Xxed+Kio3j2w01+lyIickIKiEaWnhTPuEFdeDm/kJJDGitCRCKXAsIHd4zMoqyyhhmLCvwuRUSkTgoIH/Tv0pbhPdrzjMaKEJEIpoDwyaSR2RTtO8S/NFaEiEQoBYRPLu3bkcz2rXTJq4hELAWETwJjRWSzaNNePi/UWBEiEnkUED4an5tBm/gYnl64ye9SRESOo4DwUVJCLDfkZvDG51s1VoSIRBwFhM9uH5FFda3j+Y81VoSIRBYFhM+6pyYypm9HXvhki8aKEJGIooCIAJNGZrPnYCWvLdFYESISORQQEWB4j/b07ZzM1AWbNFaEiEQMBUQEMDMmjcxizY5SZmnEORGJEAqICHHd4K6cm9WeB2ct54sdpX6XIyKigIgUMdFR/O6mwSTGR3PX8/kcrKj2uyQRaeEUEBGkY3ICT9w4mI27DvL9V5apPUJEfKWAiDAjeqZx/2U5zF66lec/2eJ3OSLSgikgItDXR/didJ90/uf1lSwtUD9NIuIPBUQEiooy/m/8INKT4vn6C5+xr6zS75JEpAUKa0CY2VgzW2Nm68zsgRDL7zezlWb2uZm9Z2bdg5bVmNkS7zE7nHVGopTEOKbcPISdpeV8a8ZSamvVHiEijStsAWFm0cAU4AqgH3CjmfU7ZrXFQK5zbiAwE/hl0LJDzrlB3mNcuOqMZIMy2/HDq/rx3uqd/HH+er/LEZEWJpxHEOcC65xzG5xzlcA04JrgFZxzc5xzZd7kx0BGGOtpkm49vztfHtiZx95Zw0frd/tdjoi0IOEMiK5AQdB0oTevLv8JvB00nWBmeWb2sZldG2oDM7vTWyevuLj4zCuOQGbGo18dSFZaIve8tJid6hZcRBpJRDRSm9ktQC7wq6DZ3Z1zucBNwONm1vPY7ZxzTznncp1zuenp6Y1UbeNrEx/DH24eyoGKKu55aTHVNbV+lyQiLUA4A6IIyAyazvDmHcXMxgAPAuOccxWH5zvniry/G4C5wOAw1hrx+nRK4ufXDeCTjXv49b/W+l2OiLQA4QyIRUBvM8s2szhgInDU1UhmNhj4E4Fw2Bk0P8XM4r3nacBIYGUYa20SvjIkgxvP7cYf5q7nvVU7/C5HRJq5egWEmSWaWZT3PMfMxplZ7Im2cc5VA3cD7wCrgBnOuRVm9oiZHb4q6VdAG+DlYy5n7QvkmdlSYA7wqHOuxQcEwE+u7kf/LsncN30JBXvKTr6BiMhpsvr092Nm+cCFQAqwkMDRQaVz7ubwlld/ubm5Li8vz+8yGsXm3Qf58u8WkJ2WyMt3nU98TLTfJYlIE2Vm+V5773Hqe4rJvMtRvwL83jl3A9C/oQqUU9M9NZFf33AOnxeW8NM3Vvldjog0U/UOCDM7H7gZeNObp5+tPvpS/07ceVEPnvt4s4YqFZGwqG9ATAa+D8zy2hF6EGgbEB995/I+DMtK4fuvLGPdTg0yJCINq14B4Zyb55wb55z7hddYvcs5d2+Ya5OTiI2O4smbhtA6Lpq7nv9MgwyJSIOq71VML5pZspklAsuBlWb2nfCWJvXRMTmB304czPriAzw4S4MMiUjDqe8ppn7Ouf3AtQS6w8gG/iNsVckpGdkrjfvH5PDqkq28oEGGRKSB1DcgYr37Hq4FZjvnqgD9VI0g37g4MMjQI6+vZFlhid/liEgzUN+A+BOwCUgE5nvjNuwPV1Fy6g4PMpTWJo6vvZBPSVmV3yWJSBNX30bqJ5xzXZ1zV7qAzcDFYa5NTlFKYhxP3jyEHfvLuX/GEg0yJCJnpL6N1G3N7DeHu9Y2s18TOJqQCDOkWwoPXtmX91bv5E/zN/hdjog0YfU9xTQVKAXGe4/9wNPhKkrOzG0jsrhqQGce++caPt6gQYZE5PTUNyB6Oud+4o0Ot8E59zDQI5yFyekLDDI0gO7tWwcGGSrVIEMicurqGxCHzOyCwxNmNhI4FJ6SpCEkJcTy+1uGUFpexb0aZEhETkN9A+IuYIqZbTKzTcCTwP8LW1XSIM7qlMxPrx3Axxv28H/vapAhETk19b2Kaalz7hxgIDDQOTcYuCSslUmDuH5oBhOHZTJlznreX61BhkSk/k5pRDnn3H7vjmqA+8NQj4TBQ+P6069zMvdNX6pBhkSk3s5kyFFrsCokrBJio/nDLUOorXXc/eJnVFTX+F2SiDQBZxIQugurCememsivbjiHpYUl/OxNDTIkIicXc6KFZlZK6CAwoFVYKpKwGXt2J/77wmz+/MFGcrPaM+6cLn6XJCIR7IQB4ZxLaqxCpHF8d+xZLN6yjwf+/jn9OifRq4P+E4tIaGdyikmaoMODDLWKjeZrz39GWaUGGRKR0BQQLVCntoFBhtYVH+DBWcs1yJCIhKSAaKEu6J3G5EtzmLW4iJc+LfC7HBGJQAqIFuyeS3pxUU46D81eoUGGROQ4CogWLCrKeHzCIFLbxPH1FzXIkIgcTQHRwrVPjOPJm4awbV8533p5qdojROQIBYQwtHsKP7iyL++u2sFTGmRIRDwKCAHgjpFZXDmgE798Zw2faJAhEUEBIR4z4xdfHUi39q35+guf8frSrTrdJNLCKSDkiKSEWP5861A6Jidwz0uLGf+nj1hepKubRFoqBYQcpVeHJF6/5wL+9ysD2FB8kKufXMB3Zy7VsKUiLZACQo4THWXceG435nxnNP99YQ9mLS7iksfm8cd569VVuEgLooCQOiUnxPKDK/vyz/tGMbxHex59ezVf+r/5/HPFdrVPiLQACgg5qey0RP5y2zD+Nulc4qKjuPO5fG756yes2V7qd2kiEkYKCKm3i3LSefubF/LwuP4sL9rPFb+dz49eXc6eg5V+lyYiYaCAkFMSEx3FbSOymPvt0fzH8O68+OkWRv9qDk8v3EhVTa3f5YlIA1JAyGlJSYzj4WvO5u1vXsg5me14+PWVXPHbD5i7ZqffpYlIAwlrQJjZWDNbY2brzOyBEMvvN7OVZva5mb1nZt2Dlt1mZl94j9vCWaecvpyOSfxt0rn85dZcqmtquf3pRUx6ZhHriw/4XZqInCEL19UoZhYNrAUuAwqBRcCNzrmVQetcDHzinCszs68Bo51zE8ysPZAH5BIYEzsfGOqc21vX++Xm5rq8vLyw/Fukfiqra3nmw4387r11HKqq4bYRWdx7aW/ator1uzQRqYOZ5TvnckMtC+cRxLnAOufcBudcJTANuCZ4BefcHOdcmTf5MZDhPb8c+Jdzbo8XCv8CxoaxVmkAcTFR3HlRT97/9miuH5rB1IUbufixubzwyWZqanVZrEhTE86A6AoED1VW6M2ry38Cb5/KtmZ2p5nlmVlecXHxGZYrDSU9KZ5HvzqQ1+++gF4d2vDgrOV8+XcL+Gi9OgEUaUoiopHazG4hcDrpV6eynXPuKedcrnMuNz09PTzFyWk7u2tbpt85nN/fPIT9h6q48c8fc9dz+WzZXXbyjUXEd+EMiCIgM2g6w5t3FDMbAzwIjHPOVZzKthL5zIwrB3TmvW+N4ttfymHe2mLG/GYev/zHag5UVPtdnoicQDgbqWMINFJfSuDLfRFwk3NuRdA6g4GZwFjn3BdB89sTaJge4s36jEAj9Z663k+N1E3D9pJyfvmP1byyuIj0pHi+e3kfvjokg6go87s0kRbJl0Zq51w1cDfwDrAKmOGcW2Fmj5jZOG+1XwFtgJfNbImZzfa23QP8D4FQWQQ8cqJwkKajU9sEfjNhELO+PoKu7VrxnZmfc+3vF5K/Wf95RSJN2I4gGpuOIJqe2lrHa0uLePTt1ezYX8E1g7rwvbFn0aVdK79LE2kx/LrMVeSEoqKM6wZn8P63RnPPJb34x/LtXPLruTz+7loOVapbcRG/KSDEd4nxMXzrS3149/5RXHpWRx5/9wsu/fVcZuYXUl6loBDxi04xScT5ZMNuHnljJSu27qdtq1iuG9yVCcMy6ds52e/SRJqdE51iUkBIRKqtdXy8cTfTFxXw9vLtVFbXck5GWyYM68bV53QmKUHdd4g0BAWENGn7yip5dXER0xYVsHp7Ka1io7lqYGcmDstkaPcUzHSJrMjpUkBIs+Cc4/PCEqYtKmD2kiIOVtbQMz2RicO68ZUhXUltE+93iSJNjgJCmp2DFdW8uWwb0xcVkL95L7HRxmX9OjJhWDcu6JVGtG68E6kXBYQ0a1/sKGX6ogL+/lkhe8uq6NquFTfkZnBDbiZddU+FyAkpIKRFqKiu4d2VO5m2aAsL1u0C4KLe6UwclsmlfTsSF6OrukWOpYCQFqdgTxkv5xfycl4B20rKSU2M46tDMxifm0mvDm38Lk8kYiggpMWqqXXM/6KY6Z8W8O6qHVTXOoZlpTBhWDeuHNCJ1nExfpco4isFhAhQXFrBK58VMn1RARt2HSQpPoZxg7owcVg3zu6arMtlpUVSQIgEcc6Rt3kv0z4t4M1lWymvqqVf52QmnpvJNed0pW1r3YQnLYcCQqQO+8urmL1kK9MXFbCsqIT4mCiuHNCZCcMyOS+7vY4qpNlTQIjUw/KiEmbkFTBrcRGl5dVkpbZm/LBMrh+SQYfkBL/LEwkLBYTIKSivquHt5duY9mkBn2zcQ3SUcXGfDozPzeDiszoQG63LZaX5OFFA6BIOkWMkxEZz3eAMrhucwcZdB5mRV8DM/ELeXbWD9KR4vjKkK+NzM+mZrstlpXnTEYRIPVTX1DJ3TTHT8wp4f/VOarzLZcfnZnLVwM66XFaaLJ1iEmlAO0vLeeWzImZ4l8smxkVz9TldGD8sk8GZ7dSwLU2KAkIkDJxz5G/ey/RFBbzx+TYOVdWQ07EN43MzuW6wepeVpkEBIRJmByqqeWPpVqbnFbB4yz5io40xfTsyflgmF/VOV++yErEUECKNaO2OUmYsKuCVxUXsOVhJ57YJXD80gxuGZtIttbXf5YkcRQEh4oPK6lreW7WDGXkFzFtbTK2D83ukMmFYJmPP7kRCbLTfJYooIET8tq3kEH/PL2RGXiFb9pSRnBDDNYO6MmFYJmd3bet3edKCKSBEIkRtrePjjbuZsaiAt5dvp6I60A/U+NwMrh3clXat4/wuUVoYBYRIBCo5VMXsJUVMzytgedF+4mKiuLx/JybkZjKiZypRatiWRqCAEIlwK7aW8HJeIbMWF1FyqIqMlFbcMDST63MzNGyqhJUCQqSJKK+q4Z8rdzBjUQEL1u3CDC7olcaEYZmM6dtRDdvS4BQQIk3Q4WFTZ+YVsLWknPiYKM7rkcronHRG9UmnR1qi7tqWM6aAEGnCamodH63fzfurdzJ37U42FB8EICOlFaP7pDMqpwMjeqaSGK/+oOTUKSBEmpGCPWXMW1vM3DXFfLh+F2WVNcRGG8Oy2jPKO7ro0zFJRxdSLwoIkWaqsrqWvM17mLemmHlri1m9vRSATskJjMpJZ3SfdEb0SqNtKw2jKqEpIERaiG0lh5i/NhAWH3yxi9LyaqKjjCHd2jG6TwdG5aTTr3OyLqGVIxQQIi1QdU0tiwv2MW9NMXPX7mR50X4A0trEc1FOGqNy0rmwdzrtE3VzXkumgBARiksr+OCLQNvFB18Us7esCjM4J6PdkbaLczLaqefZFkYBISJHqal1LCsqYe6ancxbW8ySgn04B+1ax3Jh73RG5aRzUU4aHZIS/C5VwkwBISIntPdgJR+s23WksXvXgQoA+ndJ9hq7OzC4Wztio6N8rlQamm8BYWZjgd8C0cBfnHOPHrP8IuBxYCAw0Tk3M2hZDbDMm9zinBt3ovdSQIg0jNpax8pt+5m3tph5a4rJ37KXmlpHUnwMI3qlMigzhX5dkunXOZn0JI2a19T5EhBmFg2sBS4DCoFFwI3OuZVB62QBycC3gdnHBMQB51yb+r6fAkIkPPaXV/Hhul1Hrowq3HvoyLL0pHj6dU6mb+fkI6GRnZaodowm5EQBEc5bL88F1jnnNnhFTAOuAY4EhHNuk7esNox1iMgZSE6IZezZnRl7dmcASsqqWLltf+CxNfD3w/UbqKoJ/NhMiI2iT6dAWBwOjbM6JelO7yYonP/FugIFQdOFwHmnsH2CmeUB1cCjzrlXj13BzO4E7gTo1q3bGZQqIvXVtnUs5/dM5fyeqUfmVVbXsm7ngaDQKOGtZdt46dMtAJhBVmriUaHRr0syHZLidcd3BIvkSO/unCsysx7A+2a2zDm3PngF59xTwFMQOMXkR5EiAnExUYEv/i7JMDQwzznH1pLyQGB4ofF50T7eXLbtyHbtE+OOC40eaYnE+NgYfqiyhr1llYHHwSr2llWyr6ySvWWHn1d5y6toEx/Nf1/Yg1E56c0y6MIZEEVAZtB0hjevXpxzRd7fDWY2FxgMrD/hRiISMcyMru1a0bVdKy7r1/HI/P3lVazeVsrKrSVHTlU9s3ATlTWBM81xMVH06Zh0JDD6dk6mb+ckkhJOrbuQ2lpHaXk1e7wv+31HfeH/+++eg5VHzauorvuMd5v4GNq1jiWldRztWseyofggtz+9iCHd2jF5TA4X9k5rVkERzkbqGAKN1JcSCIZFwE3OuRUh1n0GeONwI7WZpQBlzrkKM0sDPgKuCW7gPpYaqUWarqqaWjYUH2TltpIj7Rort+5nb1nVkXW6tW99JDSy0hI5WFHt/coP/Jo/9lf+vrJKauv4eosyaOd9yae0jvMesaQkBs+LpV3rONp789q1iiMu5ugjm8rqWl7OL2DK++vYWlLO0O4pTEEygqkAAAqISURBVB7Tmwt6NZ2g8PMy1ysJXMYaDUx1zv3MzB4B8pxzs81sGDALSAHKge3Ouf5mNgL4E1ALRAGPO+f+eqL3UkCINC/OOXbsrzguNDbtLjtqvfiYKO9L3PuSD/riP/y3feLRYZCUENOg/VFVVNfwcl4hU+asY1tJObndU7jvshxG9EyN+KDQjXIi0mwcqKimaO8hkhJiSGkdR6u4yBllr6K6hhmLCpgyZz3b95dzblZ7Jo/pzfkRHBQKCBGRRlReVcOMvAKmzFnHjv0VnJsdCIoRPdP8Lu04CggRER+UV9Uw7dMt/H7uenaWVnBednvuuyyH4T1ST75xI1FAiIj4qLyqhpe8oCgureD8HqlMHtOb8yIgKBQQIiIRoLyqhhc/2cIf5gWCYkTPVO67LIdhWe19q0kBISISQcqranj+4838cd4Gdh2oYGSvVO4bk0OuD0GhgBARiUCHKmt44ZPN/HHeenYdqOTC3mlMHtObod0bLygUECIiEayssprnP97Mn+ZtYPfBw0GRw9DuKWF/bwWEiEgTUFZZzXMfbeZP8zew52AlF+Wkc9+Y3gzuFr6gUECIiDQhByuqee7jzTzlBcXoPulMHpPDoMx2Df5eCggRkSboYEU1z360iT/P38Desiou9oLinAYMCgWEiEgTdqCimmc/3MSfP9jAvrIqLjmrA5PH9GZgxpkHhQJCRKQZKC2v4m8fBU49lRyqYkzfDnzz0hwGZLQ97ddUQIiINCOl5VU8szBwRLG/vJqrBnTmyZsGn1aHgH6NSS0iImGQlBDLPZf25raRWTyzcBMV1TVh6S1WASEi0kQlJ8Ry76W9w/b6/g38KiIiEU0BISIiISkgREQkJAWEiIiEpIAQEZGQFBAiIhKSAkJEREJSQIiISEjNpqsNMysGNvtdxxlKA3b5XUQE0edxNH0e/6bP4mhn8nl0d86lh1rQbAKiOTCzvLr6RGmJ9HkcTZ/Hv+mzOFq4Pg+dYhIRkZAUECIiEpICIrI85XcBEUafx9H0efybPoujheXzUBuEiIiEpCMIEREJSQEhIiIhKSB8YmaZZjbHzFaa2Qoz+6Y3v72Z/cvMvvD+pvhda2Mxs2gzW2xmb3jT2Wb2iZmtM7PpZhbnd42NxczamdlMM1ttZqvM7PwWvm/c5/1/stzMXjKzhJa0f5jZVDPbaWbLg+aF3B8s4Anvc/nczIac7vsqIPxTDXzLOdcPGA58w8z6AQ8A7znnegPvedMtxTeBVUHTvwD+zznXC9gL/KcvVfnjt8A/nHNnAecQ+Fxa5L5hZl2Be4Fc59zZQDQwkZa1fzwDjD1mXl37wxVAb+9xJ/CH031TBYRPnHPbnHOfec9LCXwBdAWuAZ71VnsWuNafChuXmWUAVwF/8aYNuASY6a3Skj6LtsBFwF8BnHOVzrl9tNB9wxMDtDKzGKA1sI0WtH845+YDe46ZXdf+cA3wNxfwMdDOzDqfzvsqICKAmWUBg4FPgI7OuW3eou1AR5/KamyPA98Far3pVGCfc67amy4kEKAtQTZQDDztnXL7i5kl0kL3DedcEfAYsIVAMJQA+bTc/eOwuvaHrkBB0Hqn/dkoIHxmZm2AvwOTnXP7g5e5wDXIzf46ZDP7MrDTOZfvdy0RIgYYAvzBOTcYOMgxp5Nayr4B4J1bv4ZAcHYBEjn+dEuLFq79QQHhIzOLJRAOLzjnXvFm7zh8OOj93elXfY1oJDDOzDYB0wicOvgtgUPjGG+dDKDIn/IaXSFQ6Jz7xJueSSAwWuK+ATAG2OicK3bOVQGvENhnWur+cVhd+0MRkBm03ml/NgoIn3jn2P8KrHLO/SZo0WzgNu/5bcBrjV1bY3POfd85l+GcyyLQ+Pi+c+5mYA5wvbdai/gsAJxz24ECM+vjzboUWEkL3Dc8W4DhZtba+//m8OfRIvePIHXtD7OBW72rmYYDJUGnok6J7qT2iZldAHwALOPf591/QKAdYgbQjUD35eOdc8c2TjVbZjYa+LZz7stm1oPAEUV7YDFwi3Ouws/6GouZDSLQYB8HbADuIPCDrkXuG2b2MDCBwNV/i4H/InBevUXsH2b2EjCaQLfeO4CfAK8SYn/wQvRJAqfhyoA7nHN5p/W+CggREQlFp5hERCQkBYSIiISkgBARkZAUECIiEpICQkREQlJASEQys3QzW+D13nlt0PzXzKxLHdtc63V4eKrvNc7MTtjxnZl1MbOZJ1rndJnZJjNLO8k6PwjHe58qM3vIzL7tdx3SOBQQEqluBP4InAtMBjCzq4HFzrmtdWxzLRAyIILuuD2Oc262c+7RExXjnNvqnLv+ROuEWUQEhLQsCgiJVFUEeu2MB2q8L/jJwC9DrWxmI4BxwK/MbImZ9TSzuWb2uJnlAd80s6u98QMWm9m7ZtbR2/Z2M3vSe/6M15f+h2a2wcyu9+ZnHe6L31v/FTP7h9cX/y+D6vhPM1trZp+a2Z8Pv+4xtaaa2T+98Q3+AljQslfNLN9bdqc371ECPZkuMbMX6lovxPscOTIxs1wzm+s9H+W91hLvs0jy5n/HzBZ5Ywg8HPQ6D3r/pgVAn1DvJc2Uc04PPSLuAbQF3gTyCHStcC9w+0m2eQa4Pmh6LvD7oOkU/n1z6H8Bv/ae3w48GfQaLxP48dQPWOfNzwKWB62/wasxgcBdrJkEOpLbRODO3lgCd8o/GaLOJ4Afe8+vItDJWpo33d772wpYDqR60weOeY2Q6x2zzqag180F5nrPXwdGes/bEOgc8EsEBr4379/+BoEux4cSuNu/NZAMrCNwp7vv+4ge4X/Uedgt4ifnXAmBL8/DvXk+AFxnZn8m8EX/a+fcR/V4qelBzzOA6V7HZnHAxjq2edU5VwusPHyUEcJ7Xo2Y2UqgO4FuEOY5r/sLM3sZyAmx7UXAV7x/55tmtjdo2b1mdp33PJPAoC+7Q7xGfdcLZSHwG+9o5BXnXKGZfYlASCz21mnjvWYSMMs5V+b9m2bX8z2kGdApJmkKfgT8jEC7xAICHZM9VM9tDwY9/x2BX/QDgP9H4Nd/KMH9+Vg91qmBM/+x5fVDNQY43zl3DoEv6+NqrO96BPotOvz/+JHlLtDe8l8Ejj4WmtlZBP6d/+ucG+Q9ejnn/nqm/yZp2hQQEtHMrDeQ4ZybS+A0Ry2BUzKtQqxeSuAXb13a8u9uj287wXqnaxEwysxSvDaTr9ax3nzgJgAzu4LAEdHh+vY658q8L+3hQdtUWaB7+JOtF2wTgVNEBNdiZj2dc8ucc7/waj4LeAeYZIHxSTCzrmbWwav1WjNr5bVVXF2vT0KaBQWERLqfAQ96z18CvkbgS+23IdadBnzHa3jtGWL5Q8DLZpYP7GroQl1g5LOfA58SOI2zicDoZ8d6GLjIzFYQONW0xZv/DyDGzFYBjwIfB23zFPC5d1roROsd+z6/9Rrpa4LmT/YuH/6cwMUAbzvn/gm8CHxkZssIjEGR5ALD4k4HlgJvE/jspYVQb64iDcjM2jjnDnhHELOAqc65WX7XJXI6dAQh0rAeMrMlBK4s2kigz36RJklHECIiEpKOIEREJCQFhIiIhKSAEBGRkBQQIiISkgJCRERC+v+Ql8sGoDnX8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdata_vector, val_loss_vector, train_loss_vector = analyse_training_data_amount()\n",
    "\n",
    "plt.plot(tdata_vector, val_loss_vector, label='Validation Loss')\n",
    "plt.plot(tdata_vector, train_loss_vector, label='Training Loss')\n",
    "plt.xlabel(\"% training data used\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OVJVenpPhAE"
   },
   "source": [
    "### Experiment setup:\n",
    "1. We train the model for varying no. of layers, starting from layer=1 and increasing it everytime by 1 up till max lr=5 or till model validation loss values are not nan/inf.\n",
    "\n",
    "2. We record the model validation and training loss for each of these layers and plot a graph of Loss vs No. of layers to see its trend.\n",
    "\n",
    "3. Every other parameter remains constant.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "- From the graph in the above cell we see that the validation loss decreases with increasing learning rate, but then increases exponentially after a certain point.\n",
    "\n",
    "- Using the elbow method, we find optimal learning rate to be <= 0.6. After this, the model starts to diverge and hence loss increases.\n",
    "\n",
    "- Thus, increasing the learning rate increases the chances of the model converging faster, upto a certain point, after which it causes the model learning to diverge.\n",
    "\n",
    "- From the graph in the above cell we see that the validation loss decreases with increasing layer, but then increases exponentially after a certain point.\n",
    "\n",
    "- Using the elbow method, we find optimal no. of layers to be = 4. After this, the model starts to diverge and hence loss increases. This is because <>\n",
    "\n",
    "\n",
    "\n",
    "We see from the above graph that increasing the training data decreases the loss. This means that neural networks work better with as much training data as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq6eF7KSX7oS"
   },
   "source": [
    "Write down your analysis and conclusions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwprtbLqL12q"
   },
   "source": [
    "- \n",
    "-\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS5242_HW2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
