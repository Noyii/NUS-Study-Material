{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 [15% of your grade, 70 points in total]\n",
    "\n",
    "Hi! Welcome to assignment 3. Here, we are going to build a simple automatic speech recognition (ASR) system using the SpeechBrain framework, and check your understanding of some important concepts related to ASR. This assignment constitutes 15% of your final grade.\n",
    "\n",
    "You are required to:\n",
    "- Finish this notebook. Successfully run all the code cells and answer all the questions.\n",
    "- When you need to embed screenshot in the notebook, put the picture in './resources'.\n",
    "\n",
    "**Submission**\n",
    "After finishing, **zip the whole assignment directory (but please exclude \"datasets\" directory)**, then submit to Canvas. **Naming: \"eXXXXXXX_Name_Assignment3.zip\"**.\n",
    "\n",
    "**Late Policy**\n",
    "Please submit before **Wednesday, Recess Week, 27 September 2023, 23:59**. For each late day, your will get -25% marks.\n",
    "\n",
    "**Honor Code**\n",
    "Note that plagiarism will not be condoned. You may discuss the questions with your classmates or search on the internet for references, but you MUST NOT submit your code/answers that is copied directly from other sources. If you referred to the code or tutorial somewhere, please explicitly attribute the source somewhere in your code, e.g., in the comment.\n",
    "\n",
    "**Note** You might need to restart the jupyter kernel to clear the imported py files before running some code cells.\n",
    "\n",
    "**Useful Resources**\n",
    "- (Paper) [Recent Advances in End-to-End Automatic Speech Recognition](https://arxiv.org/abs/2111.01690)\n",
    "- (Code) [SpeechBrain ASR from Scratch](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing#scrollTo=IVCCe6cXPzJ0)\n",
    "- (Video) [End-to-End Models for Speech Processing](https://www.youtube.com/watch?v=3MjIkWxXigM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "We will continue using the same conda environment as the assignment 2, but some additional packages are needed.\n",
    "1. Enter the conda environment by:\n",
    "\n",
    "        conda activate 4347\n",
    "2. Install packages\n",
    "\n",
    "        # Install SpeechBrain and other libraries\n",
    "        pip install -r requirement.txt\n",
    "\n",
    "        # Install CMU Dictionary\n",
    "        python\n",
    "        nltk.download('cmudict')\n",
    "        exit()\n",
    "\n",
    "3. When you run this notebook in your IDE, switch the interpreter to the 4347 conda environment.\n",
    "4. You may be prompted to install the jupyter package. Click \"confirm\" in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 1 - Automatic Speech Recognition (ASR) [28 mark(s)]\n",
    "An automatic speech ASR system recognize spoken words from audio. If we build it using singing data, it becomes a lyric transcription system. As you have learned in the lecture, in recent decades, the performance of ASR systems has advanced significantly thanks to end-to-end (E2E) ASR models and large-scale open-source datasets.\n",
    "\n",
    "We are not going to build a well-performed E2E ASR system in this assignment because it's too demanding for both computation resources and scale of data. Instead, we will\n",
    "- Use phoneme as the recognition unit. In English, they have tighter relationship with the pronunciation, hence is less data-demanding.\n",
    "- Use a simple model with a toy dataset.\n",
    "- Train the model from scratch.\n",
    "- Decode the output without language model.\n",
    "\n",
    "This is just for simplicity and let you know the general idea of ASR system and SpeechBrain framework, but not what we do to solve real-world problems. For current state-of-the-art ASR systems, they tend to\n",
    "- Use grapheme (e.g., character, word, sub-word) as the recognition unit. This make the recognition workflow simpler.\n",
    "- Use huge models with huge datasets.\n",
    "- Transfer learning is commonly adopted -- systems are first trained with large-scale corpus from various domains, or even unlabeled data (audio-only, no text annotation), and then fine-tuned with some domain-specific labeled data.\n",
    "- Language models participate in the decoding process, making the output with higher fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we will be using phoneme as the target for the dataset, our goal is to recognize a sequence of spoken phonemes from audio. But many speech dataset do not provide phoneme annotation (as in this assignment). So we need to obtain the phoneme sequence from sentences ourselves.\n",
    "\n",
    "### Task 1: Prepare phoneme annotation  [4 mark(s)]\n",
    "1. Please finish the code of PhonemeUtil Class in utils.py, so that you can pass the below tests. Please using the CMU Dictionary in nltk to obtain the pronunciation. Use the first pronunciation if multiple ones exists for a word. If a word is not in the dictionary, mark its phoneme as \"\\<UNK\\>\".  **[2 mark(s)]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "phoneme_util = PhonemeUtil()\n",
    "sentences = [\n",
    "    \"This is a test asdfdsaf\",\n",
    "    \"For you phoneme tool\",\n",
    "    \"thhat ensure you can get\",\n",
    "    \"Correct labels\",\n",
    "]\n",
    "out = [phoneme_util.word_to_phoneme_sequence(s) for s in sentences]\n",
    "ans = [['DH', 'IH', 'S', 'IH', 'Z', 'AH', 'T', 'EH', 'S', 'T', '<UNK>'], ['F', 'AO', 'R', 'Y', 'UW', 'F', 'OW', 'N', 'IY', 'M', 'T', 'UW', 'L'], ['<UNK>', 'EH', 'N', 'SH', 'UH', 'R', 'Y', 'UW', 'K', 'AE', 'N', 'G', 'EH', 'T'], ['K', 'ER', 'EH', 'K', 'T', 'L', 'EY', 'B', 'AH', 'L', 'Z']]\n",
    "for i,j in zip(out, ans):\n",
    "    assert i == j\n",
    "print('Congratulations!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Run the code below to obtain phoneme annotation for tiny LibriSpeech dataset. After this, the phoneme annotations will be stored to 'phn' property in the annotation files for each audio.  **[2 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats!\n"
     ]
    }
   ],
   "source": [
    "phoneme_util = PhonemeUtil()\n",
    "dataset_dir = './datasets/tiny_librispeech'\n",
    "annot_dir_complete = jpath(dataset_dir, 'annotation_word')\n",
    "annot_dir_word = jpath(dataset_dir, 'annotation')\n",
    "if not os.path.exists(annot_dir_word):\n",
    "    os.mkdir(annot_dir_word)\n",
    "splits = ['train', 'valid', 'test']\n",
    "for split in splits:\n",
    "    annot_fp_old = jpath(annot_dir_complete, split+'.json')\n",
    "    annot_fp_new = jpath(annot_dir_word, split+'.json')\n",
    "    data = read_json(annot_fp_old)\n",
    "    for id in data:\n",
    "        entry = data[id]\n",
    "        sentence = entry['words']\n",
    "        phonemes = phoneme_util.word_to_phoneme_sequence(sentence)\n",
    "        data[id]['phn'] = ' '.join(phonemes)\n",
    "    save_json(data, annot_fp_new)\n",
    "data = read_json(jpath(dataset_dir, 'annotation', 'test.json'))\n",
    "\n",
    "t = 'R AA B AH N <UNK> S AO DH AE T HH IH Z D AW T S AH V W AA R AH N T AH N HH AE D B IH N AH N F EH R AH N D HH IY B IH K EY M AH SH EY M D AH V HH IH M S EH L F F AO R HH AA R B ER IH NG DH EH M'\n",
    "assert data['61-70970-0036']['phn'] == t\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Prepare tokenizer [3 mark(s)]\n",
    "In both training and inference, a tokenizer help to convert labels (in our case, phoneme annotations) from text to integer numbers so that the model can handle them easily.\n",
    "\n",
    "1. Please finish the code of PhonemeTonekizer Class in utils.py so that it can pass the cell below. **[3 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats!\n"
     ]
    }
   ],
   "source": [
    "from utils import PhonemeTokenizer\n",
    "tokenizer = PhonemeTokenizer()\n",
    "assert len(tokenizer.vocab) == 41\n",
    "assert tokenizer.token_to_id['<UNK>'] == 40\n",
    "assert tokenizer.id_to_token[0] == '<blank>'\n",
    "\n",
    "phn_seqs = [\n",
    "    ['CH', 'AO', 'B', 'T', 'S', 'OY'],\n",
    "    ['B', 'AE', 'AA', 'AH', 'ER', 'TH'],\n",
    "    ['<UNK>', 'D', 'B', '<UNK>', 'HH', 'TH']\n",
    "]\n",
    "ans = [\n",
    "    [8, 4, 7, 31, 29, 26],\n",
    "    [7, 2, 1, 3, 12, 32],\n",
    "    [40, 9, 7, 40, 16, 32],\n",
    "]\n",
    "\n",
    "assert tokenizer.encode_seq(phn_seqs[0]) == ans[0]\n",
    "assert tokenizer.encode_seq(phn_seqs[1]) == ans[1]\n",
    "assert tokenizer.encode_seq(phn_seqs[2]) == ans[2]\n",
    "assert tokenizer.decode_seq(ans[0]) == phn_seqs[0]\n",
    "assert tokenizer.decode_seq(ans[1]) == phn_seqs[1]\n",
    "assert tokenizer.decode_seq(ans[2]) == phn_seqs[2]\n",
    "assert tokenizer.decode_seq_batch(ans) == phn_seqs\n",
    "\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: ASR Baseline [8 mark(s)]\n",
    "\n",
    "We are now ready for building the first ASR system. Please finish the tasks below:\n",
    "\n",
    "1. The current code uses the validation set as the testing set, while the code for preparing the test data is missing. Please complete it. **[1 mark(s)]**\n",
    "2. Please use Checkpointer class of speechbrain to help you save the model with the lowest Phoneme Error Rate (PER) during training. Save the checkpoint under the directory \"results/baseline/best_ckpt\". **[1 mark(s)]**\n",
    "3. Load the best model (lowest PER) for evaluation, instead of using the model from the last epoch. **[1 mark(s)]**\n",
    "4. Please use speechbrain.utils.metric_stats.ErrorRateStats.write_stats to help you save the output of your model on the whole test set to help you know your model's performance better. In the output file, please use phoneme tokens instead of token ids (numbers). Save the file to \"results/baseline/results.txt\" **[1 mark(s)]**\n",
    "5. Please log your training, validation, and evaluation statistics to the result folder, in whatever way you like. **[1 mark(s)]**\n",
    "\n",
    "Run the training and testing by\n",
    "\n",
    "    python train.py hparam_baseline.yaml\n",
    "Expected PER: 90%.\n",
    "\n",
    "**NOTE**: Please keep the (1) training log, (2) model checkpoint and the (3) corresponding result files, when submitting you assignment. **[3 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/Users/niharika/Documents/Study_Material/Sem_3/Sound and Music/Assignments/Assignment 3/train_.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python train_baseline.py hparam_baseline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Modifying the Model [13 mark(s)]\n",
    "\n",
    "You may have spot some of the issues during the training, like the slow converging speed, overfitting, etc. Please make the following changes to your model by modifying the yaml file.\n",
    "1. (Please create a new .yaml file from the hparam_baseline.yaml, naming it hparam_modified.yaml) **[1 mark(s)]**\n",
    "2. Increase the N_epoch to 20. **[1 mark(s)]**\n",
    "3. Increase the learning rate to 5e-3 **[1 mark(s)]**\n",
    "4. Add weight decay = 0.1 to the optimizer **[1 mark(s)]**\n",
    "5. Add a variable named \"drop_p\", with value 0.2. **[1 mark(s)]**\n",
    "6. Add 3 dropout layers to the model, after act1, act2, and RNN. All with the same dropout rate of \"drop_p\" (you need to use a variable reference here). **[1 mark(s)]**\n",
    "7. Change the output_dir from \"results/baseline\" to \"results/drop0.2x2_lr0.005_wd0.1\". **[1 mark(s)]**\n",
    "\n",
    "There are some other changes you need to make in the train.py file:\n",
    "1. Use the speechbrain.nnet.schedulers.NewBobScheduler to schedule the learning rate or training according to loss on validation set. If the validation loss did not decrease after an epoch of training, use that scheduler to adjust the learning rate. **[2 mark(s)]**\n",
    "2. Before the training of each epoch, print out and log the current learning rate. **[1 mark(s)]**\n",
    "\n",
    "Run the training and testing by\n",
    "\n",
    "    python train.py hparam_modified.yaml\n",
    "Expected PER: 65%.\n",
    "\n",
    "**NOTE**: Please keep the (1) training log, (2) model checkpoint and the (3) corresponding result files, when submitting you assignment. **[3 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Epoch: 0, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:31<00:00,  2.53it/s, train_loss=3.69]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 10.69it/s]\n",
      "Epoch 0 complete\n",
      "Train loss: 3.69\n",
      "Stage.VALID loss: 3.43\n",
      "Stage.VALID PER: 100.00\n",
      "Epoch: 1, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:28<00:00,  2.84it/s, train_loss=3.39]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 13.56it/s]\n",
      "Epoch 1 complete\n",
      "Train loss: 3.39\n",
      "Stage.VALID loss: 3.27\n",
      "Stage.VALID PER: 99.91\n",
      "Epoch: 2, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:28<00:00,  2.77it/s, train_loss=3.07]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 13.29it/s]\n",
      "Epoch 2 complete\n",
      "Train loss: 3.07\n",
      "Stage.VALID loss: 2.86\n",
      "Stage.VALID PER: 90.28\n",
      "Epoch: 3, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:30<00:00,  2.60it/s, train_loss=2.55]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.17it/s]\n",
      "Epoch 3 complete\n",
      "Train loss: 2.55\n",
      "Stage.VALID loss: 2.56\n",
      "Stage.VALID PER: 78.06\n",
      "Epoch: 4, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:29<00:00,  2.73it/s, train_loss=2.19]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.48it/s]\n",
      "Epoch 4 complete\n",
      "Train loss: 2.19\n",
      "Stage.VALID loss: 2.39\n",
      "Stage.VALID PER: 71.86\n",
      "Epoch: 5, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:27<00:00,  2.86it/s, train_loss=1.99]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 13.19it/s]\n",
      "Epoch 5 complete\n",
      "Train loss: 1.99\n",
      "Stage.VALID loss: 2.38\n",
      "Stage.VALID PER: 69.28\n",
      "Epoch: 6, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:28<00:00,  2.85it/s, train_loss=1.84]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 13.03it/s]\n",
      "Epoch 6 complete\n",
      "Train loss: 1.84\n",
      "Stage.VALID loss: 2.31\n",
      "Stage.VALID PER: 65.23\n",
      "Epoch: 7, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:26<00:00,  2.97it/s, train_loss=1.74]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.99it/s]\n",
      "Epoch 7 complete\n",
      "Train loss: 1.74\n",
      "Stage.VALID loss: 2.19\n",
      "Stage.VALID PER: 63.94\n",
      "Epoch: 8, Train LR: 0.005\n",
      "100%|██████████████████████████| 80/80 [00:27<00:00,  2.90it/s, train_loss=1.66]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 13.05it/s]\n",
      "Epoch 8 complete\n",
      "Train loss: 1.66\n",
      "Stage.VALID loss: 2.22\n",
      "Stage.VALID PER: 63.68\n",
      "Epoch: 9, Train LR: 0.0025\n",
      "100%|██████████████████████████| 80/80 [00:28<00:00,  2.81it/s, train_loss=1.48]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.37it/s]\n",
      "Epoch 9 complete\n",
      "Train loss: 1.48\n",
      "Stage.VALID loss: 2.20\n",
      "Stage.VALID PER: 63.60\n",
      "Epoch: 10, Train LR: 0.0025\n",
      "100%|███████████████████████████| 80/80 [00:27<00:00,  2.90it/s, train_loss=1.4]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 13.09it/s]\n",
      "Epoch 10 complete\n",
      "Train loss: 1.40\n",
      "Stage.VALID loss: 2.18\n",
      "Stage.VALID PER: 62.56\n",
      "Epoch: 11, Train LR: 0.0025\n",
      "100%|██████████████████████████| 80/80 [00:26<00:00,  2.97it/s, train_loss=1.36]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.96it/s]\n",
      "Epoch 11 complete\n",
      "Train loss: 1.36\n",
      "Stage.VALID loss: 2.20\n",
      "Stage.VALID PER: 61.88\n",
      "Epoch: 12, Train LR: 0.00125\n",
      "100%|██████████████████████████| 80/80 [00:26<00:00,  2.97it/s, train_loss=1.24]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.81it/s]\n",
      "Epoch 12 complete\n",
      "Train loss: 1.24\n",
      "Stage.VALID loss: 2.12\n",
      "Stage.VALID PER: 58.78\n",
      "Epoch: 13, Train LR: 0.00125\n",
      "100%|███████████████████████████| 80/80 [00:27<00:00,  2.89it/s, train_loss=1.2]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.96it/s]\n",
      "Epoch 13 complete\n",
      "Train loss: 1.20\n",
      "Stage.VALID loss: 2.10\n",
      "Stage.VALID PER: 58.78\n",
      "Epoch: 14, Train LR: 0.00125\n",
      "100%|██████████████████████████| 80/80 [00:26<00:00,  3.01it/s, train_loss=1.16]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 13.08it/s]\n",
      "Epoch 14 complete\n",
      "Train loss: 1.16\n",
      "Stage.VALID loss: 2.15\n",
      "Stage.VALID PER: 58.00\n",
      "Epoch: 15, Train LR: 0.000625\n",
      "100%|██████████████████████████| 80/80 [00:27<00:00,  2.91it/s, train_loss=1.09]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.56it/s]\n",
      "Epoch 15 complete\n",
      "Train loss: 1.09\n",
      "Stage.VALID loss: 2.11\n",
      "Stage.VALID PER: 56.20\n",
      "Epoch: 16, Train LR: 0.000625\n",
      "100%|██████████████████████████| 80/80 [00:26<00:00,  3.00it/s, train_loss=1.07]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.90it/s]\n",
      "Epoch 16 complete\n",
      "Train loss: 1.07\n",
      "Stage.VALID loss: 2.11\n",
      "Stage.VALID PER: 56.54\n",
      "Epoch: 17, Train LR: 0.0003125\n",
      "100%|██████████████████████████| 80/80 [00:27<00:00,  2.86it/s, train_loss=1.03]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 11.66it/s]\n",
      "Epoch 17 complete\n",
      "Train loss: 1.03\n",
      "Stage.VALID loss: 2.14\n",
      "Stage.VALID PER: 56.28\n",
      "Epoch: 18, Train LR: 0.00015625\n",
      "100%|██████████████████████████| 80/80 [00:28<00:00,  2.81it/s, train_loss=1.02]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.80it/s]\n",
      "Epoch 18 complete\n",
      "Train loss: 1.02\n",
      "Stage.VALID loss: 2.14\n",
      "Stage.VALID PER: 56.54\n",
      "Epoch: 19, Train LR: 7.8125e-05\n",
      "100%|█████████████████████████| 80/80 [00:28<00:00,  2.83it/s, train_loss=0.975]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 12.62it/s]\n",
      "Epoch 19 complete\n",
      "Train loss: 0.97\n",
      "Stage.VALID loss: 2.15\n",
      "Stage.VALID PER: 56.71\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 23.38it/s]\n",
      "Stage.TEST loss: 2.22\n",
      "Stage.TEST PER: 59.76\n"
     ]
    }
   ],
   "source": [
    "!python train.py hparam_modified.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 2 - Questions [42 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Result Analysis [2 mark(s)]\n",
    "1. How does your system perform? Briefly introduce your system's performance with objective metric scores and the result file for the test set. **[2 mark(s)]**\n",
    "\n",
    "(Your Answer)\n",
    "\n",
    "System's performance:\n",
    "1. Validation PER: 56.713, Validation Loss: 2.152\n",
    "2. Test PER: 59.756, Test Loss: 2.224\n",
    "\n",
    "Results on test set: \n",
    "Average Word Error Rate (WER) = 57.53%\n",
    "\n",
    "Analysis:\n",
    "Let's take an example of a sample from the test data.\n",
    "\n",
    "```\n",
    "Reference sentence = \"ROBIN CAREFULLY DESCENDED THE LADDER AND FOUND HIMSELF SOON UPON FIRM ROCKY GROUND\",\n",
    "\n",
    "Alignment (1st sentence is ground truth, 3rd sentence is prediction, 2nd sentence shows min-edit operations on phonemes) = \n",
    "61-70970-0027, %WER 64.41 [ 38 / 59, 0 ins, 21 del, 17 sub ]\n",
    "R ;   AA  ;   B   ; AH ; N ; K  ;   EH  ;   R   ; F ;   AH  ; L ; IY ; D ; IH ; S ; EH ; N ;   D   ;   AH  ;   D   ;   DH  ;   AH  ; L ;   AE  ; D ; ER ; AH ; N ;   D   ; F ; AW ; N ; D  ; HH ;   IH  ;   M   ; S ; EH ; L ;   F   ; S ; UW ; N ; AH ; P ; AA ; N ; F ; ER ; M ;   R   ;   AA  ;   K   ; IY ; G ;   R   ;   AW  ; N ;   D  \n",
    "S ;   D   ;   D   ; =  ; S ; S  ;   D   ;   D   ; = ;   D   ; = ; =  ; = ; S  ; = ; =  ; S ;   D   ;   D   ;   D   ;   D   ;   D   ; = ;   D   ; = ; S  ; S  ; = ;   D   ; = ; S  ; S ; S  ; S  ;   D   ;   D   ; = ; S  ; = ;   D   ; = ; S  ; S ; =  ; = ; S  ; = ; = ; S  ; S ;   D   ;   D   ;   D   ; =  ; = ;   D   ;   D   ; = ;   D  \n",
    "W ; <eps> ; <eps> ; AH ; T ; ER ; <eps> ; <eps> ; F ; <eps> ; L ; IY ; D ; AH ; S ; EH ; T ; <eps> ; <eps> ; <eps> ; <eps> ; <eps> ; L ; <eps> ; D ; AY ; IH ; N ; <eps> ; F ; AA ; T ; AH ; N  ; <eps> ; <eps> ; S ; OW ; L ; <eps> ; S ; IY ; D ; AH ; P ; AO ; N ; F ; AH ; P ; <eps> ; <eps> ; <eps> ; IY ; G ; <eps> ; <eps> ; N ; <eps>\n",
    "```\n",
    "\n",
    "- This alignment shows that the sentence has a WER = 64.41%, wherein, we are required to do 21 deletion and 12 substitution operations on the phonemes in order to completely align the predictions to the ground truth.\n",
    "\n",
    "- Looking at the predictions, a high number of them were `<eps>` (no sound), which means that the system is not able to learn proper words at all.\n",
    "\n",
    "Overall, the performance of the system is not very good since a lot of operations (more than >50%) are required to align the predictions to the ground truth. This means that our system is predicting highly erroneous annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Tokenization [8 mark(s)]\n",
    "1. Do you think detecting phoneme sequence from speech recording is more difficult than detecting character or word sequence? Why? **[2 mark(s)]**\n",
    "(Your Answer)\n",
    "```\n",
    "I feel that detecting phoneme sequences from a speech recording can be more challenging than word sequences but less challenging than character sequences.\n",
    "\n",
    "1. Characters are the most granular part of a word. Therefore, detecting them from a speech recording can be exceptionally challenging since it highly depends on every character being enunciated properly. For e.g., \n",
    "    - if someone with an accent pronounces the word \"water\" as \"watahh\" vs \"waterr\", then character-level detection is difficult.\n",
    "    - Multiple words have silent characters in them, such as, \"jalapeno\" which is pronounced as \"halepeno\". In this case, the system will fail to detect the character properly.\n",
    "\n",
    "2. On the other hand, \n",
    "    - it is possible that the ground truth phonemes for the same word are pronounced differently by speakers with varied accents, or the same speaker in a different context. For e.g.,\n",
    "        - \"mate\" being pronounced as \"ma-ai-tt\" in an irish accent vs \"m-ae-t\" in indian accent.\n",
    "    - It is also possible that the speech recording is either too noisy or too fast, and a lot of the phonemes go completely undetected.\n",
    "\n",
    "3. However, detecting a sequence of words from a speech recording can be easier compared to the above 2 processes, since no matter how the pronunciations of the granular phonemes are, it will finally coalesce into a single meaningful word at the end. (This still might fail if the system is not robust to highly obscure accents). It is also relatively easy to detect the start and stop of a word, due to long pauses or silences (in a normal recording).\n",
    "```\n",
    "</br>\n",
    "\n",
    "2. For the task of speech recognition, what are the drawbacks of using phoneme as the detecting unit? **[2 mark(s)]**\n",
    "(Your Answer)\n",
    "```\n",
    "1. Modelling phonemes distribution:\n",
    "    - Modelling phonemes to recognize all of their contextual variations is a complex problem compared to directly recognizing words. \n",
    "    - Some languages have a large number of phonemes, wherein managing a large inventory of phonemes requires substantial computational resources and data. \n",
    "    - Annotating a substantial amount of phonetic data for training can be resource-intensive.\n",
    "\n",
    "2. Different speakers may pronounce phonemes differently due to accents. This variability of diverse speakers can affect system performance if the model is not robust.\n",
    "```\n",
    "</br>\n",
    "\n",
    "3. What is the advantage of sub-word tokenizer compared to word-level tokenizer? **[2 mark(s)]**\n",
    "(Your Answer)\n",
    "```\n",
    "1. Multiple sub-words come together to form a new word. This means that there are lesser sub-words than words. Therefore, using a sub-word tokenizer is advantageous since its vocabulary size would be much lesser compared to a word-level tokenizer, without any loss of information.\n",
    "\n",
    "2. Sub-word tokenization can break down words into smaller units, which helps in dealing with OOV words, i.e., previously unseen words can be represented by a combination of known sub-words, allowing the model to generalize better.\n",
    "```\n",
    "</br>\n",
    "\n",
    "4. If we are changing our tokenizer to the type of grapheme, which level do you think is the best, among {character, word, sub-word}? Please state your reason. **[2 mark(s)]**\n",
    "(Your Answer)\n",
    "```\n",
    "I think that a sub-word level tokenizer is the best amongst the three. \n",
    "\n",
    "1. It has lesser vocabulary size as compared to a word-level tokenizer as described above.\n",
    "2. It generalizes better to out-of-distribution words by breaking it down into smaller sub-units.\n",
    "3. For the context of speech recognition, it is easier to detect sub-words rather than characters due to the intuition of how natural language is spoken (more emphasis given to sub-words/phonemes than individual characters).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Modeling [7 mark(s)]\n",
    "Connectionist Temporal Classification (CTC) is a type of loss function that is commonly used in ASR, especially when we do not know the precise alignment between the annotation and the audio.\n",
    "\n",
    "1. Explain how does CTC deal with the misalignment issue between audio and annotation, i.e., the number of frames in the audio is much higher than the number of phoneme/character/sub-word/word in the annotation, and we do not know their correspondence. **[1 mark(s)]**\n",
    "2. Why does CTC need an additional blank token in the prediction? **[1 mark(s)]**\n",
    "3. Here are several decoded output from a CTC model. Write out their final recognition result. (\"-\" is CTC blank token, and \"_\" represent space) **[2 mark(s)]**\n",
    "\n",
    "    (1) heeel-ll-l_lllooo--wooooorld\n",
    "\n",
    "    (2) hhhhee-llow_wo--rr-rllll--dd\n",
    "    \n",
    "4. Recall the formula of CTC loss:\n",
    "   $$L_{CTC} = -log(\\sum_{\\pi \\in B^{-1}(W)} \\prod_{t=1}^Tp(\\pi_t|\\mathbf{x}_t))$$\n",
    "   Does this summation mark means that we have to list out all possible alignments between frames and texts, compute the probability for each pair, and add them together? Is there more efficient way to compute the CTC loss? If you think so, please briefly explain a more efficient algorithm. **[3 mark(s)]**\n",
    "\n",
    "(Your Answer)\n",
    "\n",
    "1. \n",
    "\n",
    "        CTC Loss: The purpose of CTC loss is to align transcripts with the audio features.\n",
    "        \n",
    "        Transcript: \"of\"\n",
    "\n",
    "        CTC will try to align the transcript by repeating the characters. CTC also introduces a blank token $\\epsilon$\n",
    "\n",
    "| $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ | \n",
    "|----|----|----|----|----|\n",
    "| $\\epsilon$ | $\\epsilon$ | o | o | f |\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "| $\\epsilon$ | o | o | f | f |\n",
    "|----|----|----|----|----|\n",
    "| $\\epsilon$ | o | o | o | f |\n",
    "\n",
    "\n",
    "        After all the possible alignments are iterated, CTC will compute the probablity of the word occurring given the audio features $P(W|X)$ and sum it.\n",
    "        The negative log of the sum is the CTC Loss.\n",
    "\n",
    "        So, if our model predict accurate words, then the probablity will be higher. Which means our loss will be lower.\n",
    "\n",
    "\n",
    "2. \n",
    "\n",
    "        Once CTC has found the word alignment, it will collapse characters that are the same.\n",
    "\n",
    "        o o o f f → of. This was a simple case and naively combining characters worked.\n",
    "\n",
    "        If we have multiple letters, this naive method will give erroneous outputs.\n",
    "\n",
    "        h h e e e l l l o o o o. --> helo. This is incorrect, we wanted it to be \"hello\". \n",
    "\n",
    "        To address this issue CTC introduces a blank token.\n",
    "\n",
    "3. \n",
    "Using beam search for CTC loss,\n",
    "\n",
    "        a. heeel-ll-l_lllooo--wooooorld -> helll_loworld\n",
    "        b. hhhhee-llow_wo--rr-rllll--dd -> helow_worrld\n",
    "\n",
    "4. \n",
    "\n",
    "        Yes, the formula is exactly what is described in the question. It is very inefficient to compute the probablities for all the alignments by listing them out. Instead, we can use beam search to iterate through all possible alignments efficiently using a heuristic (beam_size) and compute loss on the candidate with highest probability.\n",
    "\n",
    "```\n",
    "Beam Search Algorithm:\n",
    "        \n",
    "Initialization:\n",
    "\n",
    "1. Start with an empty list of candidate sequences.\n",
    "2. Initialize a single candidate sequence with a special \"start\" token (e.g., <start>).\n",
    "\n",
    "Generating Candidate Sequences:\n",
    "\n",
    "1. At each step, expand the top-k candidate sequences from the previous step. k is a hyperparameter = beam_size.\n",
    "\n",
    "2. For each candidate sequence, generate a set of next-token candidates using the probabilistic model.\n",
    "\n",
    "3. Compute the probability score for each next-token candidate based on the model's predictions.\n",
    "\n",
    "4. Combine the next-token candidates with their parent sequences to create new candidate sequences.\n",
    "\n",
    "5. Keep only the top-k candidate sequences based on their cumulative probability scores.\n",
    "\n",
    "Checking for Termination:\n",
    "\n",
    "1. Check if any of the candidate sequences end with a special \"end\" token (e.g., <end>). If so, mark them as completed sequences.\n",
    "2. Continue expanding candidates until a maximum length is reached or until a certain number of completed sequences are obtained.\n",
    "\n",
    "Termination and Output:\n",
    "\n",
    "1. Sort the completed sequences based on their cumulative probability scores.\n",
    "2. Return the top-ranked completed sequence as the final output.\n",
    "```\n",
    "\n",
    "References: \n",
    "1. https://medium.com/@kushagrabh13/modeling-sequences-with-ctc-part-2-14ab45ef896e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Language Model [7 mark(s)]\n",
    "1. Consider the two sequences below:\n",
    "    - A: I like Singapore's weather.\n",
    "    - B: I Singapore like ? weathers.\n",
    "\n",
    "    For a well-trained language model, which sentence will have lower perplexity from this model? Why? **[1 mark(s)]**\n",
    "\n",
    "```\n",
    "- Perplexity measures how well a language model can predict the next word in a sequence based on the preceding context. Since a well-trained language model must have been trained on vast amounts of text data that contain well-formed sentences, they are more likely to assign lower perplexity to grammatical and fluent sentences.\n",
    "\n",
    "- Therefore, sentence A will have lower perplexity from this model.\n",
    "\n",
    "- Whereas, sentence B contains non-standard language elements like the question mark in the middle of the sentence and the unusual word \"weathers\". These elements increase the perplexity of the sentence.\n",
    "```\n",
    "</br>\n",
    "\n",
    "2. Given the corpus below:\n",
    "\n",
    "            I love to play football\n",
    "            He loves to watch football\n",
    "            I love to watch movies\n",
    "            She loves to play tennis\n",
    "    (1) Assuming we are using a word-level tokenizer. Calculate the below bigram probability by #this bigram/#all bigram: **[3 mark(s)]**\n",
    "    [Not done since its not graded].\n",
    "    \n",
    "    a. P(love | I)\n",
    "\n",
    "    b. P(to | love)\n",
    "\n",
    "    c. P(football | play)\n",
    "    \n",
    "    d. P(movies | watch)\n",
    "   </br>\n",
    "   \n",
    "    (2) Use the probability you obtained above, calculate the probability of below sentences **[2 mark(s)]**\n",
    "    a. I love to watch football\n",
    "    b. She loves to play football\n",
    "   </br>\n",
    "   \n",
    "    (3) Why it's not a good idea to use a large n value for n-gram language models? **[1 mark(s)]**\n",
    "    ```\n",
    "    Longer values of n means longer sequences of words. These longer sequences are less likely to appear frequently in the training data (mostly only once or not at all), since the data will become extremely sparse. Therefore, without proper smoothning, it is difficult to estimate accurate probailities for them. This results in overfitting to the training data and poor model generalization.\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Beam Search [4 mark(s)]\n",
    "Assume we have a simplified language model that can predict the probability of next word. We have generated a start part of the sentence \"I want to\". Now we are using beam search to predict the rest of the sentence. Use letter \"G\" denote the generated part. Let's use beam size of 2 for this question.\n",
    "\n",
    "        Probability calculated by language model:\n",
    "        p(eat | G): 0.4\n",
    "        p(play | G): 0.3\n",
    "        p(go | G): 0.2\n",
    "        p(watch | G): 0.1\n",
    "        p(a sandwich | G eat): 0.5\n",
    "        p(dinner | G eat): 0.4\n",
    "        p(an apple | G eat): 0.1\n",
    "        p(football | G play): 0.6\n",
    "        p(games | G play): 0.4\n",
    "1. Let's continue the generation from G=\"I want to\". After the first step of beam search, what tokens will be selected, and what are the resulting candidate sequence? **[1 mark(s)]**\n",
    "\n",
    "```\n",
    "Tokens selected: [\"eat\", \"play\"]\n",
    "Candidate sequences: [\n",
    "        \"I want to eat a sandwich\",\n",
    "        \"I want to eat dinner\",\n",
    "        \"I want to eat an apple\",\n",
    "        \"I want to play football\",\n",
    "        \"I want to play games\"\n",
    "]\n",
    "```\n",
    "\n",
    "2. In the 2nd step of beam search, what are the two beams starting with \"G eat\"? What are their probability respectively? **[1 mark(s)]**\n",
    "\n",
    "```\n",
    "\"I want to eat a sandwich\" : 0.5*0.4 = 0.20\n",
    "\"I want to eat dinner\" : 0.4*0.4 = 0.16\n",
    "```\n",
    "\n",
    "3. In the 2nd step of beam search, what are the two beams starting with \"G play\"? What are their probability respectively? **[1 mark(s)]**\n",
    "\n",
    "```\n",
    "\"I want to play football\" : 0.6*0.3 = 0.18\n",
    "\"I want to play games\" : 0.4*0.3 = 0.12\n",
    "```\n",
    "\n",
    "4. What are the resulting candidate sequence from the 2nd step of beam search? **[1 mark(s)]**\n",
    "\n",
    "```\n",
    "Candidate sequences: [\n",
    "        \"I want to eat a sandwich\",\n",
    "        \"I want to play football\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Word Error Rate [3 mark(s)]\n",
    "\n",
    "Consider an automatic speech recognition system that transcribes a spoken segment into text. We compare the transcription of the system with a human-annotated reference transcript to calculate the system's Word Error Rate.\n",
    "\n",
    "Reference Transcript:\n",
    "\"I am excited to learn about speech recognition.\"\n",
    "\n",
    "System's Transcription (Hypothesis):\n",
    "\"I am excited learn about speech recognise.\"\n",
    "\n",
    "1. Calculate the number of insertions, deletions, and substitutions. **[1 mark(s)]**\n",
    "2. Compute the Word Error Rate (WER) using the formula: **[1 mark(s)]**\n",
    "$$WER=\\frac{\\text{Insertions}+\\text{Deletions}+\\text{Substitutions}}{\\text{Number of words in Reference}}$$\n",
    "3. Why might WER be a more resonable metric for ASR compared to a simple accuracy rate (correct words divided by total words)? **[1 mark(s)]**\n",
    "\n",
    "(Your Answer)\n",
    "\n",
    "1. \n",
    "\n",
    "<center>\n",
    "\n",
    "|I| am| excited |to |learn |about |speech |recognition|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|=|=|=|I|=|=|=|S|\n",
    "|I| am |excited | |learn| about| speech| recognise|\n",
    "\n",
    "</center>\n",
    "\n",
    "        Insertions: 1\n",
    "        Deletions: 0\n",
    "        Substitutions: 1\n",
    "\n",
    "2. \n",
    "\n",
    "        WER = 2 / 8 = 0.25\n",
    "3. \n",
    "\n",
    "        Simply calculating the accuracy will not capture the ordering. In ASR system we care about the ordering and alignment of the phonemes/words/sub-words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Possible Improvement [3 mark(s)]\n",
    "1. The performance of the recognition system in Section 1 might still have room to improve. What are possible reasons for the not-so-good performance, and directions of improvement? Please list 3 pairs of them. **[3 mark(s)]**\n",
    "\n",
    "```\n",
    "1. Hyperparameter Tuning: By analysing the validation and test logs, we can see that the final validation and test PER scores are similar and the model hasn't overfit yet. This means that we can perform some hyperparamater tuning to get the model to converge, e.g., increase number of epochs.\n",
    "\n",
    "2. Add regularization: If the convergence rate is too slow, it means that the model is not learning new and complex patterns easily. We can further add regularization to our model - in the form of dropout layers, increased dropout rate, or regularization in the model objective.\n",
    "\n",
    "3. More training data: Increasing training data increases variance of the model. Therefore, more complex patterns and phoneme relationships in the annotations can be learnt.\n",
    "\n",
    "All these methods in combination can help increase system performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Speech vs Singing [6 mark(s)]\n",
    "1. What are the properties that are different between audio of from speech recording and that of singing recording? What are the similar/same properties that are shared between them? **[2 mark(s)]**\n",
    "2. What are the properties that are different between spoken texts and lyrics? What are the similar/same properties that are shared between them? **[1 mark(s)]**\n",
    "3. Given the limited paired singing dataset of audio and lyric, how can we build a lyric transcription system with better performance? Please answer from 3 perspectives. **[3 mark(s)]**\n",
    "\n",
    "(Your Answer)\n",
    "\n",
    "1. \n",
    "\n",
    "        Differences:\n",
    "\n",
    "            - Range of Pitch: Speaking voice normally ranges from 75Hz to 600Hz, but the range of pitch in singing voice is very big (eg: opera)\n",
    "            - Loudness: Speaking voice is usually not loud, whereas singing voice can vary a lot.\n",
    "            - Rate of Speech: Speaking voice has a standard speaking rate, but singing voice depends on the genre of music. (Rap: Fast, Opera: Slow)\n",
    "            - Periodicity: While both types are apreriodic, Singing voice follows a rhythm and generally has patterns.\n",
    "            - Sampling Rate: Sampling Rate of singing voice needs to much higher, in order to reliably reconstruct the audio.\n",
    "\n",
    "        Similarities:\n",
    "\n",
    "            - Both involve content that can be broken down into phonemes. While the articulation/intonation will be different, the phonemes dont change.\n",
    "            - Both produce sounds that fall within our hearing range.\n",
    "            - Both are aperiodic\n",
    "\n",
    "2. \n",
    "\n",
    "        Differences:\n",
    "\n",
    "            - Intonation: Lyrics contain information on how to say the words.\n",
    "            - Rhythm: Lyrics often follow a rhythm based on the instruments. Spoken texts do not have this constraint.\n",
    "        \n",
    "        Similarities:\n",
    "\n",
    "            - Language: Both are represented using some form of language.\n",
    "            - Grammar: Both follow the syntax and rules of grammar (atleast for english)\n",
    "\n",
    "3. \n",
    "        Model Architecture: \n",
    "        \n",
    "            - We can add attention mechanism to our architecture. Lyrics have a strong dependence on the rhythm of the music. The stressors in phoneme are more important in the case of lyrics, compared to spoken voice.\n",
    "\n",
    "        Training:\n",
    "\n",
    "            - Incorporate feedback from human annotators. Humans will evaluate the misclassified results and the feedback will be used to fine tune the model. (eg: RLHF).\n",
    "            - Incorporate a language model over the outputs to significantly improve accuracy.\n",
    "            - Add regularization during the trianing process (Dropout layers/L2/L1).\n",
    "            - Tune the hyperparameters such as learning rate, dropout rate etc so that the model converges.\n",
    "\n",
    "        Data Augmentation: \n",
    "\n",
    "            - Generate synthetic data from the training data distribution to increase the size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Timing Survey [2 mark(s)]\n",
    "\n",
    "- What do you think is the most difficult part? Which part did you spent most time on it? **[1 mark(s)]**\n",
    "```\n",
    "Answering the theory questions and understanding the SpeechBrain library.\n",
    "```\n",
    "</br>\n",
    "\n",
    "- How much time did you spent on the assignment? Please fill an estimated time here if you did not time yourself. **[1 mark(s)]**\n",
    "```\n",
    "3 days.\n",
    "```\n",
    "</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0dadc3a26be8ee61448f9b2cf38da1ad7bf9dce237469de4abf8b731c62695b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
