{"cells":[{"cell_type":"markdown","metadata":{"id":"48JBRWUfTS58"},"source":["# Warm-ups: Implement label flipping attcks"]},{"cell_type":"markdown","metadata":{"id":"wrf2BSSe6INY"},"source":["## Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lgj7hu7yTLcX"},"outputs":[],"source":["import copy\n","import numpy as np\n","import os"]},{"cell_type":"markdown","metadata":{"id":"9CTpbhu88cb0"},"source":["### If you are using Google Colab, you need to upload this notebook and the codebase to your Google Drive. Then you need to mount your Google Drive in Colab and set your working directory. If you are running on your local machine, you can ignore the following line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WREld4LanEkm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWxeNJwv6kW9"},"outputs":[],"source":["root_dir = \"/content/drive/My Drive/\"\n","project_dir = \"CS5562 2023 Spring/Robustness/Assignment 2\" # Change to your path\n","os.chdir(root_dir + project_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FDukDiM7q7E"},"outputs":[],"source":["# Make sure the path is correct\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"pkPrPU2l55f-"},"source":["## Implement random label flipping attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6hgEjg_nyLX"},"outputs":[],"source":["from utilities import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcQuG9un6B6S"},"outputs":[],"source":["from attack import Attack\n","\n","class Random_Label_Flip_Attack(Attack):\n","    \"\"\"\n","    Random label flipping attack\n","    \"\"\"\n","    def attack(self, eps):\n","        n_poison = int(eps * len(self.clean_dataset))\n","\n","        ####################\n","        # TODO: modify the following part to build your attack model based on label flipping attack\n","        index = np.random.choice(self.clean_dataset.X.shape[0], n_poison, replace=False)\n","        X, Y_modified = self.clean_dataset[index]\n","        Y_modified = Y_modified*(-1)\n","        ####################\n","        return dataset(X, Y_modified)"]},{"cell_type":"markdown","metadata":{"id":"HvWvx6-pn40i"},"source":["## Implement your label flipping attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJm4qUKI6Cfi"},"outputs":[],"source":["class Label_Flip_Attack(Attack):\n","    \"\"\"\n","        Label flipping attack: students implement their own label flipping attack here\n","    \"\"\"\n","    def attack(self, eps):\n","        n_poison = int(eps * len(self.clean_dataset))\n","\n","        ####################\n","        # TODO: modify the following part to build your attack model based on label flipping attack\n","        index = np.random.choice(self.clean_dataset.X.shape[0], n_poison, replace=False)\n","        X, Y_modified = self.clean_dataset[index]\n","\n","        ####################\n","        return dataset(X, Y_modified)"]},{"cell_type":"markdown","metadata":{"id":"bjLMJoXh-1Qt"},"source":["# Test your code"]},{"cell_type":"markdown","metadata":{"id":"dOZdeGvh61-q"},"source":["## Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EPdpd3CHG2C"},"outputs":[],"source":["def compute_attack_grade(attack, victim_model,eps,clean_train_dataset,test_dataset):\n","    # target model structure is known to the adversary\n","    target_model = copy.deepcopy(victim_model)\n","    if attack == 'KKT':\n","        attacker = KKT_Attack(target_model,clean_train_dataset,test_dataset)\n","    elif attack == 'label-flip':\n","        attacker = Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n","    elif attack == 'adaptive':\n","        attacker = Adaptive_Attack(target_model, clean_train_dataset, test_dataset)\n","    elif attack == 'random-label-flip':\n","        attacker = Random_Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n","    poisoned_dataset = attacker.attack(eps)\n","    assert len(poisoned_dataset) <= int(eps*len(clean_train_dataset))\n","\n","    train_dataset = combine_datset(clean_train_dataset,poisoned_dataset)\n","    clean_model = copy.deepcopy(target_model)\n","\n","    # performance without any attack\n","    clean_model.train(clean_train_dataset)\n","    clean_loss,clean_acc = clean_model.score(test_dataset)\n","    print('\\nAvg loss of clean model: %0.5f, avg classification accuracy: %0.5f'%(clean_loss,clean_acc))\n","\n","    # attack the victim model\n","    victim_model.train(train_dataset)\n","    poisoned_loss,poisoned_acc =victim_model.score(test_dataset)\n","    print('\\nAvg loss of poisoned model:%0.5f, avg classification accuracy: %0.5f'%(poisoned_loss,poisoned_acc))\n","\n","    grade = poisoned_loss - clean_loss\n","\n","    # # for generating figures\n","    # distance_to_center_diff(clean_train_dataset,poisoned_dataset)\n","    # loss_diff(clean_train_dataset, poisoned_dataset,clean_model)\n","\n","    return len(poisoned_dataset)/len(clean_train_dataset),grade"]},{"cell_type":"markdown","metadata":{"id":"grFB2DU28kbY"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"0sb62Syv-QXm"},"source":["### Random label flipping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7pntdu1_dvx"},"outputs":[],"source":["from model import Undefended_Model\n","\n","train_dataset,test_dataset = load_dataset('mnist_17')\n","base_model = load_model(\"nn\", \"mnist_17\")\n","target_model = Undefended_Model(base_model,\"nn\")\n","defense_name = 'undefended'\n","fraction, attack_grade = compute_attack_grade(\"random-label-flip\", target_model, 0.2, train_dataset, test_dataset)\n","print('\\n\\n-----------result---------')\n","print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"random-label-flip\",defense_name,\"nn\",\"mnist_17\",attack_grade,fraction))"]},{"cell_type":"markdown","metadata":{"id":"r9wkcikfHrHR"},"source":["### Label flipping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qB0n2Eb2Hk6I"},"outputs":[],"source":["from model import Undefended_Model\n","\n","train_dataset,test_dataset = load_dataset('mnist_17')\n","base_model = load_model(\"svm\", \"mnist_17\")\n","target_model = Undefended_Model(base_model,\"svm\")\n","defense_name = 'undefended'\n","fraction, attack_grade = compute_attack_grade(\"label-flip\", target_model, 0.2, train_dataset, test_dataset)\n","print('\\n\\n-----------result---------')\n","print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"label-flip\",defense_name,\"svm\",\"mnist_17\",attack_grade,fraction))"]},{"cell_type":"markdown","metadata":{"id":"hXVnAKn2IOPB"},"source":["# Report"]},{"cell_type":"markdown","metadata":{"id":"SQ8-Y7EGIQeO"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('torch')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.12"},"vscode":{"interpreter":{"hash":"5c32e4f2e3a56fb35e93c35983d6902bd49db4719b2e0a10b71e4a5eb06b0593"}}},"nbformat":4,"nbformat_minor":0}
