{"cells":[{"cell_type":"markdown","metadata":{"id":"48JBRWUfTS58"},"source":["# Task 2: Data Sanitization"]},{"cell_type":"markdown","metadata":{"id":"wrf2BSSe6INY"},"source":["## Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lgj7hu7yTLcX"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","import os\n","import copy"]},{"cell_type":"markdown","metadata":{"id":"9CTpbhu88cb0"},"source":["### If you are using Google Colab, you need to upload this notebook and the codebase to your Google Drive. Then you need to mount your Google Drive in Colab and set your working directory. If you are running on your local machine, you can ignore the following line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WREld4LanEkm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWxeNJwv6kW9"},"outputs":[],"source":["root_dir = \"/content/drive/My Drive/\"\n","project_dir = \"CS5562 2023 Spring/Robustness/Assignment 2\" # Change to your path\n","os.chdir(root_dir + project_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FDukDiM7q7E"},"outputs":[],"source":["# Make sure the path is correct\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"pkPrPU2l55f-"},"source":["## Implement data sanitizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6hgEjg_nyLX"},"outputs":[],"source":["from utilities import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcQuG9un6B6S"},"outputs":[],"source":["def data_sanitizer(training_data, estimate_eps):\n","    \"\"\"\n","       Removes the estimate_eps fraction of points from X and Y.\n","    \"\"\"\n","\n","    n_est_poisoned = int(estimate_eps * len(training_data))\n","\n","    #################\n","    # TODO: decide which points need to be deleted\n","    index = np.random.choice(training_data.X.shape[0], n_est_poisoned, replace=False)\n","\n","    ################\n","    training_data_copy = copy.deepcopy(training_data)\n","    del training_data_copy[index]\n","    return training_data_copy"]},{"cell_type":"markdown","metadata":{"id":"bjLMJoXh-1Qt"},"source":["# Test your code"]},{"cell_type":"markdown","metadata":{"id":"dOZdeGvh61-q"},"source":["## Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjC-8NijLp57"},"outputs":[],"source":["from model import Model\n","\n","\n","class Data_Sanitized_Model(Model):\n","    def __init__(self, model, model_name, estimated_eps):\n","        super().__init__(model, model_name)\n","        self.estimated_eps = estimated_eps\n","\n","    def train(self, train_dataset):\n","        sanitized_data = data_sanitizer(training_data=train_dataset, estimate_eps=self.estimated_eps)\n","        self.model.fit(sanitized_data.X, sanitized_data.Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EPdpd3CHG2C"},"outputs":[],"source":["def compute_attack_grade(attack, victim_model,eps,clean_train_dataset,test_dataset):\n","    # target model structure is known to the adversary\n","    target_model = copy.deepcopy(victim_model)\n","    if attack == 'KKT':\n","        attacker = KKT_Attack(target_model,clean_train_dataset,test_dataset)\n","    elif attack == 'label-flip':\n","        attacker = Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n","    elif attack == 'adaptive':\n","        attacker = Adaptive_Attack(target_model, clean_train_dataset, test_dataset)\n","    elif attack == 'random-label-flip':\n","        attacker = Random_Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n","    poisoned_dataset = attacker.attack(eps)\n","    assert len(poisoned_dataset) <= int(eps*len(clean_train_dataset))\n","\n","    train_dataset = combine_datset(clean_train_dataset,poisoned_dataset)\n","    clean_model = copy.deepcopy(target_model)\n","\n","    # performance without any attack\n","    clean_model.train(clean_train_dataset)\n","    clean_loss,clean_acc = clean_model.score(test_dataset)\n","    print('\\nAvg loss of clean model: %0.5f, avg classification accuracy: %0.5f'%(clean_loss,clean_acc))\n","\n","    # attack the victim model\n","    victim_model.train(train_dataset)\n","    poisoned_loss,poisoned_acc =victim_model.score(test_dataset)\n","    print('\\nAvg loss of poisoned model:%0.5f, avg classification accuracy: %0.5f'%(poisoned_loss,poisoned_acc))\n","\n","    grade = poisoned_loss - clean_loss\n","\n","    # # for generating figures\n","    # distance_to_center_diff(clean_train_dataset,poisoned_dataset)\n","    # loss_diff(clean_train_dataset, poisoned_dataset,clean_model)\n","\n","    return len(poisoned_dataset)/len(clean_train_dataset),grade"]},{"cell_type":"markdown","metadata":{"id":"yshzy7i_Mk_d"},"source":["## Copy and Paste your KKT attack here:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLRNiKyuMo-t"},"outputs":[],"source":["from attack import Attack\n","\n","class KKT_Attack(Attack):\n","    \"\"\"\n","        KKT attack\n","    \"\"\"\n","    def attack(self, eps):\n","        n_poison = int(eps * len(self.clean_dataset))\n","\n","        ####################\n","        # TODO: update the following part to build your attack model based on KKT attack\n","        index = np.random.choice(self.clean_dataset.X.shape[0], n_poison, replace=False)\n","        X_modified, Y_modified = self.clean_dataset[index]\n","\n","\n","        ####################\n","\n","        return dataset(X_modified, Y_modified)"]},{"cell_type":"markdown","metadata":{"id":"grFB2DU28kbY"},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7pntdu1_dvx"},"outputs":[],"source":["train_dataset,test_dataset = load_dataset('mnist_17')\n","base_model = load_model(\"svm\", \"mnist_17\")\n","target_model = Data_Sanitized_Model(base_model,\"svm\", 0.2)\n","defense_name = 'data_sanitization'\n","fraction, attack_grade = compute_attack_grade(\"KKT\", target_model, 0.2, train_dataset, test_dataset)\n","print('\\n\\n-----------result---------')\n","print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"KKT\",defense_name,\"svm\",\"mnist_17\",attack_grade,fraction))"]},{"cell_type":"markdown","metadata":{"id":"hXVnAKn2IOPB"},"source":["# Report"]},{"cell_type":"markdown","metadata":{"id":"SQ8-Y7EGIQeO"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('torch')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.12"},"vscode":{"interpreter":{"hash":"5c32e4f2e3a56fb35e93c35983d6902bd49db4719b2e0a10b71e4a5eb06b0593"}}},"nbformat":4,"nbformat_minor":0}
