{"cells":[{"cell_type":"markdown","metadata":{"id":"48JBRWUfTS58"},"source":["# Warm-ups: Implement label flipping attcks"]},{"cell_type":"markdown","metadata":{"id":"wrf2BSSe6INY"},"source":["## Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lgj7hu7yTLcX"},"outputs":[],"source":["import copy\n","import numpy as np\n","import os"]},{"cell_type":"markdown","metadata":{"id":"9CTpbhu88cb0"},"source":["### If you are using Google Colab, you need to upload this notebook and the codebase to your Google Drive. Then you need to mount your Google Drive in Colab and set your working directory. If you are running on your local machine, you can ignore the following line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WREld4LanEkm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695109152737,"user_tz":-480,"elapsed":274999,"user":{"displayName":"Niharika Shrivastava","userId":"07199383378542377502"}},"outputId":"5ab918d9-6756-4e9e-e7fa-a44594f061e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWxeNJwv6kW9"},"outputs":[],"source":["root_dir = \"/content/drive/My Drive/\"\n","project_dir = \"Assignment2\" # Change to your path\n","os.chdir(root_dir + project_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FDukDiM7q7E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695109152737,"user_tz":-480,"elapsed":13,"user":{"displayName":"Niharika Shrivastava","userId":"07199383378542377502"}},"outputId":"8034b7b7-d868-46c0-cc46-3445151f1a00"},"outputs":[{"output_type":"stream","name":"stdout","text":["attack.py\t\t\t    dataset\n","CS5562_Assignment_2_Task1.ipynb     defense.py\n","CS5562_Assignment_2_Task2.ipynb     environment.yml\n","CS5562_Assignment_2_Task3.ipynb     model.py\n","CS5562_Assignment_2_Task4.ipynb     __pycache__\n","CS5562_Assignment_2_Warm_ups.ipynb  utilities.py\n"]}],"source":["# Make sure the path is correct\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"pkPrPU2l55f-"},"source":["## Implement random label flipping attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6hgEjg_nyLX"},"outputs":[],"source":["from utilities import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcQuG9un6B6S"},"outputs":[],"source":["from attack import Attack\n","\n","class Random_Label_Flip_Attack(Attack):\n","    \"\"\"\n","    Random label flipping attack\n","    \"\"\"\n","    def attack(self, eps):\n","        n_poison = int(eps * len(self.clean_dataset))\n","\n","        ####################\n","        # TODO: modify the following part to build your attack model based on label flipping attack\n","        index = np.random.choice(self.clean_dataset.X.shape[0], n_poison, replace=False)\n","        X, Y_modified = self.clean_dataset[index]\n","        Y_modified = Y_modified*(-1)\n","        ####################\n","        return dataset(X, Y_modified)"]},{"cell_type":"markdown","metadata":{"id":"HvWvx6-pn40i"},"source":["## Implement your label flipping attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJm4qUKI6Cfi"},"outputs":[],"source":["class Label_Flip_Attack(Attack):\n","    \"\"\"\n","        Label flipping attack: students implement their own label flipping attack here\n","    \"\"\"\n","    def attack(self, eps):\n","        n_poison = int(eps * len(self.clean_dataset))\n","\n","        ####################\n","        # TODO: modify the following part to build your attack model based on label flipping attack\n","        self.target_model.train(self.clean_dataset)\n","        svm = self.target_model.model\n","\n","        w_norm = np.linalg.norm(svm.coef_)\n","        distances = abs(svm.decision_function(self.clean_dataset.X) / w_norm)\n","\n","        # Get k largest distances\n","        index = np.argpartition(np.array(distances), -n_poison)[-n_poison:]\n","        X, Y_modified = self.clean_dataset[index]\n","        Y_modified = Y_modified*(-1)\n","\n","        ####################\n","        return dataset(X, Y_modified)"]},{"cell_type":"markdown","metadata":{"id":"bjLMJoXh-1Qt"},"source":["# Test your code"]},{"cell_type":"markdown","metadata":{"id":"dOZdeGvh61-q"},"source":["## Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EPdpd3CHG2C"},"outputs":[],"source":["def compute_attack_grade(attack, victim_model,eps,clean_train_dataset,test_dataset):\n","    # target model structure is known to the adversary\n","    target_model = copy.deepcopy(victim_model)\n","    if attack == 'KKT':\n","        attacker = KKT_Attack(target_model,clean_train_dataset,test_dataset)\n","    elif attack == 'label-flip':\n","        attacker = Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n","    elif attack == 'adaptive':\n","        attacker = Adaptive_Attack(target_model, clean_train_dataset, test_dataset)\n","    elif attack == 'random-label-flip':\n","        attacker = Random_Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n","    poisoned_dataset = attacker.attack(eps)\n","    assert len(poisoned_dataset) <= int(eps*len(clean_train_dataset))\n","\n","    train_dataset = combine_datset(clean_train_dataset,poisoned_dataset)\n","    clean_model = copy.deepcopy(target_model)\n","\n","    # performance without any attack\n","    clean_model.train(clean_train_dataset)\n","    clean_loss,clean_acc = clean_model.score(test_dataset)\n","    print('\\nAvg loss of clean model: %0.5f, avg classification accuracy: %0.5f'%(clean_loss,clean_acc))\n","\n","    # attack the victim model\n","    victim_model.train(train_dataset)\n","    poisoned_loss,poisoned_acc =victim_model.score(test_dataset)\n","    print('\\nAvg loss of poisoned model:%0.5f, avg classification accuracy: %0.5f'%(poisoned_loss,poisoned_acc))\n","\n","    grade = poisoned_loss - clean_loss\n","\n","    # # for generating figures\n","    # distance_to_center_diff(clean_train_dataset,poisoned_dataset)\n","    # loss_diff(clean_train_dataset, poisoned_dataset,clean_model)\n","\n","    return len(poisoned_dataset)/len(clean_train_dataset),grade"]},{"cell_type":"markdown","metadata":{"id":"grFB2DU28kbY"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"0sb62Syv-QXm"},"source":["### Random label flipping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7pntdu1_dvx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695109207933,"user_tz":-480,"elapsed":47550,"user":{"displayName":"Niharika Shrivastava","userId":"07199383378542377502"}},"outputId":"428c3ef0-40c4-4d65-8797-e41be2b4115f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Avg loss of clean model: 0.01602, avg classification accuracy: 0.99491\n","\n","Avg loss of poisoned model:0.20555, avg classification accuracy: 0.99260\n","\n","\n","-----------result---------\n","random-label-flip attack against undefended nn model on mnist_17 dataset: 0.19 (0.20 fraction of poisoning data)\n"]}],"source":["from model import Undefended_Model\n","\n","train_dataset,test_dataset = load_dataset('mnist_17')\n","base_model = load_model(\"nn\", \"mnist_17\")\n","target_model = Undefended_Model(base_model,\"nn\")\n","defense_name = 'undefended'\n","fraction, attack_grade = compute_attack_grade(\"random-label-flip\", target_model, 0.2, train_dataset, test_dataset)\n","print('\\n\\n-----------result---------')\n","print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"random-label-flip\",defense_name,\"nn\",\"mnist_17\",attack_grade,fraction))"]},{"cell_type":"markdown","metadata":{"id":"r9wkcikfHrHR"},"source":["### Label flipping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qB0n2Eb2Hk6I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695109213167,"user_tz":-480,"elapsed":5241,"user":{"displayName":"Niharika Shrivastava","userId":"07199383378542377502"}},"outputId":"422fe1f2-22d5-43c2-b8a7-9fd5b885a494"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Avg loss of clean model: 0.01694, avg classification accuracy: 0.99260\n","\n","Avg loss of poisoned model:0.08222, avg classification accuracy: 0.98382\n","\n","\n","-----------result---------\n","label-flip attack against undefended svm model on mnist_17 dataset: 0.07 (0.20 fraction of poisoning data)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["from model import Undefended_Model\n","\n","train_dataset,test_dataset = load_dataset('mnist_17')\n","base_model = load_model(\"svm\", \"mnist_17\")\n","target_model = Undefended_Model(base_model,\"svm\")\n","defense_name = 'undefended'\n","fraction, attack_grade = compute_attack_grade(\"label-flip\", target_model, 0.2, train_dataset, test_dataset)\n","print('\\n\\n-----------result---------')\n","print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"label-flip\",defense_name,\"svm\",\"mnist_17\",attack_grade,fraction))"]},{"cell_type":"markdown","metadata":{"id":"hXVnAKn2IOPB"},"source":["# Report"]},{"cell_type":"markdown","metadata":{"id":"SQ8-Y7EGIQeO"},"source":[]}],"metadata":{"colab":{"collapsed_sections":["9CTpbhu88cb0","pkPrPU2l55f-","dOZdeGvh61-q","0sb62Syv-QXm"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.12"},"vscode":{"interpreter":{"hash":"5c32e4f2e3a56fb35e93c35983d6902bd49db4719b2e0a10b71e4a5eb06b0593"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}