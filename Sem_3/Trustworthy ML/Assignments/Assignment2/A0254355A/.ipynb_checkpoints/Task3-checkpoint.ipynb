{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48JBRWUfTS58"
   },
   "source": [
    "# Task 3: Adaptive attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrf2BSSe6INY"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6005,
     "status": "ok",
     "timestamp": 1695381421193,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "Lgj7hu7yTLcX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CTpbhu88cb0"
   },
   "source": [
    "### If you are using Google Colab, you need to upload this notebook and the codebase to your Google Drive. Then you need to mount your Google Drive in Colab and set your working directory. If you are running on your local machine, you can ignore the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68598,
     "status": "ok",
     "timestamp": 1695381489787,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "WREld4LanEkm",
    "outputId": "557e3af1-b00c-4622-9364-cbca6dc6b10d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1695381489787,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "rWxeNJwv6kW9"
   },
   "outputs": [],
   "source": [
    "root_dir = \"/content/drive/My Drive/\"\n",
    "project_dir = \"Assignment2\" # Change to your path\n",
    "os.chdir(root_dir + project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695381489788,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "2FDukDiM7q7E",
    "outputId": "457f72c3-4a44-4f80-e761-5121fadf10f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " attack.py\n",
      " CS5562_Assignment_2_Task1.ipynb\n",
      " CS5562_Assignment_2_Task2.ipynb\n",
      " CS5562_Assignment_2_Task3.ipynb\n",
      " CS5562_Assignment_2_Task4.ipynb\n",
      " CS5562_Assignment_2_Warm_ups.ipynb\n",
      " dataset\n",
      " defense.py\n",
      "'[DISCARDED]CS5562_Assignment_2_Task1.ipynb'\n",
      "'[DISCARDED]CS5562_Assignment_2_Task2.ipynb'\n",
      "'[DISCARDED]CS5562_Assignment_2_Task3.ipynb'\n",
      " environment.yml\n",
      " model.py\n",
      " __pycache__\n",
      " utilities.py\n"
     ]
    }
   ],
   "source": [
    "# Make sure the path is correct\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkPrPU2l55f-"
   },
   "source": [
    "## Implement adaptive attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3287,
     "status": "ok",
     "timestamp": 1695381493070,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "f6hgEjg_nyLX"
   },
   "outputs": [],
   "source": [
    "from utilities import *\n",
    "from attack import Attack\n",
    "from tqdm import tqdm\n",
    "from model import Undefended_Model\n",
    "\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1695381493070,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "JcQuG9un6B6S"
   },
   "outputs": [],
   "source": [
    "class Adaptive_Attack(Attack):\n",
    "    \"\"\"\n",
    "          adaptive attack: students implement their own adaptive attack here\n",
    "    \"\"\"\n",
    "    def attack(self, eps):\n",
    "        n_poison = int(eps * len(self.clean_dataset))\n",
    "\n",
    "        ####################\n",
    "        # TODO: update the following part to build your attack model based on KKT attack\n",
    "\n",
    "        # Find decoy parameters theta_decoy\n",
    "        svm_clean = copy.deepcopy(self.target_model)\n",
    "        svm_clean.train(self.clean_dataset)\n",
    "\n",
    "        test_data_flip = dataset(self.test_dataset.X, self.test_dataset.Y*(-1))\n",
    "        test_loss = svm_clean.individual_loss(test_data_flip)\n",
    "        gamma = np.percentile(test_loss, 50, axis=0)\n",
    "        index = np.where(test_loss > gamma)[0]\n",
    "        x, y = test_data_flip[index]\n",
    "\n",
    "        # Changed value of repeat (r) from [10 - 20] and chose 20 empirically\n",
    "        repeats = [20][0]\n",
    "        x_flip = np.tile(x, (repeats, 1))\n",
    "        y_flip = np.tile(y, repeats)\n",
    "\n",
    "        D_flip = dataset(x_flip, y_flip)\n",
    "        D_decoy = combine_datset(self.clean_dataset, D_flip)\n",
    "\n",
    "        base_model = load_model(\"svm\", \"mnist_17\")\n",
    "        svm_decoy = Undefended_Model(base_model,\"svm\")\n",
    "        svm_decoy.train(D_decoy)\n",
    "        print(\"Decoy theta found!\")\n",
    "\n",
    "        # Grid search\n",
    "        def hinge_grad(data, model):\n",
    "          w = model.coef_[0]\n",
    "          b = model.intercept_\n",
    "          X, Y = data.X, data.Y\n",
    "          grad = 0\n",
    "\n",
    "          for (x_, y_) in zip(X, Y):\n",
    "              v = y_ * (np.dot(w, x_) + b)\n",
    "              grad += 0 if v > 1 else -y_ * x_\n",
    "          return grad / X[0].shape\n",
    "\n",
    "        def optimization(n_features, eps_pos, eps_neg, g_decoy, model):\n",
    "          w = model.coef_\n",
    "          b = model.intercept_\n",
    "          x_pos = cp.Variable(n_features)\n",
    "          x_neg = cp.Variable(n_features)\n",
    "\n",
    "          error = g_decoy - cp.multiply(eps_pos, x_pos) + cp.multiply(eps_neg, x_neg)\n",
    "          obj = cp.Minimize(cp.sum_squares(error))\n",
    "          constraints = [\n",
    "              1 - (w@x_pos + b) >= 0,\n",
    "              1 + (w@x_neg + b) >= 0\n",
    "          ]\n",
    "\n",
    "          prob = cp.Problem(obj, constraints)\n",
    "          prob.solve()\n",
    "          x_pos = np.array(x_pos.value)\n",
    "          x_neg = np.array(x_neg.value)\n",
    "\n",
    "          return x_pos, x_neg\n",
    "\n",
    "        print(\"Finding attack points\")\n",
    "        T = 5\n",
    "        optimal_params = None\n",
    "        g_decoy = hinge_grad(self.clean_dataset, svm_decoy.model)\n",
    "\n",
    "        for t in tqdm(range(1, T-1)):\n",
    "          eps_pos = t*eps / T\n",
    "          eps_neg = eps - eps_pos\n",
    "\n",
    "          # Get x_pos, x_neg\n",
    "          features = self.clean_dataset.X[0].shape\n",
    "          x_pos, x_neg = optimization(features, eps_pos, eps_neg, g_decoy, svm_decoy.model)\n",
    "\n",
    "          # Create D_poison\n",
    "          T = 800\n",
    "          n_pos = T//2 - 1\n",
    "          n_neg = T//2 - 1\n",
    "\n",
    "          x = np.concatenate((np.tile(x_pos, (n_pos, 1)), np.tile(x_neg, (n_neg, 1))))\n",
    "          y = np.concatenate((np.ones(n_pos), (-1)*np.ones(n_neg)))\n",
    "          D_poison = dataset(x, y)\n",
    "          assert len(D_poison) <= n_poison\n",
    "\n",
    "          # Train svm\n",
    "          D_combine = combine_datset(self.clean_dataset, D_poison)\n",
    "          svm_new = copy.deepcopy(self.target_model)\n",
    "          svm_new.train(D_combine)\n",
    "\n",
    "          # Get svm, D_poison with highest test loss\n",
    "          test_loss = svm_new.score(self.test_dataset)[0]\n",
    "\n",
    "          if optimal_params is None or optimal_params['loss'] < test_loss:\n",
    "            optimal_params = {\n",
    "                'x_poison': x,\n",
    "                'y_poison': y,\n",
    "                'loss': test_loss,\n",
    "                'eps_pos': eps_pos,\n",
    "                'eps_neg': eps_neg\n",
    "                }\n",
    "          print(optimal_params['loss'], optimal_params['eps_pos'], optimal_params['eps_neg'])\n",
    "\n",
    "        print(\"Length of D_poison:\" , len(D_poison))\n",
    "        print(\"Length allowed to poison: \", n_poison)\n",
    "\n",
    "        X_modified = optimal_params['x_poison']\n",
    "        Y_modified = optimal_params['y_poison']\n",
    "\n",
    "        ####################\n",
    "        return dataset(X_modified, Y_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjLMJoXh-1Qt"
   },
   "source": [
    "# Test your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SPdZEk9N90Z"
   },
   "source": [
    "## Copy and Paste your data sanitizer from Task 2 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695381493071,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "SVCeppF8OB7a"
   },
   "outputs": [],
   "source": [
    "def data_sanitizer(training_data, estimate_eps):\n",
    "    \"\"\"\n",
    "       Removes the estimate_eps fraction of points from X and Y.\n",
    "    \"\"\"\n",
    "\n",
    "    n_est_poisoned = int(estimate_eps * len(training_data))\n",
    "\n",
    "    #################\n",
    "    # TODO: decide which points need to be deleted\n",
    "\n",
    "    def calc_dist_to_centroid(training_data):\n",
    "      class_map = {-1: 0, 1: 1}\n",
    "      X = training_data.X\n",
    "      Y = training_data.Y\n",
    "      num_classes = len(set(Y))\n",
    "      num_features = X.shape[1]\n",
    "\n",
    "      centroids = np.zeros((num_classes, num_features))\n",
    "      dis_to_centro = np.zeros(len(training_data))\n",
    "\n",
    "      for y in set(Y):\n",
    "          centroids[class_map[y], :] = np.median(X[Y == y, :], axis=0)\n",
    "\n",
    "      for i in range(len(training_data)):\n",
    "          dis_to_centro[i] = np.linalg.norm(X[i]-centroids[class_map[Y[i]]])\n",
    "\n",
    "      return dis_to_centro\n",
    "\n",
    "    # Distances of whole training data\n",
    "    distances = calc_dist_to_centroid(training_data)\n",
    "\n",
    "    # Distances after deletion\n",
    "    index, ind1, ind2 = [], [], []\n",
    "    threshold = 400\n",
    "    values, counts = np.unique(distances, return_counts=True)\n",
    "    count1_idx, count2_idx = np.argpartition(counts, -2)[-2:]\n",
    "\n",
    "    if counts[count1_idx] > threshold:\n",
    "      dist = values[count1_idx]\n",
    "      ind1 = np.where(dist == distances)[0]\n",
    "\n",
    "    if counts[count2_idx] > threshold:\n",
    "      dist = values[count2_idx]\n",
    "      ind2 = np.where(dist == distances)[0]\n",
    "\n",
    "    if len(ind1) != 0 or len(ind2) != 0:\n",
    "      index = np.concatenate((ind1, ind2))\n",
    "\n",
    "    print(\"No. of points sanitized: \", len(index))\n",
    "\n",
    "    ################\n",
    "    training_data_copy = copy.deepcopy(training_data)\n",
    "    del training_data_copy[index]\n",
    "    return training_data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOZdeGvh61-q"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695381493071,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "RjC-8NijLp57"
   },
   "outputs": [],
   "source": [
    "from model import Model\n",
    "\n",
    "\n",
    "class Data_Sanitized_Model(Model):\n",
    "    def __init__(self, model, model_name, estimated_eps):\n",
    "        super().__init__(model, model_name)\n",
    "        self.estimated_eps = estimated_eps\n",
    "\n",
    "    def train(self, train_dataset):\n",
    "        sanitized_data = data_sanitizer(training_data=train_dataset, estimate_eps=self.estimated_eps)\n",
    "        self.model.fit(sanitized_data.X, sanitized_data.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1695381493626,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "_EPdpd3CHG2C"
   },
   "outputs": [],
   "source": [
    "def compute_attack_grade(attack, victim_model,eps,clean_train_dataset,test_dataset):\n",
    "    # target model structure is known to the adversary\n",
    "    target_model = copy.deepcopy(victim_model)\n",
    "    if attack == 'KKT':\n",
    "        attacker = KKT_Attack(target_model,clean_train_dataset,test_dataset)\n",
    "    elif attack == 'label-flip':\n",
    "        attacker = Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n",
    "    elif attack == 'adaptive':\n",
    "        attacker = Adaptive_Attack(target_model, clean_train_dataset, test_dataset)\n",
    "    elif attack == 'random-label-flip':\n",
    "        attacker = Random_Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n",
    "    poisoned_dataset = attacker.attack(eps)\n",
    "    assert len(poisoned_dataset) <= int(eps*len(clean_train_dataset))\n",
    "\n",
    "    train_dataset = combine_datset(clean_train_dataset,poisoned_dataset)\n",
    "    clean_model = copy.deepcopy(target_model)\n",
    "\n",
    "    # performance without any attack\n",
    "    clean_model.train(clean_train_dataset)\n",
    "    clean_loss,clean_acc = clean_model.score(test_dataset)\n",
    "    print('\\nAvg loss of clean model: %0.5f, avg classification accuracy: %0.5f'%(clean_loss,clean_acc))\n",
    "\n",
    "    # attack the victim model\n",
    "    victim_model.train(train_dataset)\n",
    "    poisoned_loss,poisoned_acc =victim_model.score(test_dataset)\n",
    "    print('\\nAvg loss of poisoned model:%0.5f, avg classification accuracy: %0.5f'%(poisoned_loss,poisoned_acc))\n",
    "\n",
    "    grade = poisoned_loss - clean_loss\n",
    "\n",
    "    # # for generating figures\n",
    "    # distance_to_center_diff(clean_train_dataset,poisoned_dataset)\n",
    "    # loss_diff(clean_train_dataset, poisoned_dataset,clean_model)\n",
    "\n",
    "    return len(poisoned_dataset)/len(clean_train_dataset),grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grFB2DU28kbY"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232904,
     "status": "ok",
     "timestamp": 1695381726524,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "B7pntdu1_dvx",
    "outputId": "e18acbdb-5bb1-44cd-e953-9d08b7f78092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of points sanitized:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoy theta found!\n",
      "Finding attack points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of points sanitized:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\r",
      " 33%|███▎      | 1/3 [00:29<00:58, 29.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13875530930139723 0.04 0.16\n",
      "No. of points sanitized:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\r",
      " 67%|██████▋   | 2/3 [01:09<00:35, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.142245707594638 0.0005 0.1995\n",
      "No. of points sanitized:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [01:48<00:00, 36.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14234661206340612 0.0007500000000000001 0.19925\n",
      "Length of D_poison: 798\n",
      "Length allowed to poison:  2601\n",
      "No. of points sanitized:  0\n",
      "\n",
      "Avg loss of clean model: 0.01694, avg classification accuracy: 0.99260\n",
      "No. of points sanitized:  0\n",
      "\n",
      "Avg loss of poisoned model:0.14235, avg classification accuracy: 0.95747\n",
      "\n",
      "\n",
      "-----------result---------\n",
      "adaptive attack against data_sanitization svm model on mnist_17 dataset: 0.13 (0.06 fraction of poisoning data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset,test_dataset = load_dataset('mnist_17')\n",
    "base_model = load_model(\"svm\", \"mnist_17\")\n",
    "target_model = Data_Sanitized_Model(base_model,\"svm\", 0.2)\n",
    "defense_name = 'data_sanitization'\n",
    "fraction, attack_grade = compute_attack_grade(\"adaptive\", target_model, 0.2, train_dataset, test_dataset)\n",
    "print('\\n\\n-----------result---------')\n",
    "print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"adaptive\",defense_name,\"svm\",\"mnist_17\",attack_grade,fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXVnAKn2IOPB"
   },
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ8-Y7EGIQeO"
   },
   "source": [
    "**Q.1) Please describe your adaptive attack algorithm in the report, specifying how your attack bypasses the data sanitization. You can also inform us about any difficulties you faced and how you solved them.**\n",
    "\n",
    "My adaptive is the same KKT approach as Task 1 with 2 modifications:\n",
    "1. My $\\theta_{decoy}$ is calculated using an undefended SVM. This is so that my combined dataset (D_clean + D_poison) which contains a lot of poison points in order to learn the optimal $\\theta_{decoy}$ does not get sanitized by my data sanitizer. As a result, my optimal $\\theta_{decoy}$ is learnt.\n",
    "\n",
    "2. I'm limiting the number of poisoned points to be less than 400 for each class. The reason for this is because the defense in Task 2 assumes that the total number of poisoned points will be greater than 800 (2 points repeated > 400 times each). \n",
    "\n",
    "\n",
    "The result of this modification is that:\n",
    "1. Length of D_posion < Allowed length of D_poison --> Bypassed data sanitization completely\n",
    "2. 4% decrease in performance (exactly same as Task 1).\n",
    "\n",
    "```\n",
    "Avg loss of clean model: 0.01694, avg classification accuracy: 0.99260\n",
    "Avg loss of poisoned model:0.14235, avg classification accuracy: 0.95747\n",
    "```\n",
    "\n",
    "Note: I have noted through experimentatioon that since I have learnt an optimal $\\theta_{decoy}$, I can add only 2 poisoned points for each class and still my performance would decrease drastically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wrf2BSSe6INY",
    "8SPdZEk9N90Z",
    "dOZdeGvh61-q"
   ],
   "provenance": [
    {
     "file_id": "1hE7ZE5k9xoI-GOgp0SbI3BnwMh87TAHx",
     "timestamp": 1695379770844
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
