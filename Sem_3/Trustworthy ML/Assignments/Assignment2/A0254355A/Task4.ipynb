{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48JBRWUfTS58"
   },
   "source": [
    "# Task 4: Robust Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrf2BSSe6INY"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2003,
     "status": "ok",
     "timestamp": 1695336433254,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "Lgj7hu7yTLcX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CTpbhu88cb0"
   },
   "source": [
    "### If you are using Google Colab, you need to upload this notebook and the codebase to your Google Drive. Then you need to mount your Google Drive in Colab and set your working directory. If you are running on your local machine, you can ignore the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2668,
     "status": "ok",
     "timestamp": 1695336435920,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "WREld4LanEkm",
    "outputId": "cb1d94b8-1092-42fd-80e5-32b73175c098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1695336435921,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "rWxeNJwv6kW9"
   },
   "outputs": [],
   "source": [
    "root_dir = \"/content/drive/My Drive/\"\n",
    "project_dir = \"Assignment2\" # Change to your path\n",
    "os.chdir(root_dir + project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1695336436865,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "2FDukDiM7q7E",
    "outputId": "4d8e290c-a589-49ee-b4cb-bfdb10b6b944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack.py\t\t\t    dataset\n",
      "CS5562_Assignment_2_Task1.ipynb     defense.py\n",
      "CS5562_Assignment_2_Task2.ipynb     environment.yml\n",
      "CS5562_Assignment_2_Task3.ipynb     model.py\n",
      "CS5562_Assignment_2_Task4.ipynb     __pycache__\n",
      "CS5562_Assignment_2_Warm_ups.ipynb  utilities.py\n"
     ]
    }
   ],
   "source": [
    "# Make sure the path is correct\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkPrPU2l55f-"
   },
   "source": [
    "## Implement robust trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1695336439322,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "f6hgEjg_nyLX"
   },
   "outputs": [],
   "source": [
    "from utilities import *\n",
    "from defense import *\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1695337797485,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "eAmfs1BzTpQ9"
   },
   "outputs": [],
   "source": [
    "def robust_trainer(model, train_dataset, eps):\n",
    "    #################\n",
    "    # TODO: implement the robust gradient update (this is simple gradient update) based on sever\n",
    "    size = len(train_dataset)\n",
    "    n_poison = int(eps * size)\n",
    "    total_loss, total_err = 0., 0.\n",
    "\n",
    "    opt = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "    # Fit the model\n",
    "    X = torch.tensor(train_dataset.X, requires_grad=True)\n",
    "    y = torch.tensor(train_dataset.Y)\n",
    "    y = ((y + 1) / 2).reshape(-1, 1) # convert the label from {-1,1} to {0,1} for using cross entropy loss\n",
    "\n",
    "    pred = model(X.float())\n",
    "    loss = nn.BCEWithLogitsLoss()(pred, y.float())\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    total_err += ((pred > 0) * (y == 0) + (pred < 0) * (y == 1)).sum().item()\n",
    "    total_loss += loss.item() * size\n",
    "\n",
    "    # Calculate sever outliers\n",
    "    individual_grad = X.grad\n",
    "    delta_hat = np.tile(torch.mean(individual_grad, axis=0), (size, 1))\n",
    "\n",
    "    G = individual_grad - delta_hat\n",
    "    _, _, V = svd(G)\n",
    "    # Top right singular vector\n",
    "    v = V[0].reshape(-1, 1)\n",
    "\n",
    "    score = (np.dot(G, v)**2).squeeze()\n",
    "\n",
    "    # Remove points with p largest scores\n",
    "    T = int(size*eps/50)\n",
    "    index = np.argpartition(score, -T)[-T:]\n",
    "\n",
    "    del train_dataset[index]\n",
    "\n",
    "    #################\n",
    "    return total_err / train_dataset.Y.shape[0], total_loss / train_dataset.Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjLMJoXh-1Qt"
   },
   "source": [
    "# Test your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOZdeGvh61-q"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695337836477,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "JcQuG9un6B6S"
   },
   "outputs": [],
   "source": [
    "from model import Model\n",
    "\n",
    "class RobustModel(Model):\n",
    "    def __init__(self, model, model_name, estimated_eps):\n",
    "        super().__init__(model, model_name)\n",
    "        self.estimated_eps = estimated_eps\n",
    "\n",
    "    def train(self, train_dataset):\n",
    "        for i in range(50):\n",
    "            robust_trainer(self.model, train_dataset, self.estimated_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1695337811002,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "_EPdpd3CHG2C"
   },
   "outputs": [],
   "source": [
    "def compute_attack_grade(attack, victim_model,eps,clean_train_dataset,test_dataset):\n",
    "    # target model structure is known to the adversary\n",
    "    target_model = copy.deepcopy(victim_model)\n",
    "    if attack == 'KKT':\n",
    "        attacker = KKT_Attack(target_model,clean_train_dataset,test_dataset)\n",
    "    elif attack == 'label-flip':\n",
    "        attacker = Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n",
    "    elif attack == 'adaptive':\n",
    "        attacker = Adaptive_Attack(target_model, clean_train_dataset, test_dataset)\n",
    "    elif attack == 'random-label-flip':\n",
    "        attacker = Random_Label_Flip_Attack(target_model, clean_train_dataset, test_dataset)\n",
    "    poisoned_dataset = attacker.attack(eps)\n",
    "    assert len(poisoned_dataset) <= int(eps*len(clean_train_dataset))\n",
    "\n",
    "    train_dataset = combine_datset(clean_train_dataset,poisoned_dataset)\n",
    "    clean_model = copy.deepcopy(target_model)\n",
    "\n",
    "    # performance without any attack\n",
    "    clean_model.train(clean_train_dataset)\n",
    "    clean_loss,clean_acc = clean_model.score(test_dataset)\n",
    "    print('\\nAvg loss of clean model: %0.5f, avg classification accuracy: %0.5f'%(clean_loss,clean_acc))\n",
    "\n",
    "    # attack the victim model\n",
    "    victim_model.train(train_dataset)\n",
    "    poisoned_loss,poisoned_acc =victim_model.score(test_dataset)\n",
    "    print('\\nAvg loss of poisoned model:%0.5f, avg classification accuracy: %0.5f'%(poisoned_loss,poisoned_acc))\n",
    "\n",
    "    grade = poisoned_loss - clean_loss\n",
    "\n",
    "    # # for generating figures\n",
    "    # distance_to_center_diff(clean_train_dataset,poisoned_dataset)\n",
    "    # loss_diff(clean_train_dataset, poisoned_dataset,clean_model)\n",
    "\n",
    "    return len(poisoned_dataset)/len(clean_train_dataset),grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yshzy7i_Mk_d"
   },
   "source": [
    "## Copy and Paste your data flipping attack here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1695337811003,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "QLRNiKyuMo-t"
   },
   "outputs": [],
   "source": [
    "from attack import Attack\n",
    "\n",
    "class Label_Flip_Attack(Attack):\n",
    "    \"\"\"\n",
    "        Label flipping attack: students implement their own label flipping attack here\n",
    "    \"\"\"\n",
    "    def attack(self, eps):\n",
    "        n_poison = int(eps * len(self.clean_dataset))\n",
    "\n",
    "        ####################\n",
    "        # TODO: modify the following part to build your attack model based on label flipping attack\n",
    "        index = np.random.choice(self.clean_dataset.X.shape[0], n_poison, replace=False)\n",
    "        X, Y_modified = self.clean_dataset[index]\n",
    "        Y_modified = Y_modified*(-1)\n",
    "\n",
    "        ####################\n",
    "        return dataset(X, Y_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grFB2DU28kbY"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw1q6STS83eM"
   },
   "source": [
    "#### Robust Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318291,
     "status": "ok",
     "timestamp": 1695336757611,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "B7pntdu1_dvx",
    "outputId": "8d6c0c4b-ed2c-49ff-b2f1-d99651efc147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg loss of clean model: 0.05876, avg classification accuracy: 0.98333\n",
      "\n",
      "Avg loss of poisoned model:0.07806, avg classification accuracy: 0.98167\n",
      "\n",
      "\n",
      "-----------result---------\n",
      "label-flip attack against robust nn model on dogfish dataset: 0.02 (0.24 fraction of poisoning data)\n"
     ]
    }
   ],
   "source": [
    "train_dataset,test_dataset = load_dataset('dogfish')\n",
    "base_model = load_model(\"nn\", \"dogfish\")\n",
    "target_model = RobustModel(base_model, \"nn\", 0.2)\n",
    "defense_name = 'robust'\n",
    "fraction, attack_grade = compute_attack_grade(\"label-flip\", target_model, 0.2, train_dataset, test_dataset)\n",
    "print('\\n\\n-----------result---------')\n",
    "print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"label-flip\",defense_name,\"nn\",\"dogfish\",attack_grade,fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGKJeXn58-8v"
   },
   "source": [
    "#### Robust LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293836,
     "status": "ok",
     "timestamp": 1695338132249,
     "user": {
      "displayName": "Niharika Shrivastava",
      "userId": "07199383378542377502"
     },
     "user_tz": -480
    },
    "id": "DL_0QJBO8-C2",
    "outputId": "d766fe5f-5b56-4345-96fb-cf282b355c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg loss of clean model: 0.05472, avg classification accuracy: 0.98333\n",
      "\n",
      "Avg loss of poisoned model:0.06721, avg classification accuracy: 0.98333\n",
      "\n",
      "\n",
      "-----------result---------\n",
      "label-flip attack against robust lr model on dogfish dataset: 0.01 (0.24 fraction of poisoning data)\n"
     ]
    }
   ],
   "source": [
    "train_dataset,test_dataset = load_dataset('dogfish')\n",
    "base_model = load_model(\"lr\", \"dogfish\")\n",
    "target_model = RobustModel(base_model, \"lr\", 0.2)\n",
    "defense_name = 'robust'\n",
    "fraction, attack_grade = compute_attack_grade(\"label-flip\", target_model, 0.2, train_dataset, test_dataset)\n",
    "print('\\n\\n-----------result---------')\n",
    "print('%s attack against %s %s model on %s dataset: %0.2f (%0.2f fraction of poisoning data)'%(\"label-flip\",defense_name,\"lr\",\"dogfish\",attack_grade,fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXVnAKn2IOPB"
   },
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ8-Y7EGIQeO"
   },
   "source": [
    "**Q.1) Describe your defense algorithm in detail. Furthermore, please provide the whole proof of your robustness analysis in the report.**\n",
    "\n",
    "I implemented the SEVER algorithm as given in the paper. I used practical considerations mentioned in the paper for a simpler implementation.\n",
    "\n",
    "1. Fit the model using normal SGD as optimizer.\n",
    "2. Calculate the gradients of each point in the dataset wrt. to the learned parameters $∇f_i(w)$.\n",
    "3. Calculate the average gradients given by where $S$ is the length of the dataset:\n",
    "\n",
    "$$∇_{avg} = \\frac{1}{|S|} ∑_{i \\space ϵ \\space S} ∇f_i(w)$$\n",
    "\n",
    "4. Calculate the centered gradients $G$:\n",
    "\n",
    "$$G = [  ∇f_i(w) - ∇_{avg} ]_{i \\space ϵ \\space S}$$\n",
    "\n",
    "5. Extract the top right singular vector $v$ of $G$.\n",
    "\n",
    "6. Compute the outlier score $τ_i$ for every data point $i$.\n",
    "\n",
    "$$τ_i = (( ∇f_i(w) - ∇_{avg} ) \\cdot v)^2$$\n",
    "\n",
    "7. Remove top $T = int(size*eps/50)$ points from the training dataset.\n",
    "\n",
    "\n",
    "### Proof\n",
    "\n",
    "The robustness analysis follows that of the corresponding paper. [1]\n",
    "\n",
    "1. As long as the real (non-outlying) data is not too heavy-tailed, SEVER is provably robust to outliers. The value of objective $f$ cannot be decreased much by changing the input $w$ locally, while staying within the domain. The condition enforces that moving in any direction $v$ either causes us to leave domain $H$ or causes $f$ to decrease at a rate at most $γ$.\n",
    "\n",
    "2. $L$ always finds an approximate critical point of the empirical learning objective. Stochastic gradient descent satisfy the γ-approximate learner property.\n",
    "\n",
    "3. We want to ensure that the outliers do not have a large effect on the learned parameters. Intuitively, for the outliers to have such an effect, their corresponding gradients should be (i) large in magnitude and (ii) systematically pointing in a specific direction. We can detect this via singular value decomposition–if both (i) and (ii) hold then the outliers should be responsible for a large singular value in the matrix of gradients, which allows us to detect and remove them. \n",
    "\n",
    "4. Approximating the gradient of f at a given point, given access to an ε-corrupted set of samples, can be viewed as a robust mean estimation problem. We can thus use the robust mean estimation algorithm of (Diakonikolas et al., 2017a), which succeeds under fairly mild assumptions about the good samples. Assuming that the covariance matrix of ∇f(w), f ∼ p∗, is bounded, we can thus “simulate” gradient descent and approximately minimize f.\n",
    "\n",
    "5. By the performance guarantees of the latter algorithm we are in one of two cases: either the filtering algorithm removes a set of corrupted functions or it certifies that the gradient of fˆ is “close” to the gradient of f at w. In the first case, we make progress as we produce a “cleaner” set of functions. In the second case, our certification implies that the point w is also an approximate critical point of f and we are done\n",
    "\n",
    "6. Moreover, the error guarantee has no dependence on the underlying dimension d. \n",
    "\n",
    "Therefore, SEVER is a robust algorithm to outliers.\n",
    "\n",
    "\n",
    "\n",
    "### Question 1. Please train the robust logistic regression model, robust neural network models using your training algorithm and compare their accuracy in the benign and adversarial settings. Which model achieves a better test accuracy in each case? Why?\n",
    "\n",
    "The test accuracies are as follows:\n",
    "\n",
    "NN\n",
    "```\n",
    "Avg loss of clean model: 0.05876, avg classification accuracy: 0.98333\n",
    "Avg loss of poisoned model:0.07806, avg classification accuracy: 0.98167\n",
    "```\n",
    "\n",
    "LR\n",
    "```\n",
    "Avg loss of clean model: 0.05472, avg classification accuracy: 0.98333\n",
    "Avg loss of poisoned model:0.06721, avg classification accuracy: 0.98333\n",
    "```\n",
    "\n",
    "1. For the benign setting, both achieve the same accuracy. However, I have run this multiple times and on an average, neural network achieves better accuracy than LR. This is because it is able to learn complex patterns in the image, while LR is a simpler linear model. \n",
    "\n",
    "2. For the adversarial setting, LR achieves better accuracy on an average. This is because the objective of LR is always convex vs NN which may not always have a convex objective. Therefore, it is guarenteed to remove the poisoned points in LR. (Sever is a guaranteed approach for convex problems).\n",
    "\n",
    "\n",
    "### References:\n",
    "[1] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. In International Conference on Machine Learning, pages 1596–1606. PMLR, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wrf2BSSe6INY",
    "yshzy7i_Mk_d",
    "Sw1q6STS83eM",
    "HGKJeXn58-8v"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
