{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36b565b0-dc53-4172-b646-4c82e1c472be",
     "showTitle": false,
     "title": ""
    },
    "id": "yvjBmGBAxnQc"
   },
   "source": [
    "## Task 1: Spark SQL (15m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30d54257-dc20-4174-aa40-84e1f6abc56f",
     "showTitle": false,
     "title": ""
    },
    "id": "MkbrHZYEw5Cr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5607f10e-0a58-4330-bbeb-fa1d6863efb1",
     "showTitle": false,
     "title": ""
    },
    "id": "2luSAeOXxBiQ"
   },
   "outputs": [],
   "source": [
    "sales_file_location = \"/FileStore/tables/Sales_table.csv\"\n",
    "products_file_location = \"/FileStore/tables/Products_table.csv\"\n",
    "sellers_file_location = \"/FileStore/tables/Sellers_table.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "products_table = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(products_file_location)\n",
    "\n",
    "sales_table = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(sales_file_location)\n",
    "\n",
    "sellers_table = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(sellers_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb33021-930c-4fa9-b595-4ed83c279ed4",
     "showTitle": false,
     "title": ""
    },
    "id": "Ps_v7oTixnQf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| product_name|\n",
      "+-------------+\n",
      "|product_51270|\n",
      "|product_18759|\n",
      "|product_59652|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (a) Output the top 3 most popular products sold among all sellers [2m]\n",
    "# Your table should have 1 column(s): [product_name]\n",
    "sales_table.createOrReplaceTempView(\"sales\")\n",
    "products_table.createOrReplaceTempView(\"products\")\n",
    "\n",
    "popSQL = spark.sql(\"\"\"\n",
    "select products.product_name\n",
    "from sales inner join products \n",
    "on products.product_id=sales.product_id\n",
    "group by products.product_name\n",
    "order by sum(sales.num_of_items_sold) desc, products.product_name\n",
    "\"\"\")\n",
    "\n",
    "popSQL.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "866983b3-8214-4740-8f4d-90e87d1db482",
     "showTitle": false,
     "title": ""
    },
    "id": "Ljmb_1OaxC8Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| product_name|\n",
      "+-------------+\n",
      "|product_36658|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (b) Find out the total sales of the products sold by sellers 1 to 10 and output the top most sold product [2m]\n",
    "# Your table should have 1 column(s): [product_name]\n",
    "sales_table.createOrReplaceTempView(\"sales\")\n",
    "products_table.createOrReplaceTempView(\"products\")\n",
    "sellers_table.createOrReplaceTempView(\"sellers\")\n",
    "\n",
    "popSQL = spark.sql(\"\"\"\n",
    "select products.product_name\n",
    "from sales \n",
    "inner join products on products.product_id=sales.product_id\n",
    "inner join sellers on sellers.seller_id=sales.seller_id\n",
    "where sellers.seller_id >= 1 and sellers.seller_id <= 10\n",
    "group by products.product_name\n",
    "order by sum(sales.num_of_items_sold) desc\n",
    "\"\"\")\n",
    "\n",
    "popSQL.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa7bec8e-f93d-48ff-af38-d395c6fe7422",
     "showTitle": false,
     "title": ""
    },
    "id": "QtinRRycxDBS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_revenue|\n",
      "+-------------+\n",
      "|    160916699|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (c) Compute the combined revenue earned from sellers where seller_id ranges from 1 to 500 inclusive. [3m]\n",
    "# Your table should have 1 column(s): [total_revenue]\n",
    "sales_table.createOrReplaceTempView(\"sales\")\n",
    "products_table.createOrReplaceTempView(\"products\")\n",
    "sellers_table.createOrReplaceTempView(\"sellers\")\n",
    "\n",
    "popSQL = spark.sql(\"\"\"\n",
    "select sum(sales.num_of_items_sold * products.price) as total_revenue\n",
    "from sales \n",
    "inner join products on products.product_id=sales.product_id\n",
    "inner join sellers on sellers.seller_id=sales.seller_id\n",
    "where sellers.seller_id >= 1 and sellers.seller_id <= 500\n",
    "\"\"\")\n",
    "\n",
    "popSQL.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59c00e0a-34de-4614-b783-71beb7503716",
     "showTitle": false,
     "title": ""
    },
    "id": "jdG80LVMxnQf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|product_name|\n",
      "+------------+\n",
      "| product_106|\n",
      "| product_117|\n",
      "| product_363|\n",
      "| product_712|\n",
      "| product_712|\n",
      "| product_843|\n",
      "| product_897|\n",
      "| product_897|\n",
      "| product_923|\n",
      "|product_1466|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (d) Among sellers with rating >= 4 who have achieved a combined number of products sold >= 3000, find out the top 10 most expensive product sold by any of the sellers. (If there are multiple products at the same price, please sort them in ascending order of product_id) [8m]\n",
    "# Your table should have 1 column(s): [product_name]\n",
    "# To get the full mark, your query should not run for more than 1 min\n",
    "\n",
    "sales_table.createOrReplaceTempView(\"sales\")\n",
    "products_table.createOrReplaceTempView(\"products\")\n",
    "sellers_table.createOrReplaceTempView(\"sellers\")\n",
    "\n",
    "first = spark.sql(\"\"\"\n",
    "select sellers.seller_id, sum(num_of_items_sold) as total\n",
    "from sales \n",
    "inner join sellers on sellers.seller_id=sales.seller_id\n",
    "where rating >= 4 \n",
    "group by sellers.seller_id\n",
    "\"\"\")\n",
    "first.createOrReplaceTempView(\"first\")\n",
    "\n",
    "second = spark.sql(\"\"\"\n",
    "select seller_id\n",
    "from first\n",
    "where total >= 3000\n",
    "\"\"\")\n",
    "second.createOrReplaceTempView(\"second\")\n",
    "\n",
    "third = spark.sql(\"\"\"\n",
    "select product_name\n",
    "from sales \n",
    "inner join products on products.product_id=sales.product_id\n",
    "inner join second on sales.seller_id=second.seller_id\n",
    "order by price desc, products.product_id\n",
    "\"\"\")\n",
    "\n",
    "third.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2551ab92-377c-4492-9d99-258610b143a1",
     "showTitle": false,
     "title": ""
    },
    "id": "4fziMyvTxnQg"
   },
   "source": [
    "## Task 2: Spark ML (10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ebc093d-9256-4e99-85d3-3d36b50a6053",
     "showTitle": false,
     "title": ""
    },
    "id": "wtocOKQXxnQg"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eee140e-773a-4e76-9f6c-40e809e136b0",
     "showTitle": false,
     "title": ""
    },
    "id": "lQB18KhnxnQg"
   },
   "outputs": [],
   "source": [
    "bank_train_location = \"/FileStore/tables/bank_train.csv\"\n",
    "bank_test_location = \"/FileStore/tables/bank_test.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "bank_train = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(bank_train_location)\n",
    "\n",
    "bank_test = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(bank_test_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98477bc0-fdf9-4585-8cf2-24b4b0ebc3f1",
     "showTitle": false,
     "title": ""
    },
    "id": "YTZevHlAxnQg"
   },
   "source": [
    "Build ML model to predict whether the customer will subscribe bank deposit service or not. Train the model using training set and evaluate the model performance (e.g. accuracy) using testing set. \n",
    "* You can explore different methods to pre-process the data and select proper features\n",
    "* You can utilize different machine learning models and tune model hyperparameters\n",
    "* Present the final testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07aaf5a-6fb8-425a-a3c9-f52e04e49828",
     "showTitle": false,
     "title": ""
    },
    "id": "iey06VQfxnQg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+---------+-------+-------+-------+----+--------+--------+-----+--------+--------+-----+\n",
      "|age|       job|marital|education|default|balance|housing|loan|duration|campaign|pdays|previous|poutcome|label|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+--------+--------+-----+--------+--------+-----+\n",
      "| 45|    admin.|married|  unknown|     no|   2033|     no|  no|      48|       4|   -1|       0| unknown|    0|\n",
      "| 56|    admin.|married|  primary|     no|    202|    yes|  no|     178|       2|   -1|       0| unknown|    0|\n",
      "| 50| housemaid| single|secondary|     no|    799|     no|  no|      63|       1|   -1|       0| unknown|    0|\n",
      "| 58|    admin.|married|secondary|     no|   1464|    yes| yes|      53|      29|   -1|       0| unknown|    0|\n",
      "| 43|management| single| tertiary|     no|  11891|     no|  no|     821|       5|  242|       1| success|    1|\n",
      "+---+----------+-------+---------+-------+-------+-------+----+--------+--------+-----+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[3.0,0.0,3.0,0.0,...|\n",
      "|    0|[3.0,0.0,2.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data preparation (4m)\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# remove contact, day, month\n",
    "unecessary_cols = ('contact', 'day', 'month')\n",
    "bank_train = bank_train.drop(*unecessary_cols)\n",
    "bank_test = bank_test.drop(*unecessary_cols)\n",
    "bank_train.show(5)\n",
    "\n",
    "# convert categorical values to numerical features\n",
    "CONTI_FEATURES  = ['age', 'balance','duration', 'campaign', 'pdays', 'previous']\n",
    "CATE_FEATURES = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'poutcome']\n",
    "stages = [] # stages in our Pipeline\n",
    "\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    stages.append(stringIndexer)\n",
    "    \n",
    "assemblerInputs = [c + \"Index\" for c in CATE_FEATURES] + CONTI_FEATURES\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages.append(assembler)\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# To make the computation faster, convert model to a DataFrame.\n",
    "pipelineModel_train = pipeline.fit(bank_train)\n",
    "model_train = pipelineModel_train.transform(bank_train)\n",
    "input_data = model_train.rdd.map(lambda x: (x[\"label\"], DenseVector(x[\"features\"])))\n",
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "df_train.show(2)\n",
    "\n",
    "pipelineModel_test = pipeline.fit(bank_test)\n",
    "model_test = pipelineModel_test.transform(bank_test)\n",
    "input_data = model_test.rdd.map(lambda x: (x[\"label\"], DenseVector(x[\"features\"])))\n",
    "df_test = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04b59e4-6197-451c-8071-52526a5a724f",
     "showTitle": false,
     "title": ""
    },
    "id": "PsIotb9ExnQh"
   },
   "outputs": [],
   "source": [
    "# model building (4m)\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42, maxDepth=10)\n",
    "rfModel = rf.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e1c949-8291-45be-8872-c0310777c6fa",
     "showTitle": false,
     "title": ""
    },
    "id": "OC5ufJqAxnQh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy = 81.92149317088963\n"
     ]
    }
   ],
   "source": [
    "# model evaluation (2m)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = rfModel.transform(df_test)\n",
    "predictions.select(\"label\", \"prediction\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cs4225_a2_databricks_student_version",
   "notebookOrigID": 2711057211168410,
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
