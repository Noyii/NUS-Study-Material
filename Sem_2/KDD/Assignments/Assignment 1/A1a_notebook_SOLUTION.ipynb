{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013ef117",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 1 - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14255f72",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "Note on Solutions: The solutions below are just one possible set of answers - in general, other valid solutions may also exist, and as long as we find your answer reasonable and justified, it would be graded as correct. \n",
    "    \n",
    "</font>\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A1_YourName_YourNUSNETID.ipynb** (e.g., **A1_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Submission deadline is 5 March 2023 (Sunday 11.59pm). You can submit by uploading them to the Canvas submission folder. You have 4 late days which can be used for either assignment, that extend the deadline by 24 hours each. No need to send any emails to use them, just submit late and they will be counted automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16632593",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion.\n",
    "\n",
    "* **Q1: Data Cleaning & Exploratory Data Analysis (EDA) (30 Points)**\n",
    "    * 1 (a) Removing \"Dirty\" Records (6 Points)\n",
    "    * 1 (b) Handling Missing (NaN) Values (6 Points)\n",
    "    * 1 (c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "    * 1 (d) Handling of Categorical Attributes (4 Points)\n",
    "    * 1 (e) Basic Facts about a Real-World Dataset (8 Points)\n",
    "* **Q2: DBSCAN (10 Points)**\n",
    "    * 2 (a) Running DBSCAN and Visualization (3 Points)\n",
    "    * 2 (b) Effects of Data Manipulation on DBSCAN Results (3 Points)\n",
    "    * 2 (c) Identifying Noise/Outliers with Clustering beyond DBSCAN (4 Points)\n",
    "* **Q3: Clustering Algorithms (18 Points)**\n",
    "    * 3 (a) Questions about K-Means (12 Points)\n",
    "    * 3 (b) Interpreting Dendrograms (6 Points)\n",
    "* **Q4: Association Rule Mining (12 Points)**\n",
    "    * 4 (a) Compare the Runs A-D and Discuss your Observations! (4 Points) \n",
    "    * 4 (b) Compare the Runs A-D and discuss the results for building a recommendation engine! (4 Points)\n",
    "    * 4 (c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f62133",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6623e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from efficient_apriori import apriori\n",
    "from src.utils import support, confidence, show_top_rules\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df294569",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4d408",
   "metadata": {},
   "source": [
    "# Q1: Data Cleaning & Explorative Data Analysis (EDA) (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abd9b6",
   "metadata": {},
   "source": [
    "For the following tasks, we consider a dataset containing information 20,000 past resale transactions of condo flats. Each record (i.e., data samples) consists of 12 attributes. The following **data description** list all attributes together with a brief description of each attribute's data type / domain:\n",
    "\n",
    "* **transaction_id**: Unique ID of the resale transactions; an 8-digit integer number uniquely assigned to each transaction.\n",
    "* **url**: Unique link to a website documenting this transaction as a string value.\n",
    "* **name**: The name of the condo as a string value (e.g., \"estella gardens\", \"eedon green\").\n",
    "* **type**: The type of condo as string value (e.g., \"condominium\", \"apartment\").\n",
    "* **postal_district**: The postal district the condo is located in as integer value; Singapore has 28 postal districts: 1, 2, ..., 28 (cf. [here](https://www.ura.gov.sg/realEstateIIWeb/resources/misc/list_of_postal_districts.htm)).\n",
    "* **subzone**: The subzone the condo is located in as a string value.\n",
    "* **planning_area**: The planning area the condo is located in as a string value.\n",
    "* **region**: The region the condo is located in as a string value.\n",
    "* **date_of_sale**: The date (month & year) of the transaction as a string value (e.g., \"mar-19\", \"oct-20\").\n",
    "* **area_sqft**: The size of the condo flat in square feet as a positive integer value.\n",
    "* **floor_level**: The range of floors in which the flat is located in the condo as string value (e.g., \"06 to 10\", \"11 to 15\").\n",
    "* **eco_category**: The eco category of the condo as a single-character string value (e.g., \"A\", \"B\", \"C\", \"D\").\n",
    "* **price**: Resale price of the condo flat in Singapore Dollar as an integer value.\n",
    "\n",
    "Additional information: Singapore has 55 planning areas; each split into multiple subzones (if interested, you can check the corresponding [Wikipedia article](https://en.wikipedia.org/wiki/List_of_places_in_Singapore)).\n",
    "\n",
    "**Important:** In each of the following subtasks we use a slightly different version of the dataset. This allows you to focus on the specific aspects of data cleaning / data preprocessing addresses in the respective subtask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1127c3e",
   "metadata": {},
   "source": [
    "### 1 (a) Removing \"Dirty\" Records (6 Points)\n",
    "\n",
    "We argued in the lecture that almost all real-world datasets contain some form of noise that might negatively affect any applied data analysis. The very first -- and in some sense -- easiest way to identify noise is to check if all data confirms with the data description. The following code cell shows a snippet of the dataset which you will be looking at in this subtask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b356c2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>type</th>\n",
       "      <th>postal_district</th>\n",
       "      <th>subzone</th>\n",
       "      <th>planning_area</th>\n",
       "      <th>date_of_sale</th>\n",
       "      <th>area_sqft</th>\n",
       "      <th>floor_level</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82953057</td>\n",
       "      <td>treasure at tampines</td>\n",
       "      <td>tampines lane</td>\n",
       "      <td>condominium</td>\n",
       "      <td>18</td>\n",
       "      <td>tampines east</td>\n",
       "      <td>tampines</td>\n",
       "      <td>jul-19</td>\n",
       "      <td>818</td>\n",
       "      <td>06 to 10</td>\n",
       "      <td>1024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17139360</td>\n",
       "      <td>parc central residences</td>\n",
       "      <td>tampines street 86</td>\n",
       "      <td>executive condominium</td>\n",
       "      <td>18</td>\n",
       "      <td>tampines west</td>\n",
       "      <td>tampines</td>\n",
       "      <td>apr-21</td>\n",
       "      <td>990</td>\n",
       "      <td>11 to 15</td>\n",
       "      <td>1187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22288515</td>\n",
       "      <td>martina mansions</td>\n",
       "      <td>bukit timah road</td>\n",
       "      <td>apartment</td>\n",
       "      <td>10</td>\n",
       "      <td>nassim</td>\n",
       "      <td>tanglin</td>\n",
       "      <td>jan-20</td>\n",
       "      <td>1259</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>2300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80737365</td>\n",
       "      <td>the pier at robertson</td>\n",
       "      <td>mohamed sultan road</td>\n",
       "      <td>apartment</td>\n",
       "      <td>9</td>\n",
       "      <td>robertson quay</td>\n",
       "      <td>singapore river</td>\n",
       "      <td>jun-19</td>\n",
       "      <td>1044</td>\n",
       "      <td>06 to 10</td>\n",
       "      <td>2400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83154310</td>\n",
       "      <td>rivercove residences</td>\n",
       "      <td>anchorvale lane</td>\n",
       "      <td>executive condominium</td>\n",
       "      <td>19</td>\n",
       "      <td>anchorvale</td>\n",
       "      <td>sengkang</td>\n",
       "      <td>oct-18</td>\n",
       "      <td>1184</td>\n",
       "      <td>16 to 20</td>\n",
       "      <td>1239100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id                     name               street  \\\n",
       "0       82953057     treasure at tampines        tampines lane   \n",
       "1       17139360  parc central residences   tampines street 86   \n",
       "2       22288515         martina mansions     bukit timah road   \n",
       "3       80737365    the pier at robertson  mohamed sultan road   \n",
       "4       83154310     rivercove residences      anchorvale lane   \n",
       "\n",
       "                    type  postal_district         subzone    planning_area  \\\n",
       "0            condominium               18   tampines east         tampines   \n",
       "1  executive condominium               18   tampines west         tampines   \n",
       "2              apartment               10          nassim          tanglin   \n",
       "3              apartment                9  robertson quay  singapore river   \n",
       "4  executive condominium               19      anchorvale         sengkang   \n",
       "\n",
       "  date_of_sale  area_sqft floor_level    price  \n",
       "0       jul-19        818    06 to 10  1024000  \n",
       "1       apr-21        990    11 to 15  1187000  \n",
       "2       jan-20       1259    01 to 05  2300000  \n",
       "3       jun-19       1044    06 to 10  2400000  \n",
       "4       oct-18       1184    16 to 20  1239100  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_condo_dirty = pd.read_csv('data/a1-condo-resale-dirty.csv')\n",
    "\n",
    "df_condo_dirty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccfc76",
   "metadata": {},
   "source": [
    "If you check the dataset against its description, you will notice that many records are \"dirty\". **We define a record as \"dirty\" if it does not adhere to the given data description (see above)**. Such records are not guaranteed to be valid and should therefore not be used for any analysis.\n",
    "\n",
    "**Identify 2 causes of \"dirty\" records and remove all corresponding records from the dataset!** Please provide your answer in the markdown cell below. Additional (simplifying) guidelines:\n",
    "\n",
    "* Ignore missing (`NaN`) values -- that is, a record containing one or more missing values does not make this record dirty. We look at missing values in a subsequent task.\n",
    "* Ignore the correctness of string values -- we do not expect you to check, e.g., if a street name contains a typo or a planning area is indeed one of the existing 55 planning areas in Singapore\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c6731",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "* Invalid transaction ids (ids other than 8-digit integers)\n",
    "* Invalid postal districts (some records have a district of 0)\n",
    "* Invalid area sizes (some records have a negative size); `< 0` or `<= 0` is both fine, doesn't change the result\n",
    "* Removal of duplicates (ids must be unique according to data description)\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704b01b",
   "metadata": {},
   "source": [
    "Use the code cell below to actually implement your steps for removing the \"dirty\" records. The results should back up your answer above.\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete -- `pandas` is really powerful and should be your best friend here. If you use loops but the results are correct, there will be some minor deduction of points. But note that it's of course better to have a working solution using loops than having no solution at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b766993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 11)\n",
      "(19382, 11)\n",
      "After cleaning, there are now 17308 records.\n"
     ]
    }
   ],
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_cleaned = df_condo_dirty.copy()\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "print(df_cleaned.shape)\n",
    "\n",
    "# Remove all transaction with invalid transaction id (only 8 digits)\n",
    "df_cleaned = df_cleaned[df_cleaned.transaction_id.str.match('^\\d{8}$')]\n",
    "\n",
    "print(df_cleaned.shape)\n",
    "\n",
    "# Remove all transactions with invalid area size\n",
    "df_cleaned = df_cleaned.drop(df_cleaned[df_cleaned.area_sqft <= 0].index)\n",
    "\n",
    "# Remove all transactions with invalid postal district\n",
    "df_cleaned = df_cleaned.drop(df_cleaned[(df_cleaned.postal_district < 1) | (df_cleaned.postal_district > 28)].index)\n",
    "\n",
    "# Drop duplicates (w.r.t. transaction id)\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['transaction_id'])\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "print('After cleaning, there are now {} records.'.format(df_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825297c",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after the cleaning step as there is some wiggle room regarding the performed steps which would affect this result. As such, even if two solutions are correct, they do not necessarily yield the same number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6433ecc",
   "metadata": {},
   "source": [
    "### 1 (b) Handling Missing (NaN) Values (6 Points)\n",
    "\n",
    "Many to most traditional data mining algorithms do not like missing (NaN) values and will throw an error if missing values are present. We therefore have to address missing values and get rid of them. On the other hand, we want to preserve as much of our dataset as possible, so we need to be smart about that. In this subtask, you are provided with a version of our condo resale dataset that contains missing values but is otherwise clean -- so it is all about the `NaN` values here.\n",
    "\n",
    "Let's load the dataset and have a quick look -- the attributes are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdba981f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>type</th>\n",
       "      <th>postal_district</th>\n",
       "      <th>subzone</th>\n",
       "      <th>planning_area</th>\n",
       "      <th>date_of_sale</th>\n",
       "      <th>area_sqft</th>\n",
       "      <th>floor_level</th>\n",
       "      <th>eco_category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95103981</td>\n",
       "      <td>http://condo-sg.com/alex-residences-3570</td>\n",
       "      <td>alex residences</td>\n",
       "      <td>alexandra view</td>\n",
       "      <td>apartment</td>\n",
       "      <td>3</td>\n",
       "      <td>redhill</td>\n",
       "      <td>bukit merah</td>\n",
       "      <td>oct-21</td>\n",
       "      <td>883</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>1595880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69374014</td>\n",
       "      <td>http://condo-sg.com/the-rivervale-4160</td>\n",
       "      <td>the rivervale</td>\n",
       "      <td>rivervale link</td>\n",
       "      <td>executive condominium</td>\n",
       "      <td>19</td>\n",
       "      <td>rivervale</td>\n",
       "      <td>sengkang</td>\n",
       "      <td>jul-17</td>\n",
       "      <td>1313</td>\n",
       "      <td>06 to 10</td>\n",
       "      <td>A</td>\n",
       "      <td>900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54377409</td>\n",
       "      <td>http://condo-sg.com/parc-palais-5143</td>\n",
       "      <td>parc palais</td>\n",
       "      <td>hume avenue</td>\n",
       "      <td>condominium</td>\n",
       "      <td>21</td>\n",
       "      <td>hillview</td>\n",
       "      <td>bukit batok</td>\n",
       "      <td>sep-21</td>\n",
       "      <td>1389</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>1810000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64749050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hedges park condominium</td>\n",
       "      <td>flora drive</td>\n",
       "      <td>condominium</td>\n",
       "      <td>17</td>\n",
       "      <td>flora drive</td>\n",
       "      <td>pasir ris</td>\n",
       "      <td>may-19</td>\n",
       "      <td>484</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>565000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18136469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kingsford waterbay</td>\n",
       "      <td>upper serangoon view</td>\n",
       "      <td>apartment</td>\n",
       "      <td>19</td>\n",
       "      <td>kangkar</td>\n",
       "      <td>hougang</td>\n",
       "      <td>may-21</td>\n",
       "      <td>678</td>\n",
       "      <td>06 to 10</td>\n",
       "      <td>A</td>\n",
       "      <td>875000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id                                       url  \\\n",
       "0        95103981  http://condo-sg.com/alex-residences-3570   \n",
       "1        69374014    http://condo-sg.com/the-rivervale-4160   \n",
       "2        54377409      http://condo-sg.com/parc-palais-5143   \n",
       "3        64749050                                       NaN   \n",
       "4        18136469                                       NaN   \n",
       "\n",
       "                      name                street                   type  \\\n",
       "0          alex residences        alexandra view              apartment   \n",
       "1            the rivervale        rivervale link  executive condominium   \n",
       "2              parc palais           hume avenue            condominium   \n",
       "3  hedges park condominium           flora drive            condominium   \n",
       "4       kingsford waterbay  upper serangoon view              apartment   \n",
       "\n",
       "   postal_district      subzone planning_area date_of_sale  area_sqft  \\\n",
       "0                3      redhill   bukit merah       oct-21        883   \n",
       "1               19    rivervale      sengkang       jul-17       1313   \n",
       "2               21     hillview   bukit batok       sep-21       1389   \n",
       "3               17  flora drive     pasir ris       may-19        484   \n",
       "4               19      kangkar       hougang       may-21        678   \n",
       "\n",
       "  floor_level eco_category      price  \n",
       "0    01 to 05            A  1595880.0  \n",
       "1    06 to 10            A   900000.0  \n",
       "2    01 to 05            A  1810000.0  \n",
       "3    01 to 05            A   565000.0  \n",
       "4    06 to 10            A   875000.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_condo_nan = pd.read_csv('data/a1-condo-resale-nan.csv')\n",
    "\n",
    "df_condo_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f368e68",
   "metadata": {},
   "source": [
    "Since your decision for handling `NaN` values might depend in the data mining task, assume in the following that you want to use this dataset to **create a regression model to predict the resale price** from the attributes of a transaction. Of course, there will be no need to actually create such a model here :).\n",
    "\n",
    "**Identify all `NaN` values in the dataset and handle them appropriately!** After this preprocessing, the resulting dataset should no longer contain any `NaN` values. Additional (simplifying) hints or guidelines:\n",
    "\n",
    "* You can use the `.info()` function of a pandas dataset to get info about its `NaN` values.\n",
    "* You do not need to consider external knowledge (i.e., information coming from outside this dataset), or sophisticated solutions such as [`sklearn.impute.KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html). These can be very useful in practice (and maybe for your project), but their application requires certain assumptions to hold for good results. This is beyond the scope of this assignment.\n",
    "* Remove column `url`; one can argue that this is just a label and has no information for any analysis\n",
    "* Remove all records where `price` or `area_sqft` is `NaN`; these attributes are vey important for creating the model and there's no obvious way to reliably derive it\n",
    "* Derive missing `planning_area` values from subzone values; there's a clear mapping from a subzone to the corresponding planning area\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15f4dd",
   "metadata": {},
   "source": [
    "Use the code cell below to actually implement your steps for handling `NaN` values. The results should back up your answer above.\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete -- pandas is really powerful and should be your best friend here. If you use loops but the results are correct, there will be some minor deduction of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c057b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling missing values, there are now 19033 records.\n",
      "Number of records with an NaN for any attribute: 0\n"
     ]
    }
   ],
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_no_nan = df_condo_nan.copy()\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "# How to find NaN values\n",
    "#df_no_nan.info()\n",
    "\n",
    "# Drop records with NA values for \"price\" (required for creating model)\n",
    "df_no_nan = df_no_nan.dropna(subset=['price'])\n",
    "\n",
    "# Drop records with NA values for \"area_sqft\" (too important to be dropped; too difficult to reliably estimate/derive)\n",
    "df_no_nan = df_no_nan.dropna(subset=['area_sqft'])\n",
    "\n",
    "# Drop column url (not important for any analysis)\n",
    "df_no_nan = df_no_nan.drop(['url'], axis = 1)\n",
    "\n",
    "# Map from subzone back to missing planning areas\n",
    "df_tmp = df_no_nan.dropna(subset=['planning_area']) # Need to avoid an accidental mapping to NaN\n",
    "zone2area = dict (zip(df_tmp.subzone, df_tmp.planning_area)) # Create dictionary subzone->planning_area\n",
    "df_no_nan.planning_area = df_no_nan.subzone.map(zone2area) # Update planning area based on mapping\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "print('After handling missing values, there are now {} records.'.format(df_no_nan.shape[0]))\n",
    "print('Number of records with an NaN for any attribute: {}'.format((df_no_nan.isna().sum(axis=1) > 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359cd2a",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after this preprocessing step as there is some wiggle room regarding the performed steps which would affect this result. However, the number of records with `NaN` values should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2fde4",
   "metadata": {},
   "source": [
    "### 1 (c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "\n",
    "Identifying \"dirty\" records and missing data are two very fundamental and generally rather systematic steps as part of data cleaning / data preprocessing. However, as we saw in the lecture using some examples, there are many other issues with the dataset that can be considered noise and thus potentially negatively affecting any data analysis. So the more noise we can remove, the more likely we can expect meaning analysis results.\n",
    "\n",
    "For this subtask, we use a version of our condo resale dataset **with no \"dirty\" records or missing data**! Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c5ced0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>postal_district</th>\n",
       "      <th>subzone</th>\n",
       "      <th>planning_area</th>\n",
       "      <th>date_of_sale</th>\n",
       "      <th>area_sqft</th>\n",
       "      <th>floor_level</th>\n",
       "      <th>eco_category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72606080.0</td>\n",
       "      <td>https://condo-sg.com/amber-park-4904</td>\n",
       "      <td>amber park</td>\n",
       "      <td>condominium</td>\n",
       "      <td>15</td>\n",
       "      <td>marine parade</td>\n",
       "      <td>marine parade</td>\n",
       "      <td>apr-21</td>\n",
       "      <td>678</td>\n",
       "      <td>16 to 20</td>\n",
       "      <td>A</td>\n",
       "      <td>1679130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62765491.0</td>\n",
       "      <td>https://condo-sg.com/braddell-view-5318</td>\n",
       "      <td>braddell view</td>\n",
       "      <td>apartment</td>\n",
       "      <td>20</td>\n",
       "      <td>toa payoh west</td>\n",
       "      <td>Toa Payoh</td>\n",
       "      <td>oct-20</td>\n",
       "      <td>1701</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>1255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65584694.0</td>\n",
       "      <td>https://condo-sg.com/tangerine-grove-2635</td>\n",
       "      <td>tangerine grove</td>\n",
       "      <td>condominium</td>\n",
       "      <td>19</td>\n",
       "      <td>tai seng</td>\n",
       "      <td>hougang</td>\n",
       "      <td>nov-18</td>\n",
       "      <td>947</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>1130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16401608.0</td>\n",
       "      <td>https://condo-sg.com/goldenhill-park-condomini...</td>\n",
       "      <td>goldenhill park condominium</td>\n",
       "      <td>condominium</td>\n",
       "      <td>20</td>\n",
       "      <td>lorong chuan</td>\n",
       "      <td>serangoon</td>\n",
       "      <td>mar-21</td>\n",
       "      <td>1313</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>2000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53975895.0</td>\n",
       "      <td>https://condo-sg.com/kingsford-waterbay-1085</td>\n",
       "      <td>kingsford waterbay</td>\n",
       "      <td>apartment</td>\n",
       "      <td>19</td>\n",
       "      <td>kangkar</td>\n",
       "      <td>hougang</td>\n",
       "      <td>may-17</td>\n",
       "      <td>883</td>\n",
       "      <td>06 to 10</td>\n",
       "      <td>A</td>\n",
       "      <td>1030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id                                                url  \\\n",
       "0      72606080.0               https://condo-sg.com/amber-park-4904   \n",
       "1      62765491.0            https://condo-sg.com/braddell-view-5318   \n",
       "2      65584694.0          https://condo-sg.com/tangerine-grove-2635   \n",
       "3      16401608.0  https://condo-sg.com/goldenhill-park-condomini...   \n",
       "4      53975895.0       https://condo-sg.com/kingsford-waterbay-1085   \n",
       "\n",
       "                          name         type  postal_district         subzone  \\\n",
       "0                   amber park  condominium               15   marine parade   \n",
       "1                braddell view    apartment               20  toa payoh west   \n",
       "2              tangerine grove  condominium               19        tai seng   \n",
       "3  goldenhill park condominium  condominium               20    lorong chuan   \n",
       "4           kingsford waterbay    apartment               19         kangkar   \n",
       "\n",
       "   planning_area date_of_sale  area_sqft floor_level eco_category    price  \n",
       "0  marine parade       apr-21        678    16 to 20            A  1679130  \n",
       "1      Toa Payoh       oct-20       1701    01 to 05            A  1255000  \n",
       "2        hougang       nov-18        947    01 to 05            A  1130000  \n",
       "3      serangoon       mar-21       1313    01 to 05            A  2000000  \n",
       "4        hougang       may-17        883    06 to 10            A  1030000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_condo_others = pd.read_csv('data/a1-condo-resale-others.csv')\n",
    "\n",
    "df_condo_others.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999e6da",
   "metadata": {},
   "source": [
    "**Your boss points out the following data quality issues; please explain what the issues are and how you would deal with them:** Please provide your answer in the markdown cell below list necessary steps with a justification for your decision. \n",
    "\n",
    "* Outliers in `area_sqft`;\n",
    "* Inconsistent naming in `planning_area`;\n",
    "\n",
    "Additional (simplifying) guidelines:\n",
    "* You should still assume that we want to use this dataset to create a model for predicting the resale price of a flat based on its attributes. The choice of data mining task can affect your decision for what cleaning / preprocessing steps to apply.\n",
    "* There is no need to consider external knowledge. For example, you do not have to check if a value for `subzone` is indeed an existing subzone of Singapore.\n",
    "* There is no need for you to implement any processing steps! Most important are your justifications for your decisions.\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71854b00",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "* Remove Outliers: `area_sqft = 1` (area size way too small to be realistic)\n",
    "* Normalize all planning areas to lowercase (or otherwise normalize the strings)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a66fa8",
   "metadata": {},
   "source": [
    "### 1 (d) Handling Categorical Attributes (4 Points)\n",
    "\n",
    "Many to most data mining algorithms require all input features / attributes to be numerical. Our dataset with transactions resales of condo flats contains attributes that are not all numerical. As such, assuming we indeed want to utilize them, we need to convert those attributes into numerical ones. Regarding encoding techniques, we covered [One-Hot Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) in the lecture. For variables with too many categories (e.g. >30) to be suitable for one-hot encoding, a common alternative is [Target Encoding](https://contrib.scikit-learn.org/category_encoders/targetencoder.html), which replaces each category with a numeric value (roughly, the average value of the target variable for that particular category).\n",
    "\n",
    "Some common choices for how to handle a categorical attribute include\n",
    "* **Drop**: to drop a variable if it is not likely to be useful for modelling\n",
    "* **Ordinal**: treat it as an ordinal variable\n",
    "* **One-Hot Encoding**: encode each of its categories into a binary attribute\n",
    "* **Target Encoding**: encoder the entire variable into a single numerical attribute\n",
    "\n",
    "**For each of the 4 above choices, select *1* variable that you believe is suitable to be handled using that choice, and justify.**\n",
    "\n",
    "There is no single correct answer for this task; it's your justification that matters. Again, assume that we want to create a regression model to predict the resale price of a flat based on the other features.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9097516",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "* Drop `transaction_id`, `url`, `eco_category` (not useful for regression model)\n",
    "* OK-ish to drop: `subzone`, `postal_district` (too many values, kind of covered by planning area)\n",
    "* Ordinal-Enconding: `floor_level`\n",
    "* One-Hot encoding: `type` (not that many different value)\n",
    "* Target encoding: `name`, `planning area` (too many unique values for one-hot, but it's arguably an important attribute)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35806cdd",
   "metadata": {},
   "source": [
    "### 1(e) Basic Facts about a Real-World Dataset (8 Points)\n",
    "\n",
    "The following tasks are about getting basic insights into the Condo Resale Prices dataset. As the data preprocessing steps you choose to perform might affect the results of this task, we will use a modified version here. Note that this version contains 50,000 listing of condo resale transactions and does **not** contain any \"dirty\" records. This is to ensure that everyone uses the same data. This helps marking your solutions as we know which results to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc252893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>postal_district</th>\n",
       "      <th>subzone</th>\n",
       "      <th>planning_area</th>\n",
       "      <th>date_of_sale</th>\n",
       "      <th>area_sqft</th>\n",
       "      <th>floor_level</th>\n",
       "      <th>eco_category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13685562</td>\n",
       "      <td>https://condo-sg.com/the-tapestry-9447</td>\n",
       "      <td>the tapestry</td>\n",
       "      <td>condominium</td>\n",
       "      <td>18</td>\n",
       "      <td>tampines west</td>\n",
       "      <td>tampines</td>\n",
       "      <td>apr-18</td>\n",
       "      <td>700</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53321107</td>\n",
       "      <td>https://condo-sg.com/the-interlace-4916</td>\n",
       "      <td>the interlace</td>\n",
       "      <td>condominium</td>\n",
       "      <td>4</td>\n",
       "      <td>depot road</td>\n",
       "      <td>bukit merah</td>\n",
       "      <td>jun-17</td>\n",
       "      <td>1044</td>\n",
       "      <td>11 to 15</td>\n",
       "      <td>A</td>\n",
       "      <td>1200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10676043</td>\n",
       "      <td>https://condo-sg.com/d'leedon-5888</td>\n",
       "      <td>d'leedon</td>\n",
       "      <td>condominium</td>\n",
       "      <td>10</td>\n",
       "      <td>farrer court</td>\n",
       "      <td>bukit timah</td>\n",
       "      <td>aug-21</td>\n",
       "      <td>635</td>\n",
       "      <td>11 to 15</td>\n",
       "      <td>A</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88341877</td>\n",
       "      <td>https://condo-sg.com/mulberry-tree-4753</td>\n",
       "      <td>mulberry tree</td>\n",
       "      <td>apartment</td>\n",
       "      <td>11</td>\n",
       "      <td>moulmein</td>\n",
       "      <td>novena</td>\n",
       "      <td>may-18</td>\n",
       "      <td>667</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69569828</td>\n",
       "      <td>https://condo-sg.com/ue-square-3203</td>\n",
       "      <td>ue square</td>\n",
       "      <td>apartment</td>\n",
       "      <td>9</td>\n",
       "      <td>robertson quay</td>\n",
       "      <td>singapore river</td>\n",
       "      <td>apr-18</td>\n",
       "      <td>1206</td>\n",
       "      <td>01 to 05</td>\n",
       "      <td>A</td>\n",
       "      <td>1950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id                                      url           name  \\\n",
       "0        13685562   https://condo-sg.com/the-tapestry-9447   the tapestry   \n",
       "1        53321107  https://condo-sg.com/the-interlace-4916  the interlace   \n",
       "2        10676043       https://condo-sg.com/d'leedon-5888       d'leedon   \n",
       "3        88341877  https://condo-sg.com/mulberry-tree-4753  mulberry tree   \n",
       "4        69569828      https://condo-sg.com/ue-square-3203      ue square   \n",
       "\n",
       "          type  postal_district         subzone    planning_area date_of_sale  \\\n",
       "0  condominium               18   tampines west         tampines       apr-18   \n",
       "1  condominium                4      depot road      bukit merah       jun-17   \n",
       "2  condominium               10    farrer court      bukit timah       aug-21   \n",
       "3    apartment               11        moulmein           novena       may-18   \n",
       "4    apartment                9  robertson quay  singapore river       apr-18   \n",
       "\n",
       "   area_sqft floor_level eco_category    price  \n",
       "0        700    01 to 05            A   931500  \n",
       "1       1044    11 to 15            A  1200000  \n",
       "2        635    11 to 15            A  1100000  \n",
       "3        667    01 to 05            A  1100000  \n",
       "4       1206    01 to 05            A  1950000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_condo_facts = pd.read_csv('data/a1-condo-resale-facts.csv')\n",
    "\n",
    "df_condo_facts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862e416",
   "metadata": {},
   "source": [
    "Please complete the table below by answering the given questions. Use the code cell below the table to actually implement your steps that enabled you to answer the questions. There is no need for a fancy layout for any print statement; it's only important that the result is clear.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865dc73",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)-(6). Answers (1)-(4) are worth 1 Point each; Answer (5)-(6) are worth 2 Points.\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| (1)  | What is the date (month & year) of the first transactions? | <font color='red'>2016-12 (2018-04 also acceptable, interpreting 'first' as first row)</font> |\n",
    "| (2)  | For each `type`, how many transactions are in the dataset?  | <font color='red'>apartment (8,038); condominium (9,688); executive condominium (2255); strata detached (2); strata semi-detached (3); strata terrace (14)</font> |\n",
    "| (3)  | What is the planning area with the most transactions? List the name of the planning area and the number of transactions!  | <font color='red'>bedok: 1,307</font> |\n",
    "| (4)  | What is the correlation between the resale *price* and *area_sqft*? | <font color='red'>0.74</font> |\n",
    "| (5)  | Which transaction in postal district 11 had the highest price-to-area ratio (i.e., the highest price per square foot)? List the name of the condo and the price per square foot (rounded to 2 decimals)| <font color='red'>pullman residences newton: 3,083.96</font> |\n",
    "| (6)  | What is the number of transactions where the flat/apartment was between the 51st floor (inclusive) and 60th (inclusive) floor?  | <font color='red'>23</font> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ef06f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First transaction: 2016-12-01 00:00:00\n",
      "----------------------------------------------------------------------------------\n",
      "type\n",
      "apartment                8038\n",
      "condominium              9688\n",
      "executive condominium    2255\n",
      "strata detached             2\n",
      "strata semi-detached        3\n",
      "strata terrace             14\n",
      "Name: type, dtype: int64\n",
      "----------------------------------------------------------------------------------\n",
      "Number of transactions in Redhill for 3 Mio SGD or more: 16\n",
      "----------------------------------------------------------------------------------\n",
      "planning_area\n",
      "bedok    1307\n",
      "Name: planning_area, dtype: int64\n",
      "----------------------------------------------------------------------------------\n",
      "                           name    price_psf\n",
      "6613  pullman residences newton  3083.958021\n",
      "----------------------------------------------------------------------------------\n",
      "Correlation price / area (0.74)\n",
      "----------------------------------------------------------------------------------\n",
      "Number or transaction where the flat is between the 50th and 60th floor: 23\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "# Get the date (month & year) of the first transaction\n",
    "df_retail_dates = pd.to_datetime(df_condo_facts.date_of_sale, format='%b-%y')  \n",
    "print('First transaction: {}'.format(np.min(df_retail_dates)))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "\n",
    "# For each type, how many transactions\n",
    "df_group_by_type = df_condo_facts.groupby(['type'])['type'].count()\n",
    "print(df_group_by_type)\n",
    "print('----------------------------------------------------------------------------------')\n",
    "\n",
    "# Planning area with the most transactions and the number of transactions\n",
    "df_group_by_planning_area = df_condo_facts.groupby(['planning_area'])['planning_area'].count().sort_values(ascending=False)\n",
    "print(df_group_by_planning_area.head(1))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "\n",
    "# Correlation between area and price\n",
    "corr_area_sqft = df_condo_facts.area_sqft.corr(df_condo_facts.price)\n",
    "print('Correlation price / area ({:.2f})'.format(corr_area_sqft))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "\n",
    "# Condo (name) with the higest price-to-area ratio\n",
    "df_condo_facts['price_psf'] = df_condo_facts.price / df_condo_facts.area_sqft\n",
    "print(df_condo_facts[df_condo_facts.postal_district == 11].sort_values(by=['price_psf'], ascending=[False])[['name', 'price_psf']].head(1))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "\n",
    "# Number of transactions of flats between 50th and 60th floor\n",
    "df_condo_facts[['floor_level_min', 'floor_level_max']] = df_condo_facts['floor_level'].str.split(' to ', 1, expand=True)\n",
    "df_condo_facts.floor_level_min = df_condo_facts.floor_level_min.str.replace('([a-zA-Z])','',regex=True).astype('int')\n",
    "df_condo_facts.floor_level_max = df_condo_facts.floor_level_max.str.replace('([a-zA-Z])','',regex=True).astype('int')\n",
    "df_condo_facts['floor_level_avg'] = ((df_condo_facts.floor_level_min + df_condo_facts.floor_level_max) / 2).astype('int')\n",
    "df_mid_height = df_condo_facts.loc[(50 <= df_condo_facts.floor_level_min) & (60 >= df_condo_facts.floor_level_max)]\n",
    "print('Number or transaction where the flat is between the 50th and 60th floor: {}'.format(df_mid_height.shape[0]))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5336e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c78c88",
   "metadata": {},
   "source": [
    "# Q2: DBSCAN (10 Points)\n",
    "\n",
    "In Section 1 we focused on addressing any \"obvious\" noise a given dataset may contain. Obvious noise is not well defined, but in general it's this kind of noise that can be identified by simple analysis (e.g., looking at the domains of attributes, or simple statistics such as the distribution/histogram). However, some noise is more difficult to detect. Some outliers can only be identified as such when looking at the combination of different data points. Looking at individual attributes / features does not suffice here.\n",
    "\n",
    "An outlier refers to some record (i.e., data samples) that is very different compared to most other records. Next we see how we can utilize DBSCAN for this task. DBSCAN is a clustering algorithm with an explicit notion of noise points, i.e., data points that are dissimilar to data points that form clusters.\n",
    "\n",
    "We use in this section a small toy dataset -- 70 records, 2 numeric attributes -- for easy testing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ede94af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_dbscan_toy is (70, 2)\n"
     ]
    }
   ],
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a1-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f043c1",
   "metadata": {},
   "source": [
    "### 2(a) Running DBSCAN and Visualization (3 Points)\n",
    "\n",
    "**Run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset**. Use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters.\n",
    "\n",
    "The scikit-learn output contains a `labels_` variable, where, noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise.\n",
    "\n",
    "Your output of running `DBSCAN(...)` should be saved as a variable called `dbscan_clustering`, and you should construct a binary vector (of length 70) called `is_noise`, where `True` indicates noise points. (As long as the subsequent visualization works, it indicates that you have done it correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b279233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "# Run DBSCAN(...); the output should be saved in a variable called `dbscan_clustering`, \n",
    "# and also construct a binary variable called `is_noise`, where True indicates noise points.\n",
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=11).fit(X_dbscan_toy)\n",
    "is_noise = dbscan_clustering.labels_ < 0\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e4bfbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Noise points plotted as black crosses; other points plotted as circles with color indicating cluster\n",
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[~is_noise,0], X_dbscan_toy[~is_noise,1], c=dbscan_clustering.labels_[~is_noise], marker='o')\n",
    "plt.scatter(X_dbscan_toy[is_noise,0], X_dbscan_toy[is_noise,1], c='black', marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02312f",
   "metadata": {},
   "source": [
    "### 2 (b) Effects of Data Manipulation on DBSCAN Results (3 Points)\n",
    "\n",
    "Assume you have a $d$-dimensional dataset `X` in the Euclidean space, i.e., each data point as $d$ numerical features (with each feature value in the interval $[0, 1]$). After running DBSCAN over `X`, you get some clustering (again, we only assume it's not only noise). Now you create a new dataset `X_new` by multiplying all data points by 10 afterwards adding 100 to all data points (in Python, assuming X is a NumPy array this can simply be done by `X_new = X * 10 + 100`). Now you can run DBSCAN over `X_new`.\n",
    "\n",
    "**Explain how you have to change the parameters of DBSCAN for `X_new` to produce equivalent output to the original results on `X`!**. You can ignore any nondeterminism (e.g. border points being assigned to different clusters, as discussed in lecture), or duplicates.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68472d",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "    \n",
    "* The multiplication affects the absolute distances between the data points (scaling); the addition does have no effect at all (translation)\n",
    "* DBSCAN works on absolute distances between data points\n",
    "* We therefore need to scale epsilon the same way as the data (e.g., from 0.6 to 6) to get the same clusters and noise points\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997b26f",
   "metadata": {},
   "source": [
    "### 2 (c) Identifying Noise/Outliers with Clustering beyond DBSCAN (4 Points)\n",
    "\n",
    "Apart from DBSCAN, we also covered two other important clustering algorithms: K-Means and (agglomerative) hierarchical clustering. For all three clustering algorithms we looked in detail into their approach, and also discussed the individual strengths, weaknesses, and limitations. Particularly we saw that of these three algorithms, only DBSCAN has this explicit notion of noise points. But what about K-Means and hierarchical clustering?\n",
    "\n",
    "**Explain if K-Means and/or hierarchical clustering can potentially be utilized to identify noise/outliers in a dataset!** If your answer for an algorithm is \"No\", please provide a brief justification. If your answer for an algorithm is \"Yes\", provide a brief sketch (no pseudo code required; a basic description will do) how to use the algorithm for noise/outlier detection.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0dd01",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "   \n",
    "Both K-Means and hierarchical clustering can somewhat be used to identify noise points / outliers\n",
    "* K-Means: Outliers are more likely to end in (a) their own cluster or (b) are points in clusters far away from the cluster centroid\n",
    "* Hierarchical clustering: Outliers will be merged late into existing clusters; a dendrogram would visualize an outlier as a branch that connects very high up in the tree\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447469a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825407ce",
   "metadata": {},
   "source": [
    "# Q3: Clustering Algorithms (18 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0d4a2",
   "metadata": {},
   "source": [
    "### 3 (a) Questions about K-Means (12 Points)\n",
    "\n",
    "In the table below are 6 statements that are either True or False. Complete the table to specify whether a statement is True or False, and provide a brief explanation for your answer (Your explanation is more important than a simple True/False answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b51af",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(6).\n",
    "\n",
    "| No. | Statement                                                                                                   | True or False?       | Brief Explanation |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| (1)  | When using K-Means with K-Means++, then centroids are at all times at the position of existing data points | <font color=\"red\">False</font> | <font color=\"red\">The centroids are only guaranteed to match data points when initialized; this is very likely to change in the update phase</font> \n",
    "| (2)  | K-Means++ ensures that the result will not include any empty clusters. | <font color=\"red\">False</font> | <font color=\"red\">If the number of clusters $k$ is larger than the number of unique data points, there will be empty clusters.</font>\n",
    "| (3)  | K-Means, independent of the initialization method, will always converge to a local minimum (note: the global minimum is also counted as a local minimum) | <font color=\"red\">True</font> |  <font color=\"red\">This is true since it states *local* minimum, as the SSE will only ever decrease or remain the same.</font>  |\n",
    "| (4)  | K-Means++ will always converge to the global optimum. | <font color=\"red\">False</font> | <font color=\"red\">K-Means++ is still a randomized initialization method, so it's not guaranteed that an initial choice of centroids will yield the global optimum.</font>   |\n",
    "| (5)  | K-Means++ initialization is more costly than a random initialization of the centroids but generally converges faster. | <font color=\"red\">True</font> | <font color=\"red\">K-Mean++ ensures a good \"spread\" of the centroids (most of the time), yielding smaller changes of the centroids in the update phase and hence less iterations.</font> |\n",
    "| (6)  | K-Means is insensitive to data normalization/standardization -- that is, for the same $k$ and the same initial centroids, K-Means will yield the same clusters where the data is normalized/standardized or not. | <font color=\"red\">False</font> | <font color=\"red\">Normalization/Standardization can change the relative distance between data points. Hence the clusterings can differ.</font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b6d2f",
   "metadata": {},
   "source": [
    "### 3 (b) Interpreting Dendrograms (6 Points)\n",
    "\n",
    "We saw in the lecture that dendrograms are a meaningful way to visualize the hierarchical relationships between the data points with respect to the clustering. Properly interpreting is important to get a correct understanding of the underlying data.\n",
    "\n",
    "Below are the plots of 6 different datasets labeled A-F. Each dataset contains 30 data points, each with two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc664b",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-agnes-data-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f987945",
   "metadata": {},
   "source": [
    "Below are 6 dendrograms labeled 1-6. These dendograms show the clustering using **(Agglomerative) Hierarchical Clustering with Single Linkage** for the 6 datasets above, but in a random order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4f01e",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-agnes-dendrogram-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4337ac7",
   "metadata": {},
   "source": [
    "**Find the correct combinations of datasets and dendrograms** -- that is, find for each dataset the corresponding dendrogram! Give a brief explanation for each decision and complete the table below.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a132586",
   "metadata": {},
   "source": [
    "| Dataset | Dendrogram | Brief Explanation |\n",
    "| ---  | ---   | ---                  |\n",
    "| **A**    | <font color=\"red\">6</font> | <font color=\"red\">The dataset has 2 data points that can be considered outliers, which a visible as 2 data points in the dendrogram that get merged very late.</font> |\n",
    "| **B**    | <font color=\"red\">1</font> | <font color=\"red\">This is the only dataset that show points that with increasing distance which can result into this extreme form of a dendrogram of height/depth N-1</font> |\n",
    "| **C**    | <font color=\"red\">5</font> | <font color=\"red\">The data point in the middle can be considered an outlier w.r.t to Single Linkage, anf the dendrogram shows this as the single data points that gets merged last.</font> |\n",
    "| **D**    | <font color=\"red\">3</font> | <font color=\"red\">This dataset has arguably two cluster w.r.t. to Single Linkage, and these to cluster can be seen in the dendrogram as these two prominent subtrees</font> |\n",
    "| **E**    | <font color=\"red\">2</font> | <font color=\"red\">This dataset mostly looks like noise, which is best represented by a clear lack of structure in the dendrogram.</font> |\n",
    "| **F**    | <font color=\"red\">4</font> | <font color=\"red\">This dataset has 3 pronounced cluster which are represented by the 3 noticable subtrees in the dendrogram.</font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e35bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a4bf5",
   "metadata": {},
   "source": [
    "# Q4: Association Rule Mining (12 Points)\n",
    "\n",
    "Next, we focus on the **Apriori Algorithm for finding Frequent Itemsets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214755f",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "The following dataset has 5 transactions and 6 different items. The format is a list of tuples, where each tuple represents the set of items of an individual transaction. This format can also be used as input for the `efficient-apriori` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48b5ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7151926",
   "metadata": {},
   "source": [
    "#### Example of running the `efficient-apriori` package  (nothing for you to do here!)\n",
    "\n",
    "We run the apriori algorithm over the demo data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26794f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule [('cereal',) => ('milk',)] (support: 0.6, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('yogurt',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('bread', 'cereal') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cereal', 'yogurt') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese', 'yogurt') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese', 'milk') => ('yogurt',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('milk', 'yogurt')] (support: 0.4, confidence: 1.0, lift: 1.6666666666666667)\n"
     ]
    }
   ],
   "source": [
    "_, rules = apriori(transactions_demo, min_support=0.4, min_confidence=1.0)\n",
    "\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a146c",
   "metadata": {},
   "source": [
    "### Recommending Movies using Association Rule Mining (ARM)\n",
    "\n",
    "In this task, we look into using Association Rule Mining for recommending movies -- more specifically, recommending movies on physical mediums (Blu-ray, DVD, etc.), assuming that is still a thing nowadays :).\n",
    "\n",
    "**Dataset.** We use a popular movie ratings dataset from [MovieLens](https://grouplens.org/datasets/movielens/). This dataset contains user ratings for movies (1-5 stars, incl. half stars, e.g., 3.5). Specifically, we use the [MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/1m/) containing 1 Million ratings from ~6,000 users on ~4,000 movies and was released February 2003 -- so do not expect any recent Marvel movies :).\n",
    "\n",
    "While there are more sophisticated recommendation algorithms -- and we will look into those in a later lecture -- here we focus on Association Rules. We convert this rating dataset into a transaction dataset, where a transaction represents all the movies a user has purchased. We already did this for you making the following assumption: A User has purchased all the movies s/he gave the highest rating. For example, if User A gave a highest rating of 4.5 to any movie, A has purchased all movies A rated with 4.5. This is certainly a simplifying assumption, but perfectly fine for this task here.\n",
    "\n",
    "Let's have a quick look at the data. First, we load the ids and names of all movies into a dictionary. We need this dictionary since our transactions (i.e., the list of movies a user has bought) contains the ids and not the names of the movies. So to actually see the names of movies in the association rules, we need this way to map from a movie's id to its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file with movies (and der ids) into a pandas dataframe\n",
    "df_movies = pd.read_csv('data/a1-arm-movies.csv', header=None)\n",
    "# Convert dataframe to dictionary for quick lookups\n",
    "movie_map = dict(zip(df_movies[0], df_movies[1]))\n",
    "# Show the first 5 entries as example\n",
    "for movie_id, movie_name in movie_map.items():\n",
    "    print('{} -> {}'.format(movie_id, movie_name))\n",
    "    if movie_id >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26f5d2",
   "metadata": {},
   "source": [
    "No we can load the transactions. Again, a transaction is a user's shopping history, i.e., all the movies the user has bought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe341beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_histories = []\n",
    "\n",
    "# Read shopping histories; each line is a comma-separated list of the movies (i.e., their ids!) a user bought\n",
    "with open('data/a1-arm-movie-shopping-histories.csv') as file:\n",
    "    for line in file:\n",
    "        shopping_histories.append(tuple([ int(i) for i in line.strip().split(',') ]))\n",
    "\n",
    "# Show the shopping history of the first user for an example; we need movie_map to get the name of each movie\n",
    "user = 0\n",
    "\n",
    "print('Shopping history for user {} (used for Aprior algorithm)'.format(user))\n",
    "print(shopping_histories[user])\n",
    "print()\n",
    "print('Detailed shopping history for user {}'.format(user))\n",
    "for movie_id in shopping_histories[user]:\n",
    "    print('{}: {}'.format(movie_id, movie_map[movie_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58050d9b",
   "metadata": {},
   "source": [
    "With the dataset loaded, we are ready to find interesting Association Rules. For performance reasons, we use the `efficient_apriori` package.\n",
    "\n",
    "For added convenience, we provide method `show_top_rules()` which computes the Association Rules using the `efficient-apriori` package, but (a) sorts the rules w.r.t. the specified metric (default: lift), and (b) shows only the top-k rules (default: 5). The method also ensures a consistent output of each Association Rule. Each rule contains the LHS (left-hand side), RHS, as well as the support (s), confidence (c), and lift (l). Feel free to check out the code of method `show_top_rules()` in `src.utils` if anything might be unclear regarding its use.\n",
    "\n",
    "**Run the following 4 code cells and interpret the results below!** All 4 code cells find Association Rules using the `efficient-apriori` package encapsulated in the auxiliary method `show_top_rules()` for convenience. Appreciate how Runs A-B differ with respect to the input parameter of the method calls! Also, note that we call `show_top_rules()` with `id_map=None` at first, so the results will only display the movie ids. Later, you will be asked to run the cells again with `id_map=movie_map` to see the actual names of the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb470790",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run A\n",
    "show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.2, k=10, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.2, k=10, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run B\n",
    "show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.2, k=10, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.2, k=10, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run C\n",
    "show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run D\n",
    "show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6ed85",
   "metadata": {},
   "source": [
    "### 4 (a) Compare the Runs A-D and Discuss your Observations! (4 Points)\n",
    "\n",
    "You must have noticed numerous differences between the 4 runs A-D. List at least 2 differences you have found. You may want to consider the elapsed time and the resulting association rules. Briefly explain your observations! For this subtask, you do not need to look at the movie names (`id_map=None`) as your observations are not specific to the context of movie recommendations.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99128f",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    \n",
    "Possible answers (can be phrased differently):\n",
    "\n",
    "* Lower values of `min_support` significantly increase the runtime.\n",
    "\n",
    "* Larger values of `min_support` are more likely yield rules where the left-hand side and right-hand side contains only a single item.\n",
    "\n",
    "* Larger values of `min_support` yield rules with larger lifts. This seems to be because the left-hand side and/or right-hand side are more likely contain more than one item. Such rules show higher lifts here.\n",
    "\n",
    "* Choice of `min_confidence` not really that important for the runtime as the heavy lifting is done when finding Frequent Itemsets which is only determined by `min_support`\n",
    "\n",
    "* Many subsequent rules are the same just flipped. This is because the definitions of support and lift are \"symmetrical\" so `S(X->Y) = S(Y->X)` and `L(X->Y) = L(Y->X)`. This does not hold for confidence, but at least here, both rules are always larger then `min_confidence`\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed27049",
   "metadata": {},
   "source": [
    "\n",
    "### 4 (b) Compare the Runs A-D and discuss the results for building a recommendation engine! (4 Points)\n",
    "\n",
    "Now run the code cells above for Runs A-B again, but this time with `id_map=movie_map` so that the output will show for each rule the actual movie names.\n",
    "\n",
    "Comparing the results of the different runs again, but now seeing the actual movie names, should give you some further insights: particularly on (i) what you notice about the movie titles that appear on the LHS and RHS; and (ii) how the value of `min_support` affects the movies that appear. Explain the implications of your findings for building a recommendation engine.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc39f0",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    \n",
    "Possible answers:\n",
    "\n",
    "* Many association rules have LHSs and RHSs containing movies of the same franchise; while not very surprising, it is an interesting observations. This makes recommending other movies of the same series/franchise generally a safe bet\n",
    "\n",
    "* For higher values of `min_support` we mainly see rules referring to the overall most popular movies. As a consequence, we get only rules for making \"mainstream\" recommendations. In other words, for users with a rather unique/niche taste, we do not have suitable rules to make recommendations. Also, we are unlikely to make any recommendations beyond mainstream views. However, knowing which movie is generally popular allows to make recommendation to new users without any shopping history. Here, recommending overall popular movies is not a bad approach.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1cdeaf",
   "metadata": {},
   "source": [
    "### 4 (c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points)\n",
    "\n",
    "So far, we only looked at individual rules and how the set of rules changes for different parameter values for `min_support` and `min_confidence`. However, we still need some method like `make_recommendation(shopping_history)` that takes the shopping history of a user and returns 1 or more recommendations. You do *not* have to implement or provide code for such a method; just briefly sketch any reasonable way to do it, taking into account how to handle situations such as new users (empty shopping history); or users with long shopping history (more than the length of the left side of all our association rules).\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ddc7a",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    \n",
    "* Main idea: use the user's shopping history as the LHS, and find movies that appear as the RHS of a large number of association rules.\n",
    "    \n",
    "* For new users, his/her shopping history is empty. Here, recommending popular movies (high support) is a good option to fall back to.\n",
    "        \n",
    "* A user's shopping history can be longer than the LHSs of Association Rules. This could be handled in multiple ways: e.g., taking the user's recent history, or taking random (or all) samples of the user's shopping history.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a611ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
